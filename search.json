[
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "",
    "text": "In the post I will show how to tweak some of the internals of FaterRcnn in Pytorch. I am assuming the reader is someone who already have trained an object detection model using pytorch. If not there is and excellent tutorial in pytorch website.\n\n\nBasically Faster Rcnn is a two stage detector 1. The first stage is the Region proposal network which is resposible for knowing the objectness and corresponding bounding boxes. So essentially the RegionProposalNetwork will give the proposals of whether and object is there or not 2. These proposals will be used by the RoIHeads which outputs the detections . * Inside the RoIHeads roi align is done * There will be a box head and box predictor * The losses for the predictions 3. In this post i will try to show how we can add custom parts to the torchvision FasterRcnn\n\n\nCode\nimport torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(f'torch version {torch.__version__}')\nprint(f'torchvision version {torchvision.__version__}')\n\n\ntorch version 1.7.0\ntorchvision version 0.8.1"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html#small-insight-into-the-model",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html#small-insight-into-the-model",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "",
    "text": "Basically Faster Rcnn is a two stage detector 1. The first stage is the Region proposal network which is resposible for knowing the objectness and corresponding bounding boxes. So essentially the RegionProposalNetwork will give the proposals of whether and object is there or not 2. These proposals will be used by the RoIHeads which outputs the detections . * Inside the RoIHeads roi align is done * There will be a box head and box predictor * The losses for the predictions 3. In this post i will try to show how we can add custom parts to the torchvision FasterRcnn\n\n\nCode\nimport torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(f'torch version {torch.__version__}')\nprint(f'torchvision version {torchvision.__version__}')\n\n\ntorch version 1.7.0\ntorchvision version 0.8.1"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-without-fpn",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-without-fpn",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "Custom Backbone without FPN",
    "text": "Custom Backbone without FPN\nThis is pretty well written in the pytorch tutorials section, i will add some comments to it additionally\n\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n#we need to specify an outchannel of this backone specifically because this outchannel will be\n#used as an inchannel for the RPNHEAD which is producing the out of RegionProposalNetwork\n#we can know the number of outchannels by looking into the backbone \"backbone??\"\nbackbone.out_channels = 1280\n#by default the achor generator FasterRcnn assign will be for a FPN backone, so\n#we need to specify a  different anchor generator\nanchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n#here at each position in the grid there will be 3x3=9 anchors\n#and if our backbone is not FPN then the forward method will assign the name '0' to feature map\n#so we need to specify '0 as feature map name'\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                 output_size=9,\n                                            sampling_ratio=2)\n#the output size is the output shape of the roi pooled features which will be used by the box head\nmodel = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 600)]\npredictions = model(x)"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-with-fpn",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-with-fpn",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "Custom Backbone with FPN",
    "text": "Custom Backbone with FPN\nThe Resnet50Fpn available in torchvision\n\n# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)\n\n\nAdding a different resenet backbone\n\nJust change to a different resenet\nShows how we should change roi_pooler and anchor_generator along with the backbone changes if we are not using all the layers from FPN\n\n\n\nUsing all layers from FPN\n\n#hte returned layers are layer1,layer2,layer3,layer4 in returned_layers\nbackbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True)\nmodel = FasterRCNN(backbone,num_classes=2)                                                                       \n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)\n\n\n\nUsing not all layers from FPN\nThe size of the last fature map in a Resnet50.Later i will show the sizes of the feature maps we use when we use FPN.\n\n\nCode\n#just to show what will be out of of a normal resnet without fpn\nres = torchvision.models.resnet50()\npure = nn.Sequential(*list(res.children())[:-2])\ntemp = torch.rand(1,3,400,400)\npure(temp).shape\n\n\ntorch.Size([1, 2048, 13, 13])\n\n\nThe required layers can be obtained by specifying the returned layers parameters.Also the resnet backbone of different depth can be used.\n\n#the returned layers are layer1,layer2,layer3,layer4 in returned_layers\nbackbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True,\n                                                                          returned_layers=[2,3,4])\n\nHere we are using feature maps of the following shapes.\n\n\nCode\nout = backbone(temp)\nfor i in out.keys():\n    print(i,'  ',out[i].shape)\n\n\n0    torch.Size([1, 256, 50, 50])\n1    torch.Size([1, 256, 25, 25])\n2    torch.Size([1, 256, 13, 13])\npool    torch.Size([1, 256, 7, 7])\n\n\n\n#from the above we can see that the feature are feat maps should be 0,1,2,pool\n#where pool comes from the default extra block\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','pool'],\n                output_size=7,\n                sampling_ratio=2)\n\nSo essentially what we did was we selected the last three layers in FPN by specifying them in the returned layers, by default, the backbone will add a pool layer on top of the last layer. So we are left with four layers. Now the RoIAlign need to be done in these four layers. If we dnt specify the RoIAlign it will use the by default assume we have used all layers from FPN in torchvision. So we need to specifically give the feauture maps that we used. The usage of feature maps can be our application specific, some time you might need to detect small objects sometimes the object of interest will be large objects only.\n\n#we will need to give anchor_generator because the deafault anchor generator assumes we use all layers in fpn \n#since we have four layers in fpn here we need to specify 4 anchors\nanchor_sizes = ((32), (64), (128),(256) ) \naspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes)\nanchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n\nSince we have four layers in our FPN we need to specify the anchors. So here each feature map will have 4 anchors at each position.So the first feature map will have anchor size 32 and four of them will be there at each position in the feature map of aspect_ratios (0.5,1.0, 1.5,2.0). Now we can pass these to the FasterRCNN class\n\nmodel = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator,box_roi_pool=roi_pooler)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)"
  },
  {
    "objectID": "posts/2022-08-24-understanding the losses in centernet architecture.html",
    "href": "posts/2022-08-24-understanding the losses in centernet architecture.html",
    "title": "Understanding the losses in CenterNet Architecture",
    "section": "",
    "text": "Brief Intro\nThis is a small demo of the how the ground truths and loss will look in centerNet. Most of the code is from MMdetection. The idea here is to show a demo part so that the code is understandable. My repo for understanding the architecure can be found here . To understand the architecture please go through the blog by Shreejal Trivedi.\n\n\nHow to generate the groundTruth heat maps\n\nWe need to generate heatmap, the heatmap is generated such that at the center of the image, where the bounding box is the value will be 1 and decreasing around it like a gaussian.\nTo calculate the radius we use the bbox height and width which have been scaled down to the feature map level.\nWe can use the same function used in MMdetection. You can find the detailed description in this link\nLets assume that our initial image was of size (64,64) and it was scaled down to (8,8) after the feature maps and lets assume the radius is 2, in reality the radius is calculated as the function shown below based on the bounding box size\n\n\n\nCode\nimport torch\nimport numpy as np\n\n\n\n\nCode\ndef gaussian2D(radius, sigma=1, dtype=torch.float32, device='cpu'):\n    \"\"\"Generate 2D gaussian kernel.\n\n    Args:\n        radius (int): Radius of gaussian kernel.\n        sigma (int): Sigma of gaussian function. Default: 1.\n        dtype (torch.dtype): Dtype of gaussian tensor. Default: torch.float32.\n        device (str): Device of gaussian tensor. Default: 'cpu'.\n\n    Returns:\n        h (Tensor): Gaussian kernel with a\n            ``(2 * radius + 1) * (2 * radius + 1)`` shape.\n    \"\"\"\n    x = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(1, -1)\n    y = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(-1, 1)\n\n    h = (-(x * x + y * y) / (2 * sigma * sigma)).exp()\n\n    h[h &lt; torch.finfo(h.dtype).eps * h.max()] = 0\n    return h\n\n\ndef gen_gaussian_target(heatmap, center, radius, k=1):\n    \"\"\"Generate 2D gaussian heatmap.\n\n    Args:\n        heatmap (Tensor): Input heatmap, the gaussian kernel will cover on\n            it and maintain the max value.\n        center (list[int]): Coord of gaussian kernel's center.\n        radius (int): Radius of gaussian kernel.\n        k (int): Coefficient of gaussian kernel. Default: 1.\n\n    Returns:\n        out_heatmap (Tensor): Updated heatmap covered by gaussian kernel.\n    \"\"\"\n    diameter = 2 * radius + 1\n    gaussian_kernel = gaussian2D(\n        radius, sigma=diameter / 6, dtype=heatmap.dtype, device=heatmap.device)\n\n    x, y = center\n\n    height, width = heatmap.shape[:2]\n\n    left, right = min(x, radius), min(width - x, radius + 1)\n    top, bottom = min(y, radius), min(height - y, radius + 1)\n\n    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n    masked_gaussian = gaussian_kernel[radius - top:radius + bottom,\n                                      radius - left:radius + right]\n    out_heatmap = heatmap\n    torch.max(\n        masked_heatmap,\n        masked_gaussian * k,\n        out=out_heatmap[y - top:y + bottom, x - left:x + right])\n\n    return out_heatmap\n\n\n\ndef gaussian_radius(det_size, min_overlap):\n    r\"\"\"Generate 2D gaussian radius.\n\n    This function is modified from the `official github repo\n    &lt;https://github.com/princeton-vl/CornerNet-Lite/blob/master/core/sample/\n    utils.py#L65&gt;`_.\n\n    Given ``min_overlap``, radius could computed by a quadratic equation\n    according to Vieta's formulas.\n\n    There are 3 cases for computing gaussian radius, details are following:\n\n    - Explanation of figure: ``lt`` and ``br`` indicates the left-top and\n      bottom-right corner of ground truth box. ``x`` indicates the\n      generated corner at the limited position when ``radius=r``.\n\n    - Case1: one corner is inside the gt box and the other is outside.\n\n    .. code:: text\n\n        |&lt;   width   &gt;|\n\n        lt-+----------+         -\n        |  |          |         ^\n        +--x----------+--+\n        |  |          |  |\n        |  |          |  |    height\n        |  | overlap  |  |\n        |  |          |  |\n        |  |          |  |      v\n        +--+---------br--+      -\n           |          |  |\n           +----------+--x\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{(w-r)*(h-r)}{w*h+(w+h)r-r^2} \\ge {iou} \\quad\\Rightarrow\\quad\n        {r^2-(w+h)r+\\cfrac{1-iou}{1+iou}*w*h} \\ge 0 \\\\\n        {a} = 1,\\quad{b} = {-(w+h)},\\quad{c} = {\\cfrac{1-iou}{1+iou}*w*h}\n        {r} \\le \\cfrac{-b-\\sqrt{b^2-4*a*c}}{2*a}\n\n    - Case2: both two corners are inside the gt box.\n\n    .. code:: text\n\n        |&lt;   width   &gt;|\n\n        lt-+----------+         -\n        |  |          |         ^\n        +--x-------+  |\n        |  |       |  |\n        |  |overlap|  |       height\n        |  |       |  |\n        |  +-------x--+\n        |          |  |         v\n        +----------+-br         -\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{(w-2*r)*(h-2*r)}{w*h} \\ge {iou} \\quad\\Rightarrow\\quad\n        {4r^2-2(w+h)r+(1-iou)*w*h} \\ge 0 \\\\\n        {a} = 4,\\quad {b} = {-2(w+h)},\\quad {c} = {(1-iou)*w*h}\n        {r} \\le \\cfrac{-b-\\sqrt{b^2-4*a*c}}{2*a}\n\n    - Case3: both two corners are outside the gt box.\n\n    .. code:: text\n\n           |&lt;   width   &gt;|\n\n        x--+----------------+\n        |  |                |\n        +-lt-------------+  |   -\n        |  |             |  |   ^\n        |  |             |  |\n        |  |   overlap   |  | height\n        |  |             |  |\n        |  |             |  |   v\n        |  +------------br--+   -\n        |                |  |\n        +----------------+--x\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{w*h}{(w+2*r)*(h+2*r)} \\ge {iou} \\quad\\Rightarrow\\quad\n        {4*iou*r^2+2*iou*(w+h)r+(iou-1)*w*h} \\le 0 \\\\\n        {a} = {4*iou},\\quad {b} = {2*iou*(w+h)},\\quad {c} = {(iou-1)*w*h} \\\\\n        {r} \\le \\cfrac{-b+\\sqrt{b^2-4*a*c}}{2*a}\n\n    Args:\n        det_size (list[int]): Shape of object.\n        min_overlap (float): Min IoU with ground truth for boxes generated by\n            keypoints inside the gaussian kernel.\n\n    Returns:\n        radius (int): Radius of gaussian kernel.\n    \"\"\"\n    height, width = det_size\n\n    a1 = 1\n    b1 = (height + width)\n    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n    sq1 = sqrt(b1**2 - 4 * a1 * c1)\n    r1 = (b1 - sq1) / (2 * a1)\n\n    a2 = 4\n    b2 = 2 * (height + width)\n    c2 = (1 - min_overlap) * width * height\n    sq2 = sqrt(b2**2 - 4 * a2 * c2)\n    r2 = (b2 - sq2) / (2 * a2)\n\n    a3 = 4 * min_overlap\n    b3 = -2 * min_overlap * (height + width)\n    c3 = (min_overlap - 1) * width * height\n    sq3 = sqrt(b3**2 - 4 * a3 * c3)\n    r3 = (b3 + sq3) / (2 * a3)\n    return min(r1, r2, r3)\n\n\n\nh = gaussian2D(radius=2, sigma=1, dtype=torch.float32, device='cpu')\n\n\nh\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\nOur radius was 2 so we can see that at (2,2) the magnitude is 1 and in a gaussian kernel way,it decreases around.Now this heatmap will be copied to the center as required, but if the center is at the corners then cropping of the heatmap might be requried as needed 1. As in the begining assume that the heatmap is of shape 8,8 and lets assume that the object is located at the center (3,3). 2. So we need to copy the ground truth heatMap to that position as is as shown in the code below\n\nradius = 2\nheatmap = torch.zeros((8,8))\nheight, width = heatmap.shape[:2]\ncenter=(3,3)\nx, y = center\n# we are doing this because the kernel may lie outside the heatmap for example near corners\nleft, right = min(x, radius), min(width - x, radius + 1)\ntop, bottom = min(y, radius), min(height - y, radius + 1)\n\nmasked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\nmasked_gaussian = h[radius - top:radius + bottom,\n                                      radius - left:radius + right]\n\n\nmasked_gaussian\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\n\nmasked_heatmap\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nout_heatmap = heatmap\ntorch.max(\n        masked_heatmap,\n        masked_gaussian ,\n        out=out_heatmap[y - top:y + bottom, x - left:x + right])\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\n\nout_heatmap\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n        [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n        [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n        [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n        [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n\n\n\nout_heatmap.shape\n\ntorch.Size([8, 8])\n\n\nWe can see that the heatmap has been placed with a value of 1 as (3,3) which is the center we gave.\nNow look at how the losses will work.\n\n\nHeatMap loss\n\nSuppose we have 4 classes, 2 batches and feature map size be 8, then the predicted heatmap will be of shape 2,4,8,8 . (batch,num_classes,height,width)\nSuppose we have two images and one bounding box each in two images, let the first image have class id 0 and the second image have classid 2 . So the corresponding depth will have the heatmap gaussian as the ground truth with center having one and the gaussian kernel spread around.\nFor now we will use the above geneareted heatmap as the object , that is we have object at center 3,3 at id 0 in first image and at id 2 in second image.\n\n\ngroundTruth = torch.zeros((2,4,8,8),dtype=torch.float32)\nprint(\"GroundTruth shape\",groundTruth.shape)\n# now we need to copy the heat map we generated above to the positions of the class ids,\n# here we have assumed the in the first image the class id 0 is having the bounding box\n# and in the image 2 the classid 2 is having the object, for simplicity we are assuming\n# that both the images have same heat map an center, the assignment is as follows then\ngroundTruth[0,0,:,:] = out_heatmap.clone()\ngroundTruth[1,2,:,:] = out_heatmap.clone()\nprint(\"Ground Truth \\n\",groundTruth)\n\nGroundTruth shape torch.Size([2, 4, 8, 8])\nGround Truth \n tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n\n\nNow we will make a random prediction and see how we calculate the losses\n\npred = torch.randn(2,4,8,8,dtype=torch.float32).sigmoid()\n\n\n# the loss is as the described in the paper and is a modified focal loss\nalpha = 2.0\ngamma = 4.0\neps = 1e-12\npos_weights = groundTruth.eq(1)\nneg_weigths = (1 - groundTruth).pow(gamma)\n\nFrom the heat map we can see that the negative samples will be much more and therefore they introduced this modified version of focal loss to counteract that\n\npos_loss = -(pred+eps).log()*(1 - pred).pow(alpha)*pos_weights\nneg_loss = -(1 - pred+ eps).log()*pred.pow(alpha)*neg_weigths\nfinal_loss = pos_loss + neg_loss\nprint(final_loss.sum())\n\ntensor(181.3507)\n\n\n\n\nGroundTruth WH and WH offset\n\nThe the head predicts the wh in the shape of batch,2,height,widht and the same is the shape of offset head\nFor wh head the bbox scaled down widht and height will be placed at the indexes 0 and 1.\nFor offset head the the corressponding difference in offset will be placed at the corresponding indexes.\nTo understand it better, initially we have assumed that we have an image of shape (64,64) and after scaling down it becomes (8,8). And suppose in the original image the bounding box center is at (28,28) so when we scale it down to feature map level it becomem 28/8 = 3.5 and we have to take the int of that so the offset difference is (3.5 -3) which is 0.5. Its as shown below.\n\n\n#so cx and cy will be \nctx,cty = 28/8 ,28/8\nctx_int,cty_int = int(ctx),int(cty)\n\nprint(\"orginal scaled down \",(ctx, cty) )\nprint(\"floored version \" ,(ctx_int,cty_int))\nprint(\"offset is \",(ctx- ctx_int,cty-cty_int))\n\norginal scaled down  (3.5, 3.5)\nfloored version  (3, 3)\noffset is  (0.5, 0.5)\n\n\n\n# now we generate the ground truth\ngroundTruthWH =torch.zeros((2,2,8,8),dtype=torch.float32)\ngroundTruthWHOffset = torch.zeros((2,2,8,8),dtype=torch.float32)\ngroundTruthWHOffsetWeights = torch.zeros((2,2,8,8),dtype=torch.float32)\n\n\n# so for the we have said that the object is same position in both the images, so when\n# groundTruth is set the to be predicted width and height at the same position\n\ngroundTruthWHOffset[0,0,ctx_int,cty_int] = ctx- ctx_int\ngroundTruthWHOffset[0,1,ctx_int,cty_int] = cty-cty_int\n\n\n# we are asuming the object is at the same position in both the images so the\n# above will be the same for batchid 1\ngroundTruthWHOffset[1,0,ctx_int,cty_int] = ctx- ctx_int\ngroundTruthWHOffset[1,1,ctx_int,cty_int] = cty-cty_int\n\nWe need set weights because we need to consider loss only from the places there was an object\n\ngroundTruthWHOffsetWeights[0,:,ctx_int,cty_int] = 1\n#since the second batch image is also at the same place\ngroundTruthWHOffsetWeights[1,:,ctx_int,cty_int] = 1\n\nLets make a random prediction to calculate the loss\n\npredWH = torch.randn((2,2,8,8),dtype=torch.float32)\npredWHOffset = torch.randn((2,2,8,8),dtype=torch.float32)\n\nThe loss we use for the wh and wh_offset are the same and is the l1_loss\n\ndef l1_loss(pred, target):\n    \"\"\"L1 loss.\n\n    Args:\n        pred (torch.Tensor): The prediction.\n        target (torch.Tensor): The learning target of the prediction.\n\n    Returns:\n        torch.Tensor: Calculated loss\n    \"\"\"\n    if target.numel() == 0:\n        return pred.sum() * 0\n\n    assert pred.size() == target.size()\n    loss = torch.abs(pred - target)\n    return loss\n\n\n#Note we are multiplying by the weights in the end to get the loss from required poistion only\nWHLoss = l1_loss(predWH,groundTruthWH)*groundTruthWHOffsetWeights\nWHOffsetLoss = l1_loss(predWHOffset,groundTruthWHOffset)*groundTruthWHOffsetWeights\n\nWHLoss = WHLoss.sum()\nWHOffsetLoss = WHOffsetLoss.sum()\n\n\nprint(\"Ground Truth Loss \",WHLoss)\nprint(\"Ground Offset Loss \",WHOffsetLoss)\n\nGround Truth Loss  tensor(3.6995)\nGround Offset Loss  tensor(2.0938)\n\n\n\nFinal Loss\nThe final loss is the weighted sum of the heapmap loss, wh loss and wh_offset loss.There is a little more things involved and in the repo i have showed how these are actually done in a real implemenation. Hope this was helpful"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html",
    "href": "posts/2020-09-20-pytorch_iou.html",
    "title": "Explaining IoU",
    "section": "",
    "text": "In this blogpost i will explain what is IOU, where is it used , how is it implemented\n\n\nIOU is pretty much clear by the name intersection over union. The formula is - IOU = Area of Intersection / Area of union - Area of union = First Box Area + Second Box Area -Intersection Area\n\n\n\n\nHere i will show a simple implementation in pytorch.If you look at the below picture we will get a basic idea of how to get the intersection between two boxes, the rest are simple\n\nFor the basic implementation of this can be found in this nice blogpost and from that is basic implemenation is like this\n\n\nCode\ndef batch_iou(a, b, epsilon=1e-5):\n    \"\"\" Given two arrays `a` and `b` where each row contains a bounding\n        box defined as a list of four numbers:\n            [x1,y1,x2,y2]\n        where:\n            x1,y1 represent the upper left corner\n            x2,y2 represent the lower right corner\n        It returns the Intersect of Union scores for each corresponding\n        pair of boxes.\n\n    Args:\n        a:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        b:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        epsilon:    (float) Small value to prevent division by zero\n\n    Returns:\n        (numpy array) The Intersect of Union scores for each pair of bounding\n        boxes.\n    \"\"\"\n    # COORDINATES OF THE INTERSECTION BOXES\n    x1 = np.array([a[:, 0], b[:, 0]]).max(axis=0)\n    y1 = np.array([a[:, 1], b[:, 1]]).max(axis=0)\n    x2 = np.array([a[:, 2], b[:, 2]]).min(axis=0)\n    y2 = np.array([a[:, 3], b[:, 3]]).min(axis=0)\n\n    # AREAS OF OVERLAP - Area where the boxes intersect\n    width = (x2 - x1)\n    height = (y2 - y1)\n\n    # handle case where there is NO overlap\n    width[width &lt; 0] = 0\n    height[height &lt; 0] = 0\n\n    area_overlap = width * height\n\n    # COMBINED AREAS\n    area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])\n    area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n    area_combined = area_a + area_b - area_overlap\n\n    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n    iou = area_overlap / (area_combined + epsilon)\n    return iou\n\n\n\n\n\nBut the above implementation assumes that both the bounding boxes have the same set of batches,which is rarely the case. IOU is mainly used in object detection tasks.\n\nWe will have a set of anchors for each position in the feature map,for eg say if we have a feature map of shape 5x5 and there are 3 anchors per position then there will be 5x5x3=75 total anchors\nThe Ground trouth boxes for that feature map may be much less the number of anchors\nWe need to find the matching anchors to the bounding boxes, so we can select that portion of the feature map for the downstream predictions.\n\n\n\nBasically when we get two boxes say\na- B,M,4 – the anchor boxes after reshaping(B,A,H,W,4) where A is number of anchors\nb- B,N,4 –the real bboxes. N is the max number of boxes in certain image and the other images will be padded with -1.\nwe need to compute iou between a and b so each box in a is compare with each box in b. So we should make N copies of copies of each box in a to be compare with N bboxes. Also if we want to vectorise this operation then we need to make M copies of b. So the final dimensions will be\na - B,M,N,4 b - B,M,N,4\nNow we can say like each slice of the both a and b can be compared\n\nimport torch\n#say the given anchors and bboxes are in shape x_top,y_top,x_btm,y_btm\nsample_anchors = torch.tensor([[[[[5.,5,15,15], [25,25,35,35],[1,1,9,9]]]]]) #only 1 batch\nbboxes = torch.tensor([[[1.,1,11,11], [20,20,30,30]]]) \nB = bboxes.shape[0]\nno_of_bboxes = bboxes.shape[1]\nprint('sample anchors \\n', sample_anchors,'\\n')\nprint('sample bboxes \\n', bboxes,'\\n')\nprint('sample number of anchors shape ',sample_anchors.shape)\nprint('sample bboxes shape ',bboxes.shape,'\\n')\n\nsample anchors \n tensor([[[[[ 5.,  5., 15., 15.],\n           [25., 25., 35., 35.],\n           [ 1.,  1.,  9.,  9.]]]]]) \n\nsample bboxes \n tensor([[[ 1.,  1., 11., 11.],\n         [20., 20., 30., 30.]]]) \n\nsample number of anchors shape  torch.Size([1, 1, 1, 3, 4])\nsample bboxes shape  torch.Size([1, 2, 4]) \n\n\n\nHere we need to compare the 3 anchor boxes with the two bboxes, first we reshape the anchors to be of shape batch,total_anchors,4,\nwe need to compute iou between sample_anchors and bboxes so each of the 3 anchors are compared with the bboxes which is 2 here. So for vectorized implementation we should make 3 copies of copies of each anchor in sample_anchors to be compare with 2 bboxes. Also if we should make 3 copies of b to aid in vectorized implementation. So the final dimensions will be\n\nsample_anchors - B,3,2,4\nb=boxes - B,3,2,4\n\n\nsample_anchors = sample_anchors.reshape(B,-1,4)\nno_of_anchors = sample_anchors.shape[1]\nsample_anchors = sample_anchors.unsqueeze(2).expand(-1,-1,no_of_bboxes,-1)\nprint(sample_anchors)\nprint(sample_anchors.shape)\n\ntensor([[[[ 5.,  5., 15., 15.],\n          [ 5.,  5., 15., 15.]],\n\n         [[25., 25., 35., 35.],\n          [25., 25., 35., 35.]],\n\n         [[ 1.,  1.,  9.,  9.],\n          [ 1.,  1.,  9.,  9.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\nbboxes = bboxes.unsqueeze(1).expand(-1,no_of_anchors,-1,-1)\nprint(bboxes)\nprint(bboxes.shape)\n\ntensor([[[[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\n#first we need to find the intersection for that width and height of the intersection area\n#this inturn can be obtained by finding the lefttop and bottom corner cordinates and subtracting them\n\nleft_top = torch.max(sample_anchors[:,:,:,:2],bboxes[:,:,:,:2])\nright_bottom = torch.min(sample_anchors[:,:,:,2:],bboxes[:,:,:,2:])\ndelta = right_bottom - left_top\nprint(delta)\n\ntensor([[[[  6.,   6.],\n          [ -5.,  -5.]],\n\n         [[-14., -14.],\n          [  5.,   5.]],\n\n         [[  8.,   8.],\n          [-11., -11.]]]])\n\n\n\n#The first element of delta is width and the next element is height, we can remove negative values \n#since this will be boxes that are not intersecting \n#(remember the the image top left if (0,0) and bottom y is positive downwards)\ndelta[delta&lt;0]=0\n#now find the intersection area\ninteresection_area = delta[:,:,:,0]*delta[:,:,:,1]\nprint(interesection_area)\nprint(interesection_area.shape)\n\ntensor([[[36.,  0.],\n         [ 0., 25.],\n         [64.,  0.]]])\ntorch.Size([1, 3, 2])\n\n\nA small picture represntation is tried below,we can see that first and 3rd anchors intersect with first bounding box while the 2nd anchor intersect with the next one \nFrom the intersection area above we can see that the where there are no itersection the area is zero and thus in this case the first and last anchor mathces with the first bbox while the second anchor mathces with the second one\n\n#now we need to find the Area of union which is \n#Area of union = First Box Area + Second Box Area -Intersection Area\nsample_anchors_area = (sample_anchors[:,:,:,2]-sample_anchors[:,:,:,0])*(sample_anchors[:,:,:,3] -\n                                                                        sample_anchors[:,:,:,1])\nbbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0]) * (bboxes[:,:,:,3] - bboxes[:,:,:,1])\niou = interesection_area/(sample_anchors_area+bbox_area - interesection_area)\nprint(iou)\nprint(iou.shape)\n\ntensor([[[0.2195, 0.0000],\n         [0.0000, 0.1429],\n         [0.6400, 0.0000]]])\ntorch.Size([1, 3, 2])\n\n\nso the final iou matrix will have shape (Batch,no_of_anchors,no_of_bboxes)\n\n\n\nThis iou matrix will be used for calculation the regression offsets, negative anchors,ground truth class . The other place where iou is used is for mean Average Precision at the end which if possible i will explain in another post\n\n\n\n\nBelow i will provide a small code for implementing this in a batch\n\ndef IOU(anchors,bboxes):\n    #anchors B,A,H,W,4\n    #bboxes B,N,4\n    B = anchors.shape[0]\n    anchors = anchors.reshape(B,-1,4)\n    M,N = anchors.shape[1],bboxes.shape[1]\n    \n    #expanding\n    anchors = anchors.unsqueeze(2).expand(-1,-1,N,-1)\n    bboxes = bboxes.unsqueeze(1).expand(-1,M,-1,-1)\n    \n    left_top = torch.max(anchors[:,:,:,:2],bboxes[:,:,:,:2])\n    right_bottom = torch.min(anchors[:,:,:,2:],bboxes[:,:,:,2:])\n    \n    delta = right_bottom - left_top\n    delta[delta&lt;0] = 0\n    \n    intersection_area = delta[:,:,:,0]*delta[:,:,:,1]\n    \n    anchors_area = (anchors[:,:,:,2]-anchors[:,:,:,0])*(anchors[:,:,:,3] -anchors[:,:,:,1])\n    bbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0])* (bboxes[:,:,:,3] - bboxes[:,:,:,1])\n    iou = interesection_area/(anchors_area+bbox_area - interesection_area)\n    \n    return iou"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html#what-is-iou",
    "href": "posts/2020-09-20-pytorch_iou.html#what-is-iou",
    "title": "Explaining IoU",
    "section": "",
    "text": "IOU is pretty much clear by the name intersection over union. The formula is - IOU = Area of Intersection / Area of union - Area of union = First Box Area + Second Box Area -Intersection Area"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html#how-is-it-implementedbasic",
    "href": "posts/2020-09-20-pytorch_iou.html#how-is-it-implementedbasic",
    "title": "Explaining IoU",
    "section": "",
    "text": "Here i will show a simple implementation in pytorch.If you look at the below picture we will get a basic idea of how to get the intersection between two boxes, the rest are simple\n\nFor the basic implementation of this can be found in this nice blogpost and from that is basic implemenation is like this\n\n\nCode\ndef batch_iou(a, b, epsilon=1e-5):\n    \"\"\" Given two arrays `a` and `b` where each row contains a bounding\n        box defined as a list of four numbers:\n            [x1,y1,x2,y2]\n        where:\n            x1,y1 represent the upper left corner\n            x2,y2 represent the lower right corner\n        It returns the Intersect of Union scores for each corresponding\n        pair of boxes.\n\n    Args:\n        a:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        b:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        epsilon:    (float) Small value to prevent division by zero\n\n    Returns:\n        (numpy array) The Intersect of Union scores for each pair of bounding\n        boxes.\n    \"\"\"\n    # COORDINATES OF THE INTERSECTION BOXES\n    x1 = np.array([a[:, 0], b[:, 0]]).max(axis=0)\n    y1 = np.array([a[:, 1], b[:, 1]]).max(axis=0)\n    x2 = np.array([a[:, 2], b[:, 2]]).min(axis=0)\n    y2 = np.array([a[:, 3], b[:, 3]]).min(axis=0)\n\n    # AREAS OF OVERLAP - Area where the boxes intersect\n    width = (x2 - x1)\n    height = (y2 - y1)\n\n    # handle case where there is NO overlap\n    width[width &lt; 0] = 0\n    height[height &lt; 0] = 0\n\n    area_overlap = width * height\n\n    # COMBINED AREAS\n    area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])\n    area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n    area_combined = area_a + area_b - area_overlap\n\n    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n    iou = area_overlap / (area_combined + epsilon)\n    return iou"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html#where-is-it-used-and-how-to-implement-for-that-use-case",
    "href": "posts/2020-09-20-pytorch_iou.html#where-is-it-used-and-how-to-implement-for-that-use-case",
    "title": "Explaining IoU",
    "section": "",
    "text": "But the above implementation assumes that both the bounding boxes have the same set of batches,which is rarely the case. IOU is mainly used in object detection tasks.\n\nWe will have a set of anchors for each position in the feature map,for eg say if we have a feature map of shape 5x5 and there are 3 anchors per position then there will be 5x5x3=75 total anchors\nThe Ground trouth boxes for that feature map may be much less the number of anchors\nWe need to find the matching anchors to the bounding boxes, so we can select that portion of the feature map for the downstream predictions.\n\n\n\nBasically when we get two boxes say\na- B,M,4 – the anchor boxes after reshaping(B,A,H,W,4) where A is number of anchors\nb- B,N,4 –the real bboxes. N is the max number of boxes in certain image and the other images will be padded with -1.\nwe need to compute iou between a and b so each box in a is compare with each box in b. So we should make N copies of copies of each box in a to be compare with N bboxes. Also if we want to vectorise this operation then we need to make M copies of b. So the final dimensions will be\na - B,M,N,4 b - B,M,N,4\nNow we can say like each slice of the both a and b can be compared\n\nimport torch\n#say the given anchors and bboxes are in shape x_top,y_top,x_btm,y_btm\nsample_anchors = torch.tensor([[[[[5.,5,15,15], [25,25,35,35],[1,1,9,9]]]]]) #only 1 batch\nbboxes = torch.tensor([[[1.,1,11,11], [20,20,30,30]]]) \nB = bboxes.shape[0]\nno_of_bboxes = bboxes.shape[1]\nprint('sample anchors \\n', sample_anchors,'\\n')\nprint('sample bboxes \\n', bboxes,'\\n')\nprint('sample number of anchors shape ',sample_anchors.shape)\nprint('sample bboxes shape ',bboxes.shape,'\\n')\n\nsample anchors \n tensor([[[[[ 5.,  5., 15., 15.],\n           [25., 25., 35., 35.],\n           [ 1.,  1.,  9.,  9.]]]]]) \n\nsample bboxes \n tensor([[[ 1.,  1., 11., 11.],\n         [20., 20., 30., 30.]]]) \n\nsample number of anchors shape  torch.Size([1, 1, 1, 3, 4])\nsample bboxes shape  torch.Size([1, 2, 4]) \n\n\n\nHere we need to compare the 3 anchor boxes with the two bboxes, first we reshape the anchors to be of shape batch,total_anchors,4,\nwe need to compute iou between sample_anchors and bboxes so each of the 3 anchors are compared with the bboxes which is 2 here. So for vectorized implementation we should make 3 copies of copies of each anchor in sample_anchors to be compare with 2 bboxes. Also if we should make 3 copies of b to aid in vectorized implementation. So the final dimensions will be\n\nsample_anchors - B,3,2,4\nb=boxes - B,3,2,4\n\n\nsample_anchors = sample_anchors.reshape(B,-1,4)\nno_of_anchors = sample_anchors.shape[1]\nsample_anchors = sample_anchors.unsqueeze(2).expand(-1,-1,no_of_bboxes,-1)\nprint(sample_anchors)\nprint(sample_anchors.shape)\n\ntensor([[[[ 5.,  5., 15., 15.],\n          [ 5.,  5., 15., 15.]],\n\n         [[25., 25., 35., 35.],\n          [25., 25., 35., 35.]],\n\n         [[ 1.,  1.,  9.,  9.],\n          [ 1.,  1.,  9.,  9.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\nbboxes = bboxes.unsqueeze(1).expand(-1,no_of_anchors,-1,-1)\nprint(bboxes)\nprint(bboxes.shape)\n\ntensor([[[[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\n#first we need to find the intersection for that width and height of the intersection area\n#this inturn can be obtained by finding the lefttop and bottom corner cordinates and subtracting them\n\nleft_top = torch.max(sample_anchors[:,:,:,:2],bboxes[:,:,:,:2])\nright_bottom = torch.min(sample_anchors[:,:,:,2:],bboxes[:,:,:,2:])\ndelta = right_bottom - left_top\nprint(delta)\n\ntensor([[[[  6.,   6.],\n          [ -5.,  -5.]],\n\n         [[-14., -14.],\n          [  5.,   5.]],\n\n         [[  8.,   8.],\n          [-11., -11.]]]])\n\n\n\n#The first element of delta is width and the next element is height, we can remove negative values \n#since this will be boxes that are not intersecting \n#(remember the the image top left if (0,0) and bottom y is positive downwards)\ndelta[delta&lt;0]=0\n#now find the intersection area\ninteresection_area = delta[:,:,:,0]*delta[:,:,:,1]\nprint(interesection_area)\nprint(interesection_area.shape)\n\ntensor([[[36.,  0.],\n         [ 0., 25.],\n         [64.,  0.]]])\ntorch.Size([1, 3, 2])\n\n\nA small picture represntation is tried below,we can see that first and 3rd anchors intersect with first bounding box while the 2nd anchor intersect with the next one \nFrom the intersection area above we can see that the where there are no itersection the area is zero and thus in this case the first and last anchor mathces with the first bbox while the second anchor mathces with the second one\n\n#now we need to find the Area of union which is \n#Area of union = First Box Area + Second Box Area -Intersection Area\nsample_anchors_area = (sample_anchors[:,:,:,2]-sample_anchors[:,:,:,0])*(sample_anchors[:,:,:,3] -\n                                                                        sample_anchors[:,:,:,1])\nbbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0]) * (bboxes[:,:,:,3] - bboxes[:,:,:,1])\niou = interesection_area/(sample_anchors_area+bbox_area - interesection_area)\nprint(iou)\nprint(iou.shape)\n\ntensor([[[0.2195, 0.0000],\n         [0.0000, 0.1429],\n         [0.6400, 0.0000]]])\ntorch.Size([1, 3, 2])\n\n\nso the final iou matrix will have shape (Batch,no_of_anchors,no_of_bboxes)\n\n\n\nThis iou matrix will be used for calculation the regression offsets, negative anchors,ground truth class . The other place where iou is used is for mean Average Precision at the end which if possible i will explain in another post"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html#complete-code",
    "href": "posts/2020-09-20-pytorch_iou.html#complete-code",
    "title": "Explaining IoU",
    "section": "",
    "text": "Below i will provide a small code for implementing this in a batch\n\ndef IOU(anchors,bboxes):\n    #anchors B,A,H,W,4\n    #bboxes B,N,4\n    B = anchors.shape[0]\n    anchors = anchors.reshape(B,-1,4)\n    M,N = anchors.shape[1],bboxes.shape[1]\n    \n    #expanding\n    anchors = anchors.unsqueeze(2).expand(-1,-1,N,-1)\n    bboxes = bboxes.unsqueeze(1).expand(-1,M,-1,-1)\n    \n    left_top = torch.max(anchors[:,:,:,:2],bboxes[:,:,:,:2])\n    right_bottom = torch.min(anchors[:,:,:,2:],bboxes[:,:,:,2:])\n    \n    delta = right_bottom - left_top\n    delta[delta&lt;0] = 0\n    \n    intersection_area = delta[:,:,:,0]*delta[:,:,:,1]\n    \n    anchors_area = (anchors[:,:,:,2]-anchors[:,:,:,0])*(anchors[:,:,:,3] -anchors[:,:,:,1])\n    bbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0])* (bboxes[:,:,:,3] - bboxes[:,:,:,1])\n    iou = interesection_area/(anchors_area+bbox_area - interesection_area)\n    \n    return iou"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I am Akash! . This is the place where i write some of the stuffs I am interested in . I am interested in Computer Vision, Sensor Fusion , Computer Graphics and Self Driving Cars."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Deformable DETR Code WalkThrough\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nOct 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Online RealTime Tracking\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the losses in CenterNet Architecture\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nAug 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSo what does does mobile blocks save\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHacking Into FasterRcnn in Pytorch\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nDec 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nExplaining IoU\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-07-30-so what does mobile blocks save.html",
    "href": "posts/2022-07-30-so what does mobile blocks save.html",
    "title": "So what does does mobile blocks save",
    "section": "",
    "text": "Brief Intro\nThis blog assumes the reader have a some understanding of mobileNet.This is just a lazy illustration of how much different mobileNet block save. The actual papers have the real numbers. If you want to know about the model please go through the papers MobileNetV1 MobileNetV2. In this we will briefly see how much parameters and floating point operations are required by a normal convolution block, mobileNetV1 and mobileNetV2 for the same input size to produce the same output. We will use torchinfo library for getting the summaries.\n\n\nCode\nimport torch\nimport torch.nn as nn\nfrom torchinfo import summary\nimport numpy as np\n\n\n\n# we will use the same input and outputs for all the conv blocks and mobile blocks\ninput_filters = 64\noutput_filters = 128\ninput_size = (3,input_filters,224,224)\n\n\n\nCode\ndef printInputAndOutput(model,input_filters=64):\n    rand_tensor = torch.rand((3,input_filters,224,224))\n    out = model(rand_tensor)\n    print(\"Input shape = \", rand_tensor.shape)\n    print(\"Output shap =\", out.shape)\n\n\n\n\nSimple ConvNet Block\n\nsimple_convBlock = nn.Sequential(nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=3,stride=2,\n                                          padding=1,bias=False),nn.BatchNorm2d(output_filters),\n                                nn.ReLU(inplace=True))\nprintInputAndOutput(simple_convBlock)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(simple_convBlock,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nSequential                               --                        [3, 128, 112, 112]        --                        --\n├─Conv2d: 1-1                            [3, 3]                    [3, 128, 112, 112]        73,728                    2,774,532,096\n├─BatchNorm2d: 1-2                       --                        [3, 128, 112, 112]        256                       768\n├─ReLU: 1-3                              --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 73,984\nTrainable params: 73,984\nNon-trainable params: 0\nTotal mult-adds (G): 2.77\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 77.07\nParams size (MB): 0.30\nEstimated Total Size (MB): 115.90\n============================================================================================================================================\n\n\n\n\nMobileNet Block\nThe main idea is to use depth wise convolution to reduce the parameters and floating point operations required. For more info please read the paper or watch this tutorial by Prof Maziar Raissi\n\n\nmobileNetBlock = nn.Sequential(\n                #DEPTHWISE CONV\n                #we get the depthwise convolution by specifying groups same as in_channels\n                nn.Conv2d(in_channels=input_filters,out_channels=input_filters,kernel_size=3,\n                         stride=2,padding=1,groups=input_filters,bias=False),\n                nn.BatchNorm2d(input_filters),\n                nn.ReLU(inplace=True),\n    \n                #POINTWISE CONV\n                nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=1,\n                          stride=1,padding=0,bias=False),\n                nn.BatchNorm2d(output_filters),\n                nn.ReLU(inplace=True)\n                )\nprintInputAndOutput(mobileNetBlock)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(mobileNetBlock,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nSequential                               --                        [3, 128, 112, 112]        --                        --\n├─Conv2d: 1-1                            [3, 3]                    [3, 64, 112, 112]         576                       21,676,032\n├─BatchNorm2d: 1-2                       --                        [3, 64, 112, 112]         128                       384\n├─ReLU: 1-3                              --                        [3, 64, 112, 112]         --                        --\n├─Conv2d: 1-4                            [1, 1]                    [3, 128, 112, 112]        8,192                     308,281,344\n├─BatchNorm2d: 1-5                       --                        [3, 128, 112, 112]        256                       768\n├─ReLU: 1-6                              --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 115.61\nParams size (MB): 0.04\nEstimated Total Size (MB): 154.18\n============================================================================================================================================\n\n\n\n\nMobileNetv2 Block\nThe idea here is to add a residual connection and with this better perfomance was obtained with a slight increase in number of parameters. For more info please read the paper or watch this tutorial by Prof Maziar Raissi\n\nclass MobileNetv2Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,expand_ratio,stride=1):\n        super(MobileNetv2Block,self).__init__()\n        self.conv1x1Begin = nn.Sequential(\n            nn.Conv2d(in_channels,in_channels*expand_ratio,kernel_size=1,stride=1,bias=False),\n            nn.BatchNorm2d(in_channels*expand_ratio),\n            nn.ReLU6(inplace=True))\n        \n        self.convDepthWise = nn.Sequential(\n            nn.Conv2d(in_channels*expand_ratio,in_channels*expand_ratio,kernel_size=3,stride=stride,padding=1,groups=in_channels*expand_ratio,bias=False),\n            nn.BatchNorm2d(in_channels*expand_ratio),\n            nn.ReLU6(inplace=True)\n        \n        )\n        \n        self.conv1x1Last = nn.Sequential(\n            nn.Conv2d(in_channels*expand_ratio,out_channels,kernel_size=1,stride=1,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU6(inplace=True))\n        \n        self.stride = stride\n        \n        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n        \n    def forward(self,x):\n        input_ = x     \n        x = self.conv1x1Begin(x)\n        x = self.convDepthWise(x)\n        x = self.conv1x1Last(x)\n\n        if self.use_res_connect:\n            return x+input_\n        else:\n            return x\n        \n\n\nmobileNetV2Block = MobileNetv2Block(64,128,2,2)\nprintInputAndOutput(mobileNetV2Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(mobileNetV2Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nMobileNetv2Block                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 224, 224]        --                        --\n│    └─Conv2d: 2-1                       [1, 1]                    [3, 128, 224, 224]        8,192                     1,233,125,376\n│    └─BatchNorm2d: 2-2                  --                        [3, 128, 224, 224]        256                       768\n│    └─ReLU6: 2-3                        --                        [3, 128, 224, 224]        --                        --\n├─Sequential: 1-2                        --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-4                       [3, 3]                    [3, 128, 112, 112]        1,152                     43,352,064\n│    └─BatchNorm2d: 2-5                  --                        [3, 128, 112, 112]        256                       768\n│    └─ReLU6: 2-6                        --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-3                        --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-7                       [1, 1]                    [3, 128, 112, 112]        16,384                    616,562,688\n│    └─BatchNorm2d: 2-8                  --                        [3, 128, 112, 112]        256                       768\n│    └─ReLU6: 2-9                        --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 462.42\nParams size (MB): 0.11\nEstimated Total Size (MB): 501.06\n============================================================================================================================================\n\n\n\n\nComparison\nNow we can compare the summaries of each block. From the above cells we can observe that the inputs and output shapes remains the same\n\n1)SimpleConvBlock\nTotal params: 73,984\nTrainable params: 73,984\nNon-trainable params: 0\nTotal mult-adds (G): 2.77\n\n\n2)MobileNetV1\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n\n\n3)MobileNetV2\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\nIf you look at the outputs of torchinfo you can see that the estimated total size is more for mobileNets than simpleConv block this isbecause we need to store 2 times the intermediate values during training , but this wont be a problem for inference, during inference we only need to store the parameters and architecture, and thus looking above we can see that way fewer parameters and total number of multiplications and additions needed is also low which helps in faster inference. If you want more info please read the papers which are well written. If you want to read about how the torchinfo works please read this blog by Jacob C. Kimmel\n\n\n\nDoing all the above with torchvision classes\nActually all the above were taken from torchvision only and we can do the same easily with torchvision classes as shown below . All credits are to the amazing torchvision library\n\nfrom torchvision.models.mobilenetv2 import MobileNetV2, InvertedResidual,ConvNormActivation\n\n\n#we have to put the expand_ratio as one which will reduce this to a simple mobilenetV1 block\nTorchMobileNetV1Block = InvertedResidual(64,128,stride=2,expand_ratio=1)\n\n\nTorchMobileNetV2Block = InvertedResidual(64,128,stride=2,expand_ratio=2)\n\n\nprintInputAndOutput(TorchMobileNetV1Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nprintInputAndOutput(TorchMobileNetV2Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(TorchMobileNetV1Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nInvertedResidual                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 112, 112]        --                        --\n│    └─ConvNormActivation: 2-1           --                        [3, 64, 112, 112]         --                        --\n│    │    └─Conv2d: 3-1                  [3, 3]                    [3, 64, 112, 112]         576                       21,676,032\n│    │    └─BatchNorm2d: 3-2             --                        [3, 64, 112, 112]         128                       384\n│    │    └─ReLU6: 3-3                   --                        [3, 64, 112, 112]         --                        --\n│    └─Conv2d: 2-2                       [1, 1]                    [3, 128, 112, 112]        8,192                     308,281,344\n│    └─BatchNorm2d: 2-3                  --                        [3, 128, 112, 112]        256                       768\n============================================================================================================================================\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 115.61\nParams size (MB): 0.04\nEstimated Total Size (MB): 154.18\n============================================================================================================================================\n\n\n\nsummary(TorchMobileNetV2Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nInvertedResidual                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 112, 112]        --                        --\n│    └─ConvNormActivation: 2-1           --                        [3, 128, 224, 224]        --                        --\n│    │    └─Conv2d: 3-1                  [1, 1]                    [3, 128, 224, 224]        8,192                     1,233,125,376\n│    │    └─BatchNorm2d: 3-2             --                        [3, 128, 224, 224]        256                       768\n│    │    └─ReLU6: 3-3                   --                        [3, 128, 224, 224]        --                        --\n│    └─ConvNormActivation: 2-2           --                        [3, 128, 112, 112]        --                        --\n│    │    └─Conv2d: 3-4                  [3, 3]                    [3, 128, 112, 112]        1,152                     43,352,064\n│    │    └─BatchNorm2d: 3-5             --                        [3, 128, 112, 112]        256                       768\n│    │    └─ReLU6: 3-6                   --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-3                       [1, 1]                    [3, 128, 112, 112]        16,384                    616,562,688\n│    └─BatchNorm2d: 2-4                  --                        [3, 128, 112, 112]        256                       768\n============================================================================================================================================\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 462.42\nParams size (MB): 0.11\nEstimated Total Size (MB): 501.06\n============================================================================================================================================"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html",
    "href": "posts/2025-10-31-deformable_detr.html",
    "title": "Deformable DETR Code WalkThrough",
    "section": "",
    "text": "Code\nimport torch \nimport torch.nn as nn\nprint(f'torch version {torch.__version__}')\n\n\ntorch version 2.9.0+cpu"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#overview",
    "href": "posts/2025-10-31-deformable_detr.html#overview",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Overview",
    "text": "Overview\nThis blog provides a code-level walkthrough of the internals of Deformable DETR and its core component—deformable attention—to understand how it works in practice. I’ll explore the full pipeline: starting from how a batch of images flows through the convolutional backbone, then into the encoder, and finally how deformable attention operates within the architecture both in encoder side and decoder cross attention. Along the way, I’ll highlight where Deformable DETR aligns with the original DETR and where it diverges. All examples shown below are executable in the accompanying notebook and have been tested to work end-to-end.\n\nPrerequisites\nI assume the reader have good understanding of DETR"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#why-was-deformable-detr-needed",
    "href": "posts/2025-10-31-deformable_detr.html#why-was-deformable-detr-needed",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Why was Deformable DETR needed?",
    "text": "Why was Deformable DETR needed?\nDeformable DETR is an enhancement of DETR, which was one of the first approaches to apply transformers to object detection. While DETR introduced a novel paradigm, it faced two major challenges\n1. Difficulty Detecting Small Objects\nMost modern object detection networks leverage Feature Pyramid Networks (FPN) to handle objects at multiple scales. However, DETR cannot easily incorporate FPN because its global self-attention operates over the entire feature map, making multi-scale attention computationally expensive. Deformable DETR addresses this by introducing multi-scale deformable attention, which selectively attends to a small set of key points across different feature levels instead of the entire map. This enables efficient multi-scale feature aggregation without exploding computational cost.\n2. Long Training Time\nDETR requires extensive training because the model must learn which parts of the feature map to attend to from scratch, which is slow to converge. Deformable DETR solves this by using a Deformable Attention Module, which focuses on a sparse set of relevant keys rather than all possible keys. This reduces complexity and accelerates convergence significantly."
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#convolutional-backbone",
    "href": "posts/2025-10-31-deformable_detr.html#convolutional-backbone",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Convolutional backbone",
    "text": "Convolutional backbone\n\n\npixel_values = torch.randn(4,3,1065,1066) \n\n# 1 .Initially we need to pass the images through the FPN and get features across different layers,\n# 2. Also we need to get positional embedding for each of the feature map, the positional embedding\n#  is similar to the normal sine-cosine positional embedding in the original paper,\n#  the only difference here is that since we have HxW in the feature domain , \n# suppose if our embedding dim is 256,\n#  we will have them alingned in such a way that the first 128 corresponds\n#  to vertical and the next 128 corresponds\n# to vertical so that in the end we end up with 256 and that encodes both vertical \n# and horizontal positions. https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55\n# 3. Suppose we get feature map from 4 layers and let \n# them be (4,512,134,134) ,(4,1024,67,67) , (4,2048,34,34) ,(4,2048,17,17) \n# [Note the actual feature map in the paper is created by \n# an additional conv+group norm] and there positional embeddings w\n# will have the same size as well. but with the corresponding embedding dim, \n# so they will be of size (4,256,134,134) ,(4,256,67,67) ,(4,256,34,34) ,(4,256,17,17)\n\n\nfeature_shapes = [\n    (4, 512, 134, 134),\n    (4, 1024, 67, 67),\n    (4, 2048, 34, 34),\n    (4, 2048, 17, 17)\n]\n\n# Positional embedding shapes (same spatial dims, but channel dim = 256)\nembedding_shapes = [\n    (4, 256, 134, 134),\n    (4, 256, 67, 67),\n    (4, 256, 34, 34),\n    (4, 256, 17, 17)\n]\n\n# original implementation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/backbone.py#L71\nfeature_maps = [torch.randn(shape) for shape in feature_shapes]\n\n# original implemenation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55\npositional_embeddings = [torch.randn(shape) for shape in embedding_shapes]\n\n# 4 . Now we have to have a 1x1 conv layer to reduce the channel dimension of the feature so that they match the embedding dimension of 256\nconv_layers = nn.ModuleList([\n    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n    nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n    nn.Conv2d(in_channels=2048, out_channels=256, kernel_size=1),\n    nn.Conv2d(in_channels=2048, out_channels=256, kernel_size=1)\n])\n\n# Apply the 1x1 conv layers\nreduced_feature_maps = [conv(feature) for conv, feature in zip(conv_layers, feature_maps)]\n\nfor i, (fmap,pos_emb) in enumerate(zip(reduced_feature_maps,positional_embeddings)):\n    print(f\"Reduced feature map {i+1} shape:\", fmap.shape)\n\nReduced feature map 1 shape: torch.Size([4, 256, 134, 134])\nReduced feature map 2 shape: torch.Size([4, 256, 67, 67])\nReduced feature map 3 shape: torch.Size([4, 256, 34, 34])\nReduced feature map 4 shape: torch.Size([4, 256, 17, 17])\n\n\n\n# 5 . Also we need a learnable Level embedding for each levels , since here we are using 4 layers, \n# and 256 embedding dim , the size of the level embedding will be (4,256)\n# Learnable level embedding (in actual model this would be nn.Parameter)\nlevel_embedding = torch.randn((4, 256))  # shape: (num_levels, embedding_dim)\n\n\n#6. Now we need to flatten and transpose the features and positional embedding \n# so they become the similar shape like token_len X embedding_dim , \n# for example the first feature map will become (4,134*134,256) ,similarly we have do this \n# for all the feature maps and the positional embedding. \n# and one additional thing to do is to add the level embedding to the positional embedding.\n\nfeatures_flatten = []\npositional_and_level_embedding_flattened = []\n\nfor level, (feature, pos_emb) in enumerate(zip(reduced_feature_maps, positional_embeddings)):\n    # Flatten and transpose: (B, C, H, W) -&gt; (B, HW, C)\n    feature_flatten = feature.flatten(2).transpose(1, 2)\n    positional_plus_level_embed = pos_emb.flatten(2).transpose(1, 2) + level_embedding[level].view(1, 1, -1)\n\n    features_flatten.append(feature_flatten)\n    positional_and_level_embedding_flattened.append(positional_plus_level_embed)\n\n    # Print shapes\n    print(f\"Level {level + 1}:\")\n    print(f\"  Feature shape: {feature_flatten.shape}\")\n    print(f\"  Positional + Level Embedding shape: {positional_plus_level_embed.shape}\")\n\n    \n\nLevel 1:\n  Feature shape: torch.Size([4, 17956, 256])\n  Positional + Level Embedding shape: torch.Size([4, 17956, 256])\nLevel 2:\n  Feature shape: torch.Size([4, 4489, 256])\n  Positional + Level Embedding shape: torch.Size([4, 4489, 256])\nLevel 3:\n  Feature shape: torch.Size([4, 1156, 256])\n  Positional + Level Embedding shape: torch.Size([4, 1156, 256])\nLevel 4:\n  Feature shape: torch.Size([4, 289, 256])\n  Positional + Level Embedding shape: torch.Size([4, 289, 256])\n\n\n\n# Step 7: Concatenate along sequence dimension (dim=1)\ninputs_embeds = torch.cat(features_flatten, dim=1)  # shape: (B, total_seq_len, 256)\nposition_embeddings = torch.cat(positional_and_level_embedding_flattened, dim=1)  # shape: (B, total_seq_len, 256)\n\nprint(\"Concatenated Inputs Embeds shape:\", inputs_embeds.shape)\nprint(\"Concatenated Position Embeddings shape:\", position_embeddings.shape)\n\nConcatenated Inputs Embeds shape: torch.Size([4, 23890, 256])\nConcatenated Position Embeddings shape: torch.Size([4, 23890, 256])"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#encoder",
    "href": "posts/2025-10-31-deformable_detr.html#encoder",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Encoder",
    "text": "Encoder\n\n# 8. we need to apply a initial dropout before passing it to the encoder \ninputs_embeds = nn.functional.dropout(inputs_embeds, p=0.1)\nbatch_size = inputs_embeds.shape[0]\n#9. Generating the reference points, so this is a concept that is similar to the deformable convolution ,\n# so basically for each feature_point/query\n# in the feature map we need to look into the corresponding point in the other feature\n# map as well, feature maps a re normilized  based on their height and width, \n# so we can look for the corresponding point for each query in different points as well, here\n#original implemenation https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/deformable_transformer.py#L238 \nspatial_shapes_list = [(134, 134), (67, 67), (34, 34), (17, 17)]\n\nreference_points_list = []\nfor H_, W_ in spatial_shapes_list:\n        # Create meshgrid of normalized coordinates\n        ref_y, ref_x = torch.meshgrid(\n            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32),\n            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32),\n            indexing='ij'  # Important for correct axis ordering\n        )\n        # Normalize\n        ref_y = ref_y.reshape(-1) / H_\n        ref_x = ref_x.reshape(-1) / W_\n\n        # Stack and expand to batch size\n        ref = torch.stack((ref_x, ref_y), dim=-1)  # shape: (H_*W_, 2)\n        ref = ref[None].expand(batch_size, -1, -1)  # shape: (B, H_*W_, 2)\n        reference_points_list.append(ref)\n\n\n# Concatenate all levels\nreference_points = torch.cat(reference_points_list, dim=1)  # shape: (B, total_seq_len, 2)\n# Expand to include level dimension\nreference_points = reference_points[:, :, None, :]  # shape: (B, total_seq_len, 1, 2)\n\n# Repeat across levels\nnum_levels = len(spatial_shapes_list)\nreference_points = reference_points.expand(-1, -1, num_levels, -1)  # shape: (B, total_seq_len, L, 2)\nprint(\"Reference points shape input to encoder \",reference_points.shape)\n\n\nReference points shape input to encoder  torch.Size([4, 23890, 4, 2])\n\n\n\n#so for now each query we have 4 positions (x,y) across 4 different channels, now this will be passed to the encoder.\n## DEFORMABLE MULTI SCALE ATTENTION.\n\n# params\nnum_heads = 8\nnum_levels  = 4\nn_points  = 4\nembdedding_dim = inputs_embeds.shape[-1]\nbatch_size, num_queries, _ = inputs_embeds.shape\n\nfc1 = nn.Linear(embdedding_dim, 512)\nfc2 = nn.Linear(512, embdedding_dim)\n\nlayer_norm1 = nn.LayerNorm(embdedding_dim)\nlayer_norm2 = nn.LayerNorm(embdedding_dim)\n\nfinal_layer_norm = nn.LayerNorm(embdedding_dim)\n\n# learnable parameters in the layer\nsampling_offsets_layer = nn.Linear(embdedding_dim, num_heads * num_levels * n_points *2)\nattention_weights_layer = nn.Linear(embdedding_dim,num_heads * num_levels * n_points)\nvalue_projection_layer  = nn.Linear(embdedding_dim,embdedding_dim)\noutput_projection_layer = nn.Linear(embdedding_dim,embdedding_dim)\n\n#initially we add the poistional_embedding to the input_embeds\nhidden_states = inputs_embeds + position_embeddings\nvalue = value_projection_layer(inputs_embeds)\nvalue = value.view(batch_size,num_queries, num_heads,embdedding_dim//num_heads)\nprint(f\"Value shape = {value.shape}\")\n\n# note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.\nsampling_offsets = sampling_offsets_layer(hidden_states)\nsampling_offsets = sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,2)\n\n# sampling_offsets are predicted in a normalized, unitless space (not tied to any particular feature map size).\n# Each feature map (level) can have a different spatial resolution (height, width).\n# To convert the offsets into actual positions on each feature map, they must be scaled relative to that map's size.\n\noffset_normalizer = torch.tensor(spatial_shapes_list)\noffset_normalizer = offset_normalizer[None,None,None,:,None,:]\nsampling_offsets = sampling_offsets/offset_normalizer\nprint(f\"Offset Normalizer {offset_normalizer.shape}\")\nattention_weights = attention_weights_layer(hidden_states)\nattention_weights = attention_weights.view(batch_size,num_queries,num_heads,num_levels*n_points)\n# note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels\nattention_weights = torch.nn.functional.softmax(attention_weights, -1).view(batch_size,num_queries,num_heads,num_levels,n_points) \nprint(f\"Sampling offset shape = {sampling_offsets.shape}\")\nprint(f\"Attention weights shape = {attention_weights.shape} \\n\")\n\n# Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points ,\n# we need to look into 4 more points across 8 different heads\n# so initially we had for each query 1 points account each feature dimension making it total 4 and\n#  now when we add this sampling offsets it makes 4 more across 8 differenet heads\nreference_points = reference_points[:,:,None,:,None,:]\nprint(f\"Reference points with unsqueezed dimension for head and levels = {reference_points.shape}\")\nsampling_location = reference_points + sampling_offsets\nprint(f\"Final sampling locations = {sampling_location.shape}\")\n\nValue shape = torch.Size([4, 23890, 8, 32])\nOffset Normalizer torch.Size([1, 1, 1, 4, 1, 2])\nSampling offset shape = torch.Size([4, 23890, 8, 4, 4, 2])\nAttention weights shape = torch.Size([4, 23890, 8, 4, 4]) \n\nReference points with unsqueezed dimension for head and levels = torch.Size([4, 23890, 1, 4, 1, 2])\nFinal sampling locations = torch.Size([4, 23890, 8, 4, 4, 2])\n\n\n\nDeformable Attention\n\n\n\n# Split the value tensor into per-level chunks based on spatial shapes\nvalue_list = value.split([h * w for h, w in spatial_shapes_list], dim=1)\nbatch_size, _, num_heads, hidden_dim = value.shape\n\n# Print the shape of each level's value tensor\nfor level, feature in enumerate(value_list):\n    print(f\"Splitted feature at level {level} --&gt; {feature.shape}\")\n\n# Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample\nsampling_grids = 2 * sampling_location - 1\nprint(f\"\\nSampling grid shape  = {sampling_grids.shape} \\n\")\n\nsampling_value_list = []\n\nfor level_id, (height, width) in enumerate(spatial_shapes_list):\n    # Reshape value tensor for grid sampling:\n    # (B, H*W, num_heads, C) → (B, num_heads, H*W, C) → (B*num_heads, C, H, W)\n    value_l = (\n        value_list[level_id]\n        .flatten(2)               # (B, H*W, num_heads * C)\n        .transpose(1, 2)          # (B, num_heads * C, H*W)\n        .reshape(batch_size * num_heads, hidden_dim, height, width)\n    )\n    print(f\"Value at level {level_id} {value_l.shape}\")\n\n    # Reshape sampling grid:\n    # (B, num_queries, num_heads, num_levels, num_points, 2)\n    # → (B, num_heads, num_queries, num_points, 2)\n    # → (B*num_heads, num_queries, num_points, 2)\n    sampling_grid_l = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n\n    # Sample values using bilinear interpolation\n    sampling_value_l = nn.functional.grid_sample(\n        value_l,\n        sampling_grid_l,\n        mode=\"bilinear\",\n        padding_mode=\"zeros\",\n        align_corners=False,\n    )\n\n    sampling_value_list.append(sampling_value_l)\n\n\nSplitted feature at level 0 --&gt; torch.Size([4, 17956, 8, 32])\nSplitted feature at level 1 --&gt; torch.Size([4, 4489, 8, 32])\nSplitted feature at level 2 --&gt; torch.Size([4, 1156, 8, 32])\nSplitted feature at level 3 --&gt; torch.Size([4, 289, 8, 32])\n\nSampling grid shape  = torch.Size([4, 23890, 8, 4, 4, 2]) \n\nValue at level 0 torch.Size([32, 32, 134, 134])\nValue at level 1 torch.Size([32, 32, 67, 67])\nValue at level 2 torch.Size([32, 32, 34, 34])\nValue at level 3 torch.Size([32, 32, 17, 17])\n\n\n\nfor f in sampling_value_list:\n    print(f.shape)\n\ntorch.Size([32, 32, 23890, 4])\ntorch.Size([32, 32, 23890, 4])\ntorch.Size([32, 32, 23890, 4])\ntorch.Size([32, 32, 23890, 4])\n\n\n\nfinal_key_matrix = torch.stack(sampling_value_list, dim=-2)\nprint(f\"Stacked value matrix shape before flattening = {final_key_matrix.shape}\")\nfinal_key_matrix = final_key_matrix.flatten(-2)\nprint(f\"Stacked value matrix shape after flattening = {final_key_matrix.shape}\")\n\nStacked value matrix shape before flattening = torch.Size([32, 32, 23890, 4, 4])\nStacked value matrix shape after flattening = torch.Size([32, 32, 23890, 16])\n\n\n\nattention_weights = attention_weights.transpose(1, 2).reshape(\n            batch_size * num_heads, 1, num_queries, num_levels * n_points\n        )\nattention_weights.shape\n\ntorch.Size([32, 1, 23890, 16])\n\n\n\noutput = final_key_matrix*attention_weights\noutput.shape\n\ntorch.Size([32, 32, 23890, 16])\n\n\n\noutput = output.sum(-1)\noutput.shape\n\ntorch.Size([32, 32, 23890])\n\n\n\noutput = output.view(batch_size,num_heads*hidden_dim,num_queries).transpose(1,2)\noutput = output_projection_layer(output)\noutput.shape\n\ntorch.Size([4, 23890, 256])\n\n\n\n# Feed forward layers \nhidden_states = nn.functional.dropout(hidden_states,p=0.1)\nhidden_states = inputs_embeds + hidden_states # residual\nhidden_states = layer_norm1(hidden_states)\n\nresidual = hidden_states\nhidden_states = nn.ReLU()(fc1(hidden_states))\nhidden_states = nn.functional.dropout(hidden_states,p=0.1)\nhidden_states = fc2(hidden_states)\nhidden_states  = nn.functional.dropout(hidden_states,p=0.1)\n\nhidden_states = residual+hidden_states\nhidden_states = layer_norm2(hidden_states)\n\n\nhidden_states.shape\n\ntorch.Size([4, 23890, 256])"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#decoder",
    "href": "posts/2025-10-31-deformable_detr.html#decoder",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Decoder",
    "text": "Decoder\n\nencoder_output = hidden_states.clone()\nnum_query = 300\nembedding_dim = encoder_output.shape[-1]\nnum_levels\n# Learnable query and positional embeddings\nposition_embeddings = nn.Parameter(torch.randn(num_query, embedding_dim)) #(num_query,embedding_dim)\nposition_embeddings = position_embeddings[None].expand(batch_size,-1,-1) # (batch_size,num_query,embedding_dim)\ninput_query = nn.Parameter(torch.randn(num_query, embedding_dim)) #(num_query,embedding_dim)\ninput_query = input_query[None].expand(batch_size,-1,-1) # (batch_size,num_query,embedding_dim) \n\nfc1 = nn.Linear(embdedding_dim, 512)\nfc2 = nn.Linear(512, embdedding_dim)\n\nlayer_norm1 = nn.LayerNorm(embdedding_dim)\nlayer_norm2 = nn.LayerNorm(embdedding_dim)\nlayer_norm3 = nn.LayerNorm(embedding_dim)\n\n# Linear layer to generate reference points from positional embeddings\ndecoder_reference_point_layer = nn.Linear(embedding_dim, 2)\n\n# Generate normalized reference points in [0, 1] range\nreference_points = decoder_reference_point_layer(position_embeddings).sigmoid()  # shape: (num_query, 2)\nprint(f\"Encode Reference points shape {reference_points.shape}\")\n\nEncode Reference points shape torch.Size([4, 300, 2])\n\n\n\nreference_points_input = reference_points[:,:,None,:].expand(batch_size,num_query,num_levels,2)\nreference_points_input.shape\n\ntorch.Size([4, 300, 4, 2])\n\n\n\n# Initially here we will have the normal self attention.\nresidual = input_query\nmultihead_attn = nn.MultiheadAttention(embedding_dim, num_heads)\nself_attn_output, _ = multihead_attn(input_query+position_embeddings, input_query+position_embeddings, input_query)\nhidden_state_after_self_attention = self_attn_output + residual # residual connection. \nhidden_state_after_self_attention = layer_norm1(hidden_state_after_self_attention)\nsecond_residual = hidden_state_after_self_attention \nprint(f\"Hidden state shape input to cross attention  {self_attn_output.shape}\")\n\nHidden state shape input to cross attention  torch.Size([4, 300, 256])\n\n\n\nposition_embeddings.shape,hidden_state_after_self_attention.shape,encoder_output.shape\n\n(torch.Size([4, 300, 256]),\n torch.Size([4, 300, 256]),\n torch.Size([4, 23890, 256]))\n\n\n\n## DEFORMABLE MULTI SCALE ATTENTION.\nnum_heads = 8\nnum_levels  = 4\nn_points  = 4\nembdedding_dim = hidden_state_after_self_attention.shape[-1]\nbatch_size, num_queries, _ = hidden_state_after_self_attention.shape\n\n# learnable parameters in the layer\nsampling_offsets_layer = nn.Linear(embdedding_dim, num_heads * num_levels * n_points *2)\nattention_weights_layer = nn.Linear(embdedding_dim,num_heads * num_levels * n_points)\nvalue_projection_layer  = nn.Linear(embdedding_dim,embdedding_dim)\noutput_projection_layer = nn.Linear(embdedding_dim,embdedding_dim)\n\n#initially we add the poistional_embedding to the input_embeds\nhidden_states = hidden_state_after_self_attention + position_embeddings\nvalue = value_projection_layer(encoder_output)\n_,encoder_sequence_length,_ = value.shape\nvalue = value.view(batch_size,encoder_sequence_length, num_heads,embdedding_dim//num_heads)\nprint(f\"Value shape = {value.shape}\")\n\n# note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.\nsampling_offsets = sampling_offsets_layer(hidden_states)\nsampling_offsets = sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,2) \n# sampling_offsets are predicted in a normalized, unitless space (not tied to any particular feature map size).\n# Each feature map (level) can have a different spatial resolution (height, width).\n# To convert the offsets into actual positions on each feature map, they must be scaled relative to that map's size.\n\noffset_normalizer = torch.tensor(spatial_shapes_list)\noffset_normalizer = offset_normalizer[None,None,None,:,None,:]\nsampling_offsets = sampling_offsets/offset_normalizer\nprint(f\"Offset Normalizer {offset_normalizer.shape}\")\n\nattention_weights = attention_weights_layer(hidden_states)\nattention_weights = attention_weights.view(batch_size,num_queries,num_heads,num_levels*n_points)\n# note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels\nattention_weights = torch.nn.functional.softmax(attention_weights, -1).view(batch_size,num_queries,num_heads,num_levels,n_points) \nprint(f\"Sampling offset shape = {sampling_offsets.shape}\")\nprint(f\"Attention weights shape = {attention_weights.shape} \\n\")\n\n\n# Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points ,\n# we need to look into 4 more points across 8 different heads\n# so initially we had for each query 1 points account each feature dimension making it total 4 and \n# now when we add this sampling offsets it makes 4 more across 8 differenet heads\nreference_points_input = reference_points_input[:,:,None,:,None,:]\nprint(f\"Reference points with unsqueezed dimension for head and levels = {reference_points_input.shape}\")\nsampling_location = reference_points_input + sampling_offsets\nprint(f\"Final sampling locations = {sampling_location.shape}\")\n\nValue shape = torch.Size([4, 23890, 8, 32])\nOffset Normalizer torch.Size([1, 1, 1, 4, 1, 2])\nSampling offset shape = torch.Size([4, 300, 8, 4, 4, 2])\nAttention weights shape = torch.Size([4, 300, 8, 4, 4]) \n\nReference points with unsqueezed dimension for head and levels = torch.Size([4, 300, 1, 4, 1, 2])\nFinal sampling locations = torch.Size([4, 300, 8, 4, 4, 2])\n\n\n\nDeformable Attention\n\n# Split the value tensor into per-level chunks based on spatial shapes\nvalue_list = value.split([h * w for h, w in spatial_shapes_list], dim=1)\nbatch_size, _, num_heads, hidden_dim = value.shape\n\n# Print the shape of each level's value tensor\nfor level, feature in enumerate(value_list):\n    print(f\"Splitted feature at level {level} --&gt; {feature.shape}\")\n\n# Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample\nsampling_grids = 2 * sampling_location - 1\nprint(f\"\\nSampling grid shape  = {sampling_grids.shape} \\n\")\n\nsampling_value_list = []\n\nfor level_id, (height, width) in enumerate(spatial_shapes_list):\n    # Reshape value tensor for grid sampling:\n    # (B, H*W, num_heads, C) → (B, num_heads, H*W, C) → (B*num_heads, C, H, W)\n    value_l = (\n        value_list[level_id]\n        .flatten(2)               # (B, H*W, num_heads * C)\n        .transpose(1, 2)          # (B, num_heads * C, H*W)\n        .reshape(batch_size * num_heads, hidden_dim, height, width)\n    )\n    print(f\"Value at level {level_id} {value_l.shape}\")\n\n    # Reshape sampling grid:\n    # (B, num_queries, num_heads, num_levels, num_points, 2)\n    # → (B, num_heads, num_queries, num_points, 2)\n    # → (B*num_heads, num_queries, num_points, 2)\n    sampling_grid_l = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n\n    # Sample values using bilinear interpolation\n    sampling_value_l = nn.functional.grid_sample(\n        value_l,\n        sampling_grid_l,\n        mode=\"bilinear\",\n        padding_mode=\"zeros\",\n        align_corners=False,\n    )\n\n    sampling_value_list.append(sampling_value_l)\n\nSplitted feature at level 0 --&gt; torch.Size([4, 17956, 8, 32])\nSplitted feature at level 1 --&gt; torch.Size([4, 4489, 8, 32])\nSplitted feature at level 2 --&gt; torch.Size([4, 1156, 8, 32])\nSplitted feature at level 3 --&gt; torch.Size([4, 289, 8, 32])\n\nSampling grid shape  = torch.Size([4, 300, 8, 4, 4, 2]) \n\nValue at level 0 torch.Size([32, 32, 134, 134])\nValue at level 1 torch.Size([32, 32, 67, 67])\nValue at level 2 torch.Size([32, 32, 34, 34])\nValue at level 3 torch.Size([32, 32, 17, 17])\n\n\n\nfor i,f in enumerate(sampling_value_list):\n    print(f\"Sampling points from each layer {i} {f.shape}\")\n\nSampling points from each layer 0 torch.Size([32, 32, 300, 4])\nSampling points from each layer 1 torch.Size([32, 32, 300, 4])\nSampling points from each layer 2 torch.Size([32, 32, 300, 4])\nSampling points from each layer 3 torch.Size([32, 32, 300, 4])\n\n\n\nfinal_key_matrix = torch.stack(sampling_value_list, dim=-2)\nprint(f\"Stacked value matrix shape before flattening = {final_key_matrix.shape}\")\nfinal_key_matrix = final_key_matrix.flatten(-2)\nprint(f\"Stacked value matrix shape after flattening = {final_key_matrix.shape}\")\n\nStacked value matrix shape before flattening = torch.Size([32, 32, 300, 4, 4])\nStacked value matrix shape after flattening = torch.Size([32, 32, 300, 16])\n\n\n\nattention_weights = attention_weights.transpose(1, 2).reshape(\n            batch_size * num_heads, 1, num_queries, num_levels * n_points\n        )\nattention_weights.shape\n\ntorch.Size([32, 1, 300, 16])\n\n\n\noutput = final_key_matrix*attention_weights\nprint(f\"Output after attention {output.shape}\")\noutput = output.sum(dim=-1)\nprint(f\"Final output after summation {output.shape}\")\noutput = output.view(batch_size,num_heads*hidden_dim,num_queries).transpose(1,2)\nprint(f\" Output reshaped --&gt; {output.shape}\")\noutput = output_projection_layer(output)\n\nOutput after attention torch.Size([32, 32, 300, 16])\nFinal output after summation torch.Size([32, 32, 300])\n Output reshaped --&gt; torch.Size([4, 300, 256])\n\n\n\noutput.shape\n\ntorch.Size([4, 300, 256])\n\n\n\nhidden_states = nn.functional.dropout(output,p=0.1)\nhidden_states = second_residual + hidden_states\nhidden_states = layer_norm2(hidden_states)\n\n# Fully connected\nresidual = hidden_states\nhidden_states = nn.ReLU()(fc1(hidden_states))\nhidden_states = fc2(hidden_states)\nhidden_states = hidden_states + residual\nhidden_states = layer_norm3(hidden_states)\n\n\nencoder_output = hidden_states.clone()\nencoder_output.shape\n\ntorch.Size([4, 300, 256])"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#final-box-and-class-prediction",
    "href": "posts/2025-10-31-deformable_detr.html#final-box-and-class-prediction",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Final Box and class prediction",
    "text": "Final Box and class prediction\n\n# This is needed because  model predicts an offset in unconstrained space \n# By applying inverse_sigmoid(reference_points), we map the reference points from [0, 1] to \n# unconstrained space,which is  the same space as the predicted offset.\n# Then bring it back to the constrained space , by appling sigmoid, this making learning faster.\n\nreference_points_with_inverse_sigmoid = torch.special.logit(encoder_output)\n\n\n# This is needed because  model predicts an offset in unconstrained space \n# By applying inverse_sigmoid(reference_points), we map the reference points from [0, 1] to unconstrained space,\n# which is  the same space as the predicted offset.\n# Then bring it back to the constrained space , by appling sigmoid, this making learning faster.\n\nreference_points_with_inverse_sigmoid = torch.special.logit(reference_points)\n\n# say we have 10 classes\nnum_class = 10 \nclass_pred = nn.Linear(embdedding_dim,num_class)\nbox_head  = nn.Sequential(\n    nn.Linear(embdedding_dim, 512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512, 4),\n)\n\noutput_classes = class_pred(encoder_output)\nbox_pred = box_head(encoder_output)\nbox_pred[...,:2] += reference_points_with_inverse_sigmoid\npred_boxes = box_pred.sigmoid()\nprint(f\"Final box head shape {output_classes.shape}\")\nprint(f\"Final pred boxes head shape {pred_boxes.shape} \")\n\nFinal box head shape torch.Size([4, 300, 10])\nFinal pred boxes head shape torch.Size([4, 300, 4]) \n\n\n\n# Losses are similart to DETR, only difference is Deformable detr uses focal loss for  classification and for pred boxes,\n# it uses the same loss like DETR where the losses are a combination of l1 loss and Generalized IOU loss"
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#deformable-attention-compute-and-memory-complexity",
    "href": "posts/2025-10-31-deformable_detr.html#deformable-attention-compute-and-memory-complexity",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Deformable Attention: Compute and Memory Complexity",
    "text": "Deformable Attention: Compute and Memory Complexity\nThe paper have a good summary of the complexity of the computation here I will show it bit in detail, specializing to the encoder and decoder settings, and show how \\(L\\) (levels) and \\(K\\) (points per level) enter the formulas.\n\nNotation\n\n\\(N_q\\): number of queries\n\n\\(N_k\\): number of keys\n\n\\(M\\): number of heads\n\n\\(C\\): channel dimension\n\n\\(C_v = C/M\\): per-head dimension\n\n\\(H \\times W\\): spatial size of a single feature map\n\n\\(H_l \\times W_l\\): spatial size at level \\(l\\)\n\n\\(S = \\sum_{l=1}^L H_l W_l\\): total token count across levels\n\n\\(K\\): sampled points per head per level\n\n\\(L\\): number of feature levels\n\n\n\n0) Preliminaries: What Contributes to Cost?\nFor any attention block, there are four compute buckets:\n\nLinear projections to form \\(Q, K, V\\) (and the output projection): costs scale like \\(\\mathcal{O}(N C^2)\\).\nScore computation (e.g., \\(QK^\\top\\) or its sparse substitute): costs scale like \\(\\mathcal{O}(N_q N_k C_v M) = \\mathcal{O}(N_q N_k C)\\) for dense attention.\nSoftmax + weighting: typically \\(\\mathcal{O}(N_q N_k M)\\) for softmax, and \\(\\mathcal{O}(N_q N_k C)\\) for multiplying by \\(V\\); the latter usually dominates.\nSampling / Interpolation (deformable attention only): adds a term of approximately \\(\\mathcal{O}(N_q \\cdot \\text{\\#samples} \\cdot C)\\); Appendix of the paper counts this as a constant “5” times per sample for bilinear interpolation + reduct\n\nMemory is dominated by storing the attention weights: \\(\\mathcal{O}(N_q N_k M)\\) for dense vs. \\(\\mathcal{O}(N_q M K)\\) (single-scale) or \\(\\mathcal{O}(N_q M L K)\\) (multi-scale).\n\n\n\n1) Standard Multi-Head Attention (Expression is directly from the paper Eq1)\n\\[\n\\text{MultiHeadAttn}(z_q, x) = \\sum_{m=1}^M W_m \\sum_{k \\in \\mathcal{K}} A_{mqk} W'_m x_k\n\\]\nCompute:\n\nProjections:\n\n\\(Q\\): \\(\\mathcal{O}(N_q C^2)\\)\n\\(K, V\\): \\(\\mathcal{O}(N_k C^2)\\)\n\nScores (\\(QK^\\top\\)): \\(\\mathcal{O}(M \\cdot N_q N_k C_v) = \\mathcal{O}(N_q N_k C)\\)\nSoftmax: \\(\\mathcal{O}(M \\cdot N_q N_k)\\)\nWeighted sum (AV): \\(\\mathcal{O}(M \\cdot N_q N_k C_v) = \\mathcal{O}(N_q N_k C)\\)\nOutput projection: \\(\\mathcal{O}(N_q C^2)\\)\n\nTotal (dense attention):\n\\[\n\\boxed{\\mathcal{O}\\big(N_q C^2 + N_k C^2 + N_q N_k C\\big)}\n\\]\nMemory:\n\nAttention weights: \\(\\mathcal{O}(M N_q N_k)\\) (dominant)\nKey/value caches: \\(\\mathcal{O}(N_k C)\\)\n\nSpecializations:\n\nDETR encoder (self-attention over pixels): \\(N_q = N_k = S\\)\n\\[\n  \\mathcal{O}(S^2 C) + \\mathcal{O}(S C^2) \\quad \\text{(dominated by $S^2C$)}\n  \\]\nDETR decoder cross-attention: \\(N_q = N\\) queries, \\(N_k = S\\) pixels\n\\[\n  \\mathcal{O}(N S C) + \\mathcal{O}((N+S)C^2)\n  \\]\nDETR decoder self-attention (queries only):\n\\[\n  \\mathcal{O}(2 N C^2 + N^2 C)\n  \\]\n\n\n\n\n2) Single-Scale Deformable Attention (Expression is directly from the paper Eq2)\n\\[\n\\text{DeformAttn}(z_q, p_q, x) = \\sum_{m=1}^M W_m \\sum_{k=1}^K A_{mqk} W'_m x(p_q + p_{mqk})\n\\]\nEach query attends \\(K\\) sampled points per head around reference \\(p_q\\). Sampling uses bilinear interpolation.\n\nPredict offsets + weights (a single linear with \\(3MK\\) output channels over \\(z_q\\)): \\(\\mathcal{O}(3 N_q C M K)\\)\nValue projection (\\(W'_m x\\)): two possible ways\n\nPrecompute once on the whole map: \\(\\mathcal{O}(H W C^2)\\)\nOr do per sampled value: \\(\\mathcal{O}(N_q K C^2)\\)\n\nSampling + weighted sum (bilinear + reduce): approx 5 ops per sample per channel: \\(\\mathcal{O}(5 N_q K C)\\)\nOutput projection: \\(\\mathcal{O}(N_q C^2)\\)\n\nPutting it together (App. A.1):\n\\[\n\\boxed{\n\\mathcal{O}\\Big(N_q C^2 + \\min(H W C^2, N_q K C^2) + 5 N_q K C + 3 N_q C M K\\Big)\n}\n\\]\nFor typical settings (\\(M=8\\), \\(K \\leq 4\\), \\(C=256\\)), the paper notes \\(5K + 3MK \\ll C\\), yielding the simplification:\n\\[\n\\boxed{\\mathcal{O}\\big(2 N_q C^2 + \\min(H W C^2, N_q K C^2)\\big)}\n\\]\nMemory:\n\nAttention weights: \\(\\mathcal{O}(M N_q K)\\)\nOffsets: \\(\\mathcal{O}(M N_q K \\cdot 2)\\)\nNo dense \\((N_q \\times N_k)\\) matrix—this is the major win.\n\nSpecializations:\n\nEncoder (single-scale, queries are pixels): \\(N_q = HW\\)\nWith precomputation (\\(W'_m x\\)): complexity becomes \\(\\mathcal{O}(HW C^2)\\), i.e. linear in spatial size (vs. quadratic for dense).\nDecoder cross-attention (single-scale): \\(N_q = N\\)\nWith per-query sampled values: \\(\\mathcal{O}(N K C^2)\\) (independent of \\(HW\\)).\n\n\n\n\n3) Multi-Scale Deformable Attention ( Expression is directly from the paper Eq. (3))\n\\[\n\\text{MSDeformAttn}(z_q, \\hat{p}_q, \\{x_l\\}_{l=1}^L)\n= \\sum_{m=1}^M W_m \\sum_{l=1}^L \\sum_{k=1}^K\nA_{mlqk} W'_m x_l(\\phi_l(\\hat{p}_q) + p_{mlqk})\n\\]\nEach query samples \\((L \\times K)\\) points total.\nCompute:\n\nPredict offsets + weights: \\(\\mathcal{O}(3 N_q C M L K)\\)\nValue projections (choose one):\n\nPrecompute on all levels: \\(\\sum_{l=1}^L \\mathcal{O}(H_l W_l C^2) = \\mathcal{O}(S C^2)\\)\nOr per sampled value: \\(\\mathcal{O}(N_q L K C^2)\\)\n\nSampling + weighted sum: \\(\\mathcal{O}(5 N_q L K C)\\)\nOutput projection: \\(\\mathcal{O}(N_q C^2)\\)\n\nTotal (multi-scale):\n\\[\n\\boxed{\n\\mathcal{O}\\Big(N_q C^2 + \\min(S C^2, N_q L K C^2) + 5 N_q L K C + 3 N_q C M L K\\Big)\n}\n\\]\nUnder the same “small \\((M, K, L)\\)” assumption as the paper (App. A.1):\n\\[\n\\boxed{\n\\mathcal{O}\\big(2 N_q C^2 + \\min(S C^2, N_q L K C^2)\\big)\n}\n\\]\nMemory:\n\nAttention weights: \\(\\mathcal{O}(M N_q L K)\\)\nOffsets: \\(\\mathcal{O}(M N_q L K \\cdot 2)\\)\nAgain, no dense \\((N_q \\times S)\\) matrix.\n\nSpecializations:\n\nDeformable DETR encoder (multi-scale, queries are pixels across all levels): \\(N_q = S\\)\nPrecompute values per level \\(\\rightarrow\\) \\[\n\\boxed{\\mathcal{O}(S C^2)} \\quad \\text{(linear in total tokens across scales)}\n\\] This is the paper’s claim that encoder complexity becomes linear in spatial size (Section 4.1).\nDeformable DETR decoder cross-attention: \\(N_q = N\\) queries\nUse per-query samples \\(\\rightarrow\\) \\[\n\\boxed{\\mathcal{O}(N L K C^2)} \\quad \\text{(independent of spatial resolution)}\n\\]\nDecoder self-attention: unchanged from standard: \\(\\mathcal{O}(2 N C^2 + N^2 C)\\).\n\n\n\n\n4) Side-by-side Summary (Dominant Terms)\n\n\n\n\n\n\n\n\n\nBlock\nDense MHA (DETR)\nDeformable (single-scale)\nDeformable (multi-scale)\n\n\n\n\nGeneric\n\\(\\mathcal{O}(N_q C^2 + N_k C^2 + N_q N_k C)\\)\n\\(\\mathcal{O}(2 N_q C^2 + \\min(HW C^2, N_q K C^2))\\)\n\\(\\mathcal{O}(2 N_q C^2 + \\min(S C^2, N_q L K C^2))\\)\n\n\nEncoder\n\\(N_q = N_k = S \\Rightarrow \\mathcal{O}(S^2 C)\\)\n\\(N_q = HW \\Rightarrow \\mathcal{O}(HW C^2)\\)\n\\(N_q = S \\Rightarrow \\boxed{\\mathcal{O}(S C^2)}\\)\n\n\nDecoder cross-attn\n\\(N_q = N, N_k = S \\Rightarrow \\mathcal{O}(N S C)\\)\n\\(\\mathcal{O}(N K C^2)\\)\n\\(\\boxed{\\mathcal{O}(N L K C^2)}\\)\n\n\nDecoder self-attn\n\\(\\mathcal{O}(2 N C^2 + N^2 C)\\)\nsame\nsame\n\n\nAttention memory\n\\(\\mathcal{O}(M N_q N_k)\\)\n\\(\\mathcal{O}(M N_q K)\\)\n\\(\\mathcal{O}(M N_q L K)\\)\n\n\n\n\n\n\n5) Practical Takeaways\n\nEncoder: dense self-attention is quadratic in spatial tokens; deformable makes it linear in the total number of tokens across scales (\\(S\\)).\nDecoder cross-attention: deformable cost depends on \\((L K)\\) (small, fixed hyperparameters), not on image size, so it scales with the number of queries (\\(N\\)) and channel dimension (\\(C\\)), not with \\(H, W\\).\nMemory: deformable avoids the \\(\\mathcal{O}(N_q N_k)\\) attention matrix, replacing it with \\(\\mathcal{O}(N_q L K)\\) structures—crucial for speed and convergence."
  },
  {
    "objectID": "posts/2025-10-31-deformable_detr.html#credits",
    "href": "posts/2025-10-31-deformable_detr.html#credits",
    "title": "Deformable DETR Code WalkThrough",
    "section": "Credits",
    "text": "Credits\nMost of the implementation closely follow the below two, so all credtits to them!!\n\nOriginal Detr Paper Implementation\nHugging Face Transformers"
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html",
    "title": "Simple Online RealTime Tracking",
    "section": "",
    "text": "This is an overview of the implementation details of SORT tracking algorithm. The official implemenation of the paper is present in this repo . The paper pretty much explains it straightforward , i will be more walking through the implemenation details. In a top level SORT is a tracking algorithm which falls in the class of tracking by detection and the detection, assoaciation , tracking cycle is happening in the 2D image domain."
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html#overview",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html#overview",
    "title": "Simple Online RealTime Tracking",
    "section": "",
    "text": "This is an overview of the implementation details of SORT tracking algorithm. The official implemenation of the paper is present in this repo . The paper pretty much explains it straightforward , i will be more walking through the implemenation details. In a top level SORT is a tracking algorithm which falls in the class of tracking by detection and the detection, assoaciation , tracking cycle is happening in the 2D image domain."
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html#detection",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html#detection",
    "title": "Simple Online RealTime Tracking",
    "section": "Detection",
    "text": "Detection\nSort is a tracking by detection algorithm. So the quality of the tracking will inturn depends on the quality of detector. In the officiall implementation repo, the author have already porvided detections from the MOT Benchmark. So in the implementation present in the repo we can treat the detector as a blackbox and use the detection information already present.\n\nMotion Model and why we need it\nSo we get detections in the current frame and we need to some how associate it to the detections from the previous frame. Seems like a place where we can put a Kalman filter into good use. So we use a kalman filter with constant velocity motion model and then we will treat the detections as measurements. The official implementation uses filterpy which is a python library with different kalman filter implementations.\n\nState Variables in the constant velocity model\nSo what are the state variables\n\nu –&gt; the horizontal pixel location of center of target\nv –&gt; the vertical pixel location of the center of target\ns –&gt; area (width_bbox*height_bbox)\nr –&gt; aspect ration (width_bbox/height_bbox)\n\nWe assume a constant velocity model and also assume the aspect ratio also remains constant, our process and measurement models will be based on that.\nThe state variables are [u,v,s,r,u_dot,v_dot,s_dot] where u_dot,v_dot and s_dot represent the corresponding velocities. Since we are assuming a constant velocity model we can use the normal newtons equations.\n\nu = u + u_dot * t\nv = v + v_dot * t\ns = s + s_dot * t\nr = r\nu_dot = u_dot\nv_dot = v_dot\ns_dot = s_dot\n\nThe final process model will look like\nx_(t+1) = F * x_(t) + ProcessNoise , For constant velocity model like above the process model will look something like shown below. As its shown from the output of the dot product, we get what we expected\n\nfrom sympy import symbols , Matrix\nu,v,s,r,u_dot,v_dot,s_dot = symbols('u,v,s,r,u_dot,v_dot,s_dot')\nF  = Matrix([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\nx_ = Matrix([u,v,s,r,u_dot,v_dot,s_dot])\nout = F.dot(x_)\n\n\nF\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1\\end{matrix}\\right]\\)\n\n\n\nout\n\n[u + u_dot, v + v_dot, s + s_dot, r, u_dot, v_dot, s_dot]\n\n\n\n\nMeasurement Model\nWe use the variables u,v,s and r as measurements. We get these from the bounding box coordinates of each detections. So we need the measurement model to convert form the state space to the measurement space and the model is very simple 4x7 matrix with\n\nH = Matrix([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\nH\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nout = H.dot(x_)\nout\n\n[u, v, s, r]\n\n\nSo thats about the kalman filter motion and measurement model and regarding the process noise since we are not observing velocties they are given high variances in the process matrix"
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html#core-sort-loop",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html#core-sort-loop",
    "title": "Simple Online RealTime Tracking",
    "section": "Core Sort Loop",
    "text": "Core Sort Loop\n#create instance of the SORT tracker , the min_hits are the minimum times the object needed to be redetected to be considered as a valid object\n# max_age is the maximum age above which the object is ignored\nmot_tracker = Sort(max_age=args.max_age, \n                   min_hits=args.min_hits,\n                   iou_threshold=args.iou_threshold) \n\n\n# we loop through each frame\nfor frame in range(int(seq_dets[:,0].max())):\nframe += 1 #detection and frame numbers begin at 1\ndets = seq_dets[seq_dets[:, 0]==frame, 2:7]\ndets[:, 2:4] += dets[:, 0:2] #convert to [x1,y1,w,h] to [x1,y1,x2,y2]\ntotal_frames += 1\n\n# this part is only needed if we are displaying\nif(display):\n  fn = os.path.join('mot_benchmark', phase, seq, 'img1', '%06d.jpg'%(frame))\n  im =io.imread(fn)\n  ax1.imshow(im)\n  plt.title(seq + ' Tracked Targets')\n\nstart_time = time.time()\n#The the tracker update, the kalman predict and update are done within this update method.\ntrackers = mot_tracker.update(dets)\ncycle_time = time.time() - start_time\ntotal_time += cycle_time\nHere we initially create an instance of the tracker and then loop through each frame and get the detections in those frames ,those detections are passed to the update method of the SORT. One point to note here is that within this update method the actual predict and update of the kalman is called.\n\nUpdate Sort\n\nFor each unmatched detections an new kalmanfitler will be created,and in the very first loop all the kalman tracks will be created from the detections since we are not having any trackers to match against, but from the next frame onwards these trackers will be used to match against them. When a new kalman filter object is created for an unmatched detection the kalmans state vector (the first four states which we get from measurement) is with the intial measurement itself.\nIf already initialized trackers are there the predict method for each of the existing trackers is called which is explained in detail below.\n\nfor t, trk in enumerate(trks):\n    pos = self.trackers[t].predict()[0]\n    trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n    if np.any(np.isnan(pos)):\n        to_del.append(t)\npredict method of Sort\ndef predict(self):\n    \"\"\"\n    Advances the state vector and returns the predicted bounding box estimate.\n    \"\"\"\n    if((self.kf.x[6]+self.kf.x[2])&lt;=0):\n      self.kf.x[6] *= 0.0\n    self.kf.predict()\n    self.age += 1\n    if(self.time_since_update&gt;0):\n      self.hit_streak = 0\n    self.time_since_update += 1\n    self.history.append(convert_x_to_bbox(self.kf.x))\n    return self.history[-1]\n\n\nInitially we check for negative area and if so we set the rate of change of area as zero\n\nThen we do the kalman predict method which is F@x_state, and here the covariance also gets udpated.\n\nThen we increase the age of the tracker by one and check for time since the last update was called, if it was called in the last cycle we set the hit_streak to 0.\n\nIncrease the time_since_update by 1. We set this back to zero in the update method of the kalman, this is means to know the if we had a valid kalman update for this tracker or not.\n\nWe return the converted bounding box from measurement space x_center,y_center,scale,aspect ratio to x_top,y_top,x_bottom,y_bottom.\n\n\nNow we associate the predicted trackers to detections.\nAssociate predicted tracks to detections in the current frame\n\ndef associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3): \n\"\"\"\nAssigns detections to tracked object (both represented as bounding boxes)\n\nReturns 3 lists of matches, unmatched_detections and unmatched_trackers\n\"\"\"\n# if the trackers is empty(happens in the begining of the cycle), we return all detections as unmatched\nif(len(trackers)==0):\n    return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n\n#here we find the iou between the detections and tracker\niou_matrix = iou_batch(detections, trackers)\n\n#the iou_matrix will be a one with shape detection_number x tracker_number\nif min(iou_matrix.shape) &gt; 0:\n    a = (iou_matrix &gt; iou_threshold).astype(np.int32)\n\n    #if all detection is only associated with only one tracker we can simply return where\n    if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n        matched_indices = np.stack(np.where(a), axis=1)\n    else:\n        #if more than one traker is associated with any detection we use the hungarian algo and find the indexes\n        matched_indices = linear_assignment(-iou_matrix)\nelse:\n    matched_indices = np.empty(shape=(0,2))\n\nunmatched_detections = []\n#here we loop through the detections and see if there any unmatched detections\nfor d, det in enumerate(detections):\n    if(d not in matched_indices[:,0]):\n        unmatched_detections.append(d)\nunmatched_trackers = []\n\n #here we loop through the trackers  and see if there any unmatched detections\nfor t, trk in enumerate(trackers):\n    if(t not in matched_indices[:,1]):\n        unmatched_trackers.append(t)\n\n#filter out matched with low IOU\nmatches = []\nfor m in matched_indices:\n    if(iou_matrix[m[0], m[1]]&lt;iou_threshold):\n        unmatched_detections.append(m[0])\n        unmatched_trackers.append(m[1])\nelse:\n    matches.append(m.reshape(1,2))\nif(len(matches)==0):\n    matches = np.empty((0,2),dtype=int)\nelse:\n    matches = np.concatenate(matches,axis=0)\n\nreturn matches, np.array(unmatched_detections), np.array(unmatched_trackers)\na. We first find the iou_matrix , this will have detections along the row and trackers along the column  \nb.Now if every row and column of the iou matrix only have one value above the iou_threshold then that row,col pair will be the match with with row for      detection id and col for tracked id.\nc.But if more than one mathces are there in every column then we do the linear_assignment using the hungarian alogrithm\nd.Then we check for the unmatched detection by seeing if there are any rows in the iou matches without the detection.\ne.Similarly we look for the unmatched tracks and see if there any column in the iou matches that are not there.\nf.Then we check for the iou_threshold and see and add to mathces and non matches accordingly , then finally return the matches((det,tracker) as (row,col)), unmatched_detection,unmatched_trackers\n\nNow update each of the tracker with the corresponding matched detections , the update method is explained in detail below. update\n\ndef update(self,bbox):\n    \"\"\"\n    Updates the state vector with observed bbox.\n    \"\"\"\n    self.time_since_update = 0\n    self.history = []\n    self.hits += 1\n    self.hit_streak += 1\n    self.kf.update(convert_bbox_to_z(bbox))\na.Initially we set the time since update to zero.\nb.Then we set the history as an empty list\nc.Then we increase the hits by 1\nd. Then we call the kalman update but first have the change the bounding box form x_top,y_top,x_bottom,y_bottom to the x_center,y_center,scale,aspectRation\n\nNow we reverse the trackers and get the state, then we check if time_since_update is &lt; 1 , we set it to zero in the update part so here we are checking whether we have done update and only if we have done an update we append it to the output, also we check if the hit_streak(which also increase by one in the update method) is greater than the minimum hit streak unless its the begining frames.\n\ni = len(self.trackers)\nfor trk in reversed(self.trackers):\n    d = trk.get_state()[0]\n    if (trk.time_since_update &lt; 1) and (trk.hit_streak &gt;= self.min_hits or self.frame_count &lt;= self.min_hits):\n         ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n    i -= 1\n    # remove dead tracklet\n    if(trk.time_since_update &gt; self.max_age):\n          self.trackers.pop(i)\nif(len(ret)&gt;0):\n      return np.concatenate(ret)\nreturn np.empty((0,5))\n\nWe remove the dead trackers, meaning trackers that have not been assigned to any detections, by checking the time_since_update, the time_since_update is set to zero in the udpate method and is incremented in the predict method, so if we are only doing prediction without any update the time_since_update will increase and pass the maximum age and we will pop it from the trackers\nFinally we concatenate the detections and give them as out.\nLike this we keep updating looping through each frame and detection in it and the same detections should ideally have the same id until they disappear from the frame."
  }
]