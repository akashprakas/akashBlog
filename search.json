[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi I am Akash! . This is the place where i write some of the stuffs I am interested in . I am interested in Computer Vision, Sensor Fusion , Computer Graphics and Self Driving Cars."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "akashBlog",
    "section": "",
    "text": "Simple Online RealTime Tracking\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding the losses in CenterNet Architecture\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSo what does does mobile blocks save\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHacking Into FasterRcnn in Pytorch\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nExplaining IoU\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-09-20-pytorch_iou.html",
    "href": "posts/2020-09-20-pytorch_iou.html",
    "title": "Explaining IoU",
    "section": "",
    "text": "In this blogpost i will explain what is IOU, where is it used , how is it implemented\n\n\nIOU is pretty much clear by the name intersection over union. The formula is - IOU = Area of Intersection / Area of union - Area of union = First Box Area + Second Box Area -Intersection Area\n\n\n\n\nHere i will show a simple implementation in pytorch.If you look at the below picture we will get a basic idea of how to get the intersection between two boxes, the rest are simple\n\nFor the basic implementation of this can be found in this nice blogpost and from that is basic implemenation is like this\n\n\nCode\ndef batch_iou(a, b, epsilon=1e-5):\n    \"\"\" Given two arrays `a` and `b` where each row contains a bounding\n        box defined as a list of four numbers:\n            [x1,y1,x2,y2]\n        where:\n            x1,y1 represent the upper left corner\n            x2,y2 represent the lower right corner\n        It returns the Intersect of Union scores for each corresponding\n        pair of boxes.\n\n    Args:\n        a:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        b:          (numpy array) each row containing [x1,y1,x2,y2] coordinates\n        epsilon:    (float) Small value to prevent division by zero\n\n    Returns:\n        (numpy array) The Intersect of Union scores for each pair of bounding\n        boxes.\n    \"\"\"\n    # COORDINATES OF THE INTERSECTION BOXES\n    x1 = np.array([a[:, 0], b[:, 0]]).max(axis=0)\n    y1 = np.array([a[:, 1], b[:, 1]]).max(axis=0)\n    x2 = np.array([a[:, 2], b[:, 2]]).min(axis=0)\n    y2 = np.array([a[:, 3], b[:, 3]]).min(axis=0)\n\n    # AREAS OF OVERLAP - Area where the boxes intersect\n    width = (x2 - x1)\n    height = (y2 - y1)\n\n    # handle case where there is NO overlap\n    width[width < 0] = 0\n    height[height < 0] = 0\n\n    area_overlap = width * height\n\n    # COMBINED AREAS\n    area_a = (a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1])\n    area_b = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n    area_combined = area_a + area_b - area_overlap\n\n    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n    iou = area_overlap / (area_combined + epsilon)\n    return iou\n\n\n\n\n\nBut the above implementation assumes that both the bounding boxes have the same set of batches,which is rarely the case. IOU is mainly used in object detection tasks.\n\nWe will have a set of anchors for each position in the feature map,for eg say if we have a feature map of shape 5x5 and there are 3 anchors per position then there will be 5x5x3=75 total anchors\nThe Ground trouth boxes for that feature map may be much less the number of anchors\nWe need to find the matching anchors to the bounding boxes, so we can select that portion of the feature map for the downstream predictions.\n\n\n\nBasically when we get two boxes say\na- B,M,4 – the anchor boxes after reshaping(B,A,H,W,4) where A is number of anchors\nb- B,N,4 –the real bboxes. N is the max number of boxes in certain image and the other images will be padded with -1.\nwe need to compute iou between a and b so each box in a is compare with each box in b. So we should make N copies of copies of each box in a to be compare with N bboxes. Also if we want to vectorise this operation then we need to make M copies of b. So the final dimensions will be\na - B,M,N,4 b - B,M,N,4\nNow we can say like each slice of the both a and b can be compared\n\nimport torch\n#say the given anchors and bboxes are in shape x_top,y_top,x_btm,y_btm\nsample_anchors = torch.tensor([[[[[5.,5,15,15], [25,25,35,35],[1,1,9,9]]]]]) #only 1 batch\nbboxes = torch.tensor([[[1.,1,11,11], [20,20,30,30]]]) \nB = bboxes.shape[0]\nno_of_bboxes = bboxes.shape[1]\nprint('sample anchors \\n', sample_anchors,'\\n')\nprint('sample bboxes \\n', bboxes,'\\n')\nprint('sample number of anchors shape ',sample_anchors.shape)\nprint('sample bboxes shape ',bboxes.shape,'\\n')\n\nsample anchors \n tensor([[[[[ 5.,  5., 15., 15.],\n           [25., 25., 35., 35.],\n           [ 1.,  1.,  9.,  9.]]]]]) \n\nsample bboxes \n tensor([[[ 1.,  1., 11., 11.],\n         [20., 20., 30., 30.]]]) \n\nsample number of anchors shape  torch.Size([1, 1, 1, 3, 4])\nsample bboxes shape  torch.Size([1, 2, 4]) \n\n\n\nHere we need to compare the 3 anchor boxes with the two bboxes, first we reshape the anchors to be of shape batch,total_anchors,4,\nwe need to compute iou between sample_anchors and bboxes so each of the 3 anchors are compared with the bboxes which is 2 here. So for vectorized implementation we should make 3 copies of copies of each anchor in sample_anchors to be compare with 2 bboxes. Also if we should make 3 copies of b to aid in vectorized implementation. So the final dimensions will be\n\nsample_anchors - B,3,2,4\nb=boxes - B,3,2,4\n\n\nsample_anchors = sample_anchors.reshape(B,-1,4)\nno_of_anchors = sample_anchors.shape[1]\nsample_anchors = sample_anchors.unsqueeze(2).expand(-1,-1,no_of_bboxes,-1)\nprint(sample_anchors)\nprint(sample_anchors.shape)\n\ntensor([[[[ 5.,  5., 15., 15.],\n          [ 5.,  5., 15., 15.]],\n\n         [[25., 25., 35., 35.],\n          [25., 25., 35., 35.]],\n\n         [[ 1.,  1.,  9.,  9.],\n          [ 1.,  1.,  9.,  9.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\nbboxes = bboxes.unsqueeze(1).expand(-1,no_of_anchors,-1,-1)\nprint(bboxes)\nprint(bboxes.shape)\n\ntensor([[[[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]],\n\n         [[ 1.,  1., 11., 11.],\n          [20., 20., 30., 30.]]]])\ntorch.Size([1, 3, 2, 4])\n\n\n\n#first we need to find the intersection for that width and height of the intersection area\n#this inturn can be obtained by finding the lefttop and bottom corner cordinates and subtracting them\n\nleft_top = torch.max(sample_anchors[:,:,:,:2],bboxes[:,:,:,:2])\nright_bottom = torch.min(sample_anchors[:,:,:,2:],bboxes[:,:,:,2:])\ndelta = right_bottom - left_top\nprint(delta)\n\ntensor([[[[  6.,   6.],\n          [ -5.,  -5.]],\n\n         [[-14., -14.],\n          [  5.,   5.]],\n\n         [[  8.,   8.],\n          [-11., -11.]]]])\n\n\n\n#The first element of delta is width and the next element is height, we can remove negative values \n#since this will be boxes that are not intersecting \n#(remember the the image top left if (0,0) and bottom y is positive downwards)\ndelta[delta<0]=0\n#now find the intersection area\ninteresection_area = delta[:,:,:,0]*delta[:,:,:,1]\nprint(interesection_area)\nprint(interesection_area.shape)\n\ntensor([[[36.,  0.],\n         [ 0., 25.],\n         [64.,  0.]]])\ntorch.Size([1, 3, 2])\n\n\nA small picture represntation is tried below,we can see that first and 3rd anchors intersect with first bounding box while the 2nd anchor intersect with the next one \nFrom the intersection area above we can see that the where there are no itersection the area is zero and thus in this case the first and last anchor mathces with the first bbox while the second anchor mathces with the second one\n\n#now we need to find the Area of union which is \n#Area of union = First Box Area + Second Box Area -Intersection Area\nsample_anchors_area = (sample_anchors[:,:,:,2]-sample_anchors[:,:,:,0])*(sample_anchors[:,:,:,3] -\n                                                                        sample_anchors[:,:,:,1])\nbbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0]) * (bboxes[:,:,:,3] - bboxes[:,:,:,1])\niou = interesection_area/(sample_anchors_area+bbox_area - interesection_area)\nprint(iou)\nprint(iou.shape)\n\ntensor([[[0.2195, 0.0000],\n         [0.0000, 0.1429],\n         [0.6400, 0.0000]]])\ntorch.Size([1, 3, 2])\n\n\nso the final iou matrix will have shape (Batch,no_of_anchors,no_of_bboxes)\n\n\n\nThis iou matrix will be used for calculation the regression offsets, negative anchors,ground truth class . The other place where iou is used is for mean Average Precision at the end which if possible i will explain in another post\n\n\n\n\nBelow i will provide a small code for implementing this in a batch\n\ndef IOU(anchors,bboxes):\n    #anchors B,A,H,W,4\n    #bboxes B,N,4\n    B = anchors.shape[0]\n    anchors = anchors.reshape(B,-1,4)\n    M,N = anchors.shape[1],bboxes.shape[1]\n    \n    #expanding\n    anchors = anchors.unsqueeze(2).expand(-1,-1,N,-1)\n    bboxes = bboxes.unsqueeze(1).expand(-1,M,-1,-1)\n    \n    left_top = torch.max(anchors[:,:,:,:2],bboxes[:,:,:,:2])\n    right_bottom = torch.min(anchors[:,:,:,2:],bboxes[:,:,:,2:])\n    \n    delta = right_bottom - left_top\n    delta[delta<0] = 0\n    \n    intersection_area = delta[:,:,:,0]*delta[:,:,:,1]\n    \n    anchors_area = (anchors[:,:,:,2]-anchors[:,:,:,0])*(anchors[:,:,:,3] -anchors[:,:,:,1])\n    bbox_area = (bboxes[:,:,:,2] - bboxes[:,:,:,0])* (bboxes[:,:,:,3] - bboxes[:,:,:,1])\n    iou = interesection_area/(anchors_area+bbox_area - interesection_area)\n    \n    return iou"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "",
    "text": "In the post I will show how to tweak some of the internals of FaterRcnn in Pytorch. I am assuming the reader is someone who already have trained an object detection model using pytorch. If not there is and excellent tutorial in pytorch website.\n\n\nBasically Faster Rcnn is a two stage detector 1. The first stage is the Region proposal network which is resposible for knowing the objectness and corresponding bounding boxes. So essentially the RegionProposalNetwork will give the proposals of whether and object is there or not 2. These proposals will be used by the RoIHeads which outputs the detections . * Inside the RoIHeads roi align is done * There will be a box head and box predictor * The losses for the predictions 3. In this post i will try to show how we can add custom parts to the torchvision FasterRcnn\n\n\nCode\nimport torch\nimport torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(f'torch version {torch.__version__}')\nprint(f'torchvision version {torchvision.__version__}')\n\n\ntorch version 1.7.0\ntorchvision version 0.8.1"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-without-fpn",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-without-fpn",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "Custom Backbone without FPN",
    "text": "Custom Backbone without FPN\nThis is pretty well written in the pytorch tutorials section, i will add some comments to it additionally\n\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n#we need to specify an outchannel of this backone specifically because this outchannel will be\n#used as an inchannel for the RPNHEAD which is producing the out of RegionProposalNetwork\n#we can know the number of outchannels by looking into the backbone \"backbone??\"\nbackbone.out_channels = 1280\n#by default the achor generator FasterRcnn assign will be for a FPN backone, so\n#we need to specify a  different anchor generator\nanchor_generator = AnchorGenerator(sizes=((128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n#here at each position in the grid there will be 3x3=9 anchors\n#and if our backbone is not FPN then the forward method will assign the name '0' to feature map\n#so we need to specify '0 as feature map name'\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                 output_size=9,\n                                            sampling_ratio=2)\n#the output size is the output shape of the roi pooled features which will be used by the box head\nmodel = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 600)]\npredictions = model(x)"
  },
  {
    "objectID": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-with-fpn",
    "href": "posts/2020-12-19-hacking_fasterrcnn.html#custom-backbone-with-fpn",
    "title": "Hacking Into FasterRcnn in Pytorch",
    "section": "Custom Backbone with FPN",
    "text": "Custom Backbone with FPN\nThe Resnet50Fpn available in torchvision\n\n# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)\n\n\nAdding a different resenet backbone\n\nJust change to a different resenet\nShows how we should change roi_pooler and anchor_generator along with the backbone changes if we are not using all the layers from FPN\n\n\n\nUsing all layers from FPN\n\n#hte returned layers are layer1,layer2,layer3,layer4 in returned_layers\nbackbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True)\nmodel = FasterRCNN(backbone,num_classes=2)                                                                       \n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)\n\n\n\nUsing not all layers from FPN\nThe size of the last fature map in a Resnet50.Later i will show the sizes of the feature maps we use when we use FPN.\n\n\nCode\n#just to show what will be out of of a normal resnet without fpn\nres = torchvision.models.resnet50()\npure = nn.Sequential(*list(res.children())[:-2])\ntemp = torch.rand(1,3,400,400)\npure(temp).shape\n\n\ntorch.Size([1, 2048, 13, 13])\n\n\nThe required layers can be obtained by specifying the returned layers parameters.Also the resnet backbone of different depth can be used.\n\n#the returned layers are layer1,layer2,layer3,layer4 in returned_layers\nbackbone = torchvision.models.detection.backbone_utils.resnet_fpn_backbone('resnet101',pretrained=True,\n                                                                          returned_layers=[2,3,4])\n\nHere we are using feature maps of the following shapes.\n\n\nCode\nout = backbone(temp)\nfor i in out.keys():\n    print(i,'  ',out[i].shape)\n\n\n0    torch.Size([1, 256, 50, 50])\n1    torch.Size([1, 256, 25, 25])\n2    torch.Size([1, 256, 13, 13])\npool    torch.Size([1, 256, 7, 7])\n\n\n\n#from the above we can see that the feature are feat maps should be 0,1,2,pool\n#where pool comes from the default extra block\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','pool'],\n                output_size=7,\n                sampling_ratio=2)\n\nSo essentially what we did was we selected the last three layers in FPN by specifying them in the returned layers, by default, the backbone will add a pool layer on top of the last layer. So we are left with four layers. Now the RoIAlign need to be done in these four layers. If we dnt specify the RoIAlign it will use the by default assume we have used all layers from FPN in torchvision. So we need to specifically give the feauture maps that we used. The usage of feature maps can be our application specific, some time you might need to detect small objects sometimes the object of interest will be large objects only.\n\n#we will need to give anchor_generator because the deafault anchor generator assumes we use all layers in fpn \n#since we have four layers in fpn here we need to specify 4 anchors\nanchor_sizes = ((32), (64), (128),(256) ) \naspect_ratios = ((0.5,1.0, 1.5,2.0,)) * len(anchor_sizes)\nanchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n\nSince we have four layers in our FPN we need to specify the anchors. So here each feature map will have 4 anchors at each position.So the first feature map will have anchor size 32 and four of them will be there at each position in the feature map of aspect_ratios (0.5,1.0, 1.5,2.0). Now we can pass these to the FasterRCNN class\n\nmodel = FasterRCNN(backbone,num_classes=2,rpn_anchor_generator=anchor_generator,box_roi_pool=roi_pooler)\n\n\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)"
  },
  {
    "objectID": "posts/2022-07-30-so what does mobile blocks save.html",
    "href": "posts/2022-07-30-so what does mobile blocks save.html",
    "title": "So what does does mobile blocks save",
    "section": "",
    "text": "Brief Intro\nThis blog assumes the reader have a some understanding of mobileNet.This is just a lazy illustration of how much different mobileNet block save. The actual papers have the real numbers. If you want to know about the model please go through the papers MobileNetV1 MobileNetV2. In this we will briefly see how much parameters and floating point operations are required by a normal convolution block, mobileNetV1 and mobileNetV2 for the same input size to produce the same output. We will use torchinfo library for getting the summaries.\n\n\nCode\nimport torch\nimport torch.nn as nn\nfrom torchinfo import summary\nimport numpy as np\n\n\n\n# we will use the same input and outputs for all the conv blocks and mobile blocks\ninput_filters = 64\noutput_filters = 128\ninput_size = (3,input_filters,224,224)\n\n\n\nCode\ndef printInputAndOutput(model,input_filters=64):\n    rand_tensor = torch.rand((3,input_filters,224,224))\n    out = model(rand_tensor)\n    print(\"Input shape = \", rand_tensor.shape)\n    print(\"Output shap =\", out.shape)\n\n\n\n\nSimple ConvNet Block\n\nsimple_convBlock = nn.Sequential(nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=3,stride=2,\n                                          padding=1,bias=False),nn.BatchNorm2d(output_filters),\n                                nn.ReLU(inplace=True))\nprintInputAndOutput(simple_convBlock)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(simple_convBlock,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nSequential                               --                        [3, 128, 112, 112]        --                        --\n├─Conv2d: 1-1                            [3, 3]                    [3, 128, 112, 112]        73,728                    2,774,532,096\n├─BatchNorm2d: 1-2                       --                        [3, 128, 112, 112]        256                       768\n├─ReLU: 1-3                              --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 73,984\nTrainable params: 73,984\nNon-trainable params: 0\nTotal mult-adds (G): 2.77\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 77.07\nParams size (MB): 0.30\nEstimated Total Size (MB): 115.90\n============================================================================================================================================\n\n\n\n\nMobileNet Block\nThe main idea is to use depth wise convolution to reduce the parameters and floating point operations required. For more info please read the paper or watch this tutorial by Prof Maziar Raissi\n\n\nmobileNetBlock = nn.Sequential(\n                #DEPTHWISE CONV\n                #we get the depthwise convolution by specifying groups same as in_channels\n                nn.Conv2d(in_channels=input_filters,out_channels=input_filters,kernel_size=3,\n                         stride=2,padding=1,groups=input_filters,bias=False),\n                nn.BatchNorm2d(input_filters),\n                nn.ReLU(inplace=True),\n    \n                #POINTWISE CONV\n                nn.Conv2d(in_channels=input_filters,out_channels=output_filters,kernel_size=1,\n                          stride=1,padding=0,bias=False),\n                nn.BatchNorm2d(output_filters),\n                nn.ReLU(inplace=True)\n                )\nprintInputAndOutput(mobileNetBlock)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(mobileNetBlock,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nSequential                               --                        [3, 128, 112, 112]        --                        --\n├─Conv2d: 1-1                            [3, 3]                    [3, 64, 112, 112]         576                       21,676,032\n├─BatchNorm2d: 1-2                       --                        [3, 64, 112, 112]         128                       384\n├─ReLU: 1-3                              --                        [3, 64, 112, 112]         --                        --\n├─Conv2d: 1-4                            [1, 1]                    [3, 128, 112, 112]        8,192                     308,281,344\n├─BatchNorm2d: 1-5                       --                        [3, 128, 112, 112]        256                       768\n├─ReLU: 1-6                              --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 115.61\nParams size (MB): 0.04\nEstimated Total Size (MB): 154.18\n============================================================================================================================================\n\n\n\n\nMobileNetv2 Block\nThe idea here is to add a residual connection and with this better perfomance was obtained with a slight increase in number of parameters. For more info please read the paper or watch this tutorial by Prof Maziar Raissi\n\nclass MobileNetv2Block(nn.Module):\n    \n    def __init__(self,in_channels,out_channels,expand_ratio,stride=1):\n        super(MobileNetv2Block,self).__init__()\n        self.conv1x1Begin = nn.Sequential(\n            nn.Conv2d(in_channels,in_channels*expand_ratio,kernel_size=1,stride=1,bias=False),\n            nn.BatchNorm2d(in_channels*expand_ratio),\n            nn.ReLU6(inplace=True))\n        \n        self.convDepthWise = nn.Sequential(\n            nn.Conv2d(in_channels*expand_ratio,in_channels*expand_ratio,kernel_size=3,stride=stride,padding=1,groups=in_channels*expand_ratio,bias=False),\n            nn.BatchNorm2d(in_channels*expand_ratio),\n            nn.ReLU6(inplace=True)\n        \n        )\n        \n        self.conv1x1Last = nn.Sequential(\n            nn.Conv2d(in_channels*expand_ratio,out_channels,kernel_size=1,stride=1,bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU6(inplace=True))\n        \n        self.stride = stride\n        \n        self.use_res_connect = self.stride == 1 and in_channels == out_channels\n        \n    def forward(self,x):\n        input_ = x     \n        x = self.conv1x1Begin(x)\n        x = self.convDepthWise(x)\n        x = self.conv1x1Last(x)\n\n        if self.use_res_connect:\n            return x+input_\n        else:\n            return x\n        \n\n\nmobileNetV2Block = MobileNetv2Block(64,128,2,2)\nprintInputAndOutput(mobileNetV2Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(mobileNetV2Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nMobileNetv2Block                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 224, 224]        --                        --\n│    └─Conv2d: 2-1                       [1, 1]                    [3, 128, 224, 224]        8,192                     1,233,125,376\n│    └─BatchNorm2d: 2-2                  --                        [3, 128, 224, 224]        256                       768\n│    └─ReLU6: 2-3                        --                        [3, 128, 224, 224]        --                        --\n├─Sequential: 1-2                        --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-4                       [3, 3]                    [3, 128, 112, 112]        1,152                     43,352,064\n│    └─BatchNorm2d: 2-5                  --                        [3, 128, 112, 112]        256                       768\n│    └─ReLU6: 2-6                        --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-3                        --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-7                       [1, 1]                    [3, 128, 112, 112]        16,384                    616,562,688\n│    └─BatchNorm2d: 2-8                  --                        [3, 128, 112, 112]        256                       768\n│    └─ReLU6: 2-9                        --                        [3, 128, 112, 112]        --                        --\n============================================================================================================================================\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 462.42\nParams size (MB): 0.11\nEstimated Total Size (MB): 501.06\n============================================================================================================================================\n\n\n\n\nComparison\nNow we can compare the summaries of each block. From the above cells we can observe that the inputs and output shapes remains the same\n\n1)SimpleConvBlock\nTotal params: 73,984\nTrainable params: 73,984\nNon-trainable params: 0\nTotal mult-adds (G): 2.77\n\n\n2)MobileNetV1\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n\n\n3)MobileNetV2\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\nIf you look at the outputs of torchinfo you can see that the estimated total size is more for mobileNets than simpleConv block this isbecause we need to store 2 times the intermediate values during training , but this wont be a problem for inference, during inference we only need to store the parameters and architecture, and thus looking above we can see that way fewer parameters and total number of multiplications and additions needed is also low which helps in faster inference. If you want more info please read the papers which are well written. If you want to read about how the torchinfo works please read this blog by Jacob C. Kimmel\n\n\n\nDoing all the above with torchvision classes\nActually all the above were taken from torchvision only and we can do the same easily with torchvision classes as shown below . All credits are to the amazing torchvision library\n\nfrom torchvision.models.mobilenetv2 import MobileNetV2, InvertedResidual,ConvNormActivation\n\n\n#we have to put the expand_ratio as one which will reduce this to a simple mobilenetV1 block\nTorchMobileNetV1Block = InvertedResidual(64,128,stride=2,expand_ratio=1)\n\n\nTorchMobileNetV2Block = InvertedResidual(64,128,stride=2,expand_ratio=2)\n\n\nprintInputAndOutput(TorchMobileNetV1Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nprintInputAndOutput(TorchMobileNetV2Block)\n\nInput shape =  torch.Size([3, 64, 224, 224])\nOutput shap = torch.Size([3, 128, 112, 112])\n\n\n\nsummary(TorchMobileNetV1Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nInvertedResidual                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 112, 112]        --                        --\n│    └─ConvNormActivation: 2-1           --                        [3, 64, 112, 112]         --                        --\n│    │    └─Conv2d: 3-1                  [3, 3]                    [3, 64, 112, 112]         576                       21,676,032\n│    │    └─BatchNorm2d: 3-2             --                        [3, 64, 112, 112]         128                       384\n│    │    └─ReLU6: 3-3                   --                        [3, 64, 112, 112]         --                        --\n│    └─Conv2d: 2-2                       [1, 1]                    [3, 128, 112, 112]        8,192                     308,281,344\n│    └─BatchNorm2d: 2-3                  --                        [3, 128, 112, 112]        256                       768\n============================================================================================================================================\nTotal params: 9,152\nTrainable params: 9,152\nNon-trainable params: 0\nTotal mult-adds (M): 329.96\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 115.61\nParams size (MB): 0.04\nEstimated Total Size (MB): 154.18\n============================================================================================================================================\n\n\n\nsummary(TorchMobileNetV2Block,input_size=input_size,col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"])\n\n============================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Output Shape              Param #                   Mult-Adds\n============================================================================================================================================\nInvertedResidual                         --                        [3, 128, 112, 112]        --                        --\n├─Sequential: 1-1                        --                        [3, 128, 112, 112]        --                        --\n│    └─ConvNormActivation: 2-1           --                        [3, 128, 224, 224]        --                        --\n│    │    └─Conv2d: 3-1                  [1, 1]                    [3, 128, 224, 224]        8,192                     1,233,125,376\n│    │    └─BatchNorm2d: 3-2             --                        [3, 128, 224, 224]        256                       768\n│    │    └─ReLU6: 3-3                   --                        [3, 128, 224, 224]        --                        --\n│    └─ConvNormActivation: 2-2           --                        [3, 128, 112, 112]        --                        --\n│    │    └─Conv2d: 3-4                  [3, 3]                    [3, 128, 112, 112]        1,152                     43,352,064\n│    │    └─BatchNorm2d: 3-5             --                        [3, 128, 112, 112]        256                       768\n│    │    └─ReLU6: 3-6                   --                        [3, 128, 112, 112]        --                        --\n│    └─Conv2d: 2-3                       [1, 1]                    [3, 128, 112, 112]        16,384                    616,562,688\n│    └─BatchNorm2d: 2-4                  --                        [3, 128, 112, 112]        256                       768\n============================================================================================================================================\nTotal params: 26,496\nTrainable params: 26,496\nNon-trainable params: 0\nTotal mult-adds (G): 1.89\n============================================================================================================================================\nInput size (MB): 38.54\nForward/backward pass size (MB): 462.42\nParams size (MB): 0.11\nEstimated Total Size (MB): 501.06\n============================================================================================================================================"
  },
  {
    "objectID": "posts/2022-08-24-understanding the losses in centernet architecture.html",
    "href": "posts/2022-08-24-understanding the losses in centernet architecture.html",
    "title": "Understanding the losses in CenterNet Architecture",
    "section": "",
    "text": "Brief Intro\nThis is a small demo of the how the ground truths and loss will look in centerNet. Most of the code is from MMdetection. The idea here is to show a demo part so that the code is understandable. My repo for understanding the architecure can be found here . To understand the architecture please go through the blog by Shreejal Trivedi.\n\n\nHow to generate the groundTruth heat maps\n\nWe need to generate heatmap, the heatmap is generated such that at the center of the image, where the bounding box is the value will be 1 and decreasing around it like a gaussian.\nTo calculate the radius we use the bbox height and width which have been scaled down to the feature map level.\nWe can use the same function used in MMdetection. You can find the detailed description in this link\nLets assume that our initial image was of size (64,64) and it was scaled down to (8,8) after the feature maps and lets assume the radius is 2, in reality the radius is calculated as the function shown below based on the bounding box size\n\n\n\nCode\nimport torch\nimport numpy as np\n\n\n\n\nCode\ndef gaussian2D(radius, sigma=1, dtype=torch.float32, device='cpu'):\n    \"\"\"Generate 2D gaussian kernel.\n\n    Args:\n        radius (int): Radius of gaussian kernel.\n        sigma (int): Sigma of gaussian function. Default: 1.\n        dtype (torch.dtype): Dtype of gaussian tensor. Default: torch.float32.\n        device (str): Device of gaussian tensor. Default: 'cpu'.\n\n    Returns:\n        h (Tensor): Gaussian kernel with a\n            ``(2 * radius + 1) * (2 * radius + 1)`` shape.\n    \"\"\"\n    x = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(1, -1)\n    y = torch.arange(\n        -radius, radius + 1, dtype=dtype, device=device).view(-1, 1)\n\n    h = (-(x * x + y * y) / (2 * sigma * sigma)).exp()\n\n    h[h < torch.finfo(h.dtype).eps * h.max()] = 0\n    return h\n\n\ndef gen_gaussian_target(heatmap, center, radius, k=1):\n    \"\"\"Generate 2D gaussian heatmap.\n\n    Args:\n        heatmap (Tensor): Input heatmap, the gaussian kernel will cover on\n            it and maintain the max value.\n        center (list[int]): Coord of gaussian kernel's center.\n        radius (int): Radius of gaussian kernel.\n        k (int): Coefficient of gaussian kernel. Default: 1.\n\n    Returns:\n        out_heatmap (Tensor): Updated heatmap covered by gaussian kernel.\n    \"\"\"\n    diameter = 2 * radius + 1\n    gaussian_kernel = gaussian2D(\n        radius, sigma=diameter / 6, dtype=heatmap.dtype, device=heatmap.device)\n\n    x, y = center\n\n    height, width = heatmap.shape[:2]\n\n    left, right = min(x, radius), min(width - x, radius + 1)\n    top, bottom = min(y, radius), min(height - y, radius + 1)\n\n    masked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\n    masked_gaussian = gaussian_kernel[radius - top:radius + bottom,\n                                      radius - left:radius + right]\n    out_heatmap = heatmap\n    torch.max(\n        masked_heatmap,\n        masked_gaussian * k,\n        out=out_heatmap[y - top:y + bottom, x - left:x + right])\n\n    return out_heatmap\n\n\n\ndef gaussian_radius(det_size, min_overlap):\n    r\"\"\"Generate 2D gaussian radius.\n\n    This function is modified from the `official github repo\n    <https://github.com/princeton-vl/CornerNet-Lite/blob/master/core/sample/\n    utils.py#L65>`_.\n\n    Given ``min_overlap``, radius could computed by a quadratic equation\n    according to Vieta's formulas.\n\n    There are 3 cases for computing gaussian radius, details are following:\n\n    - Explanation of figure: ``lt`` and ``br`` indicates the left-top and\n      bottom-right corner of ground truth box. ``x`` indicates the\n      generated corner at the limited position when ``radius=r``.\n\n    - Case1: one corner is inside the gt box and the other is outside.\n\n    .. code:: text\n\n        |<   width   >|\n\n        lt-+----------+         -\n        |  |          |         ^\n        +--x----------+--+\n        |  |          |  |\n        |  |          |  |    height\n        |  | overlap  |  |\n        |  |          |  |\n        |  |          |  |      v\n        +--+---------br--+      -\n           |          |  |\n           +----------+--x\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{(w-r)*(h-r)}{w*h+(w+h)r-r^2} \\ge {iou} \\quad\\Rightarrow\\quad\n        {r^2-(w+h)r+\\cfrac{1-iou}{1+iou}*w*h} \\ge 0 \\\\\n        {a} = 1,\\quad{b} = {-(w+h)},\\quad{c} = {\\cfrac{1-iou}{1+iou}*w*h}\n        {r} \\le \\cfrac{-b-\\sqrt{b^2-4*a*c}}{2*a}\n\n    - Case2: both two corners are inside the gt box.\n\n    .. code:: text\n\n        |<   width   >|\n\n        lt-+----------+         -\n        |  |          |         ^\n        +--x-------+  |\n        |  |       |  |\n        |  |overlap|  |       height\n        |  |       |  |\n        |  +-------x--+\n        |          |  |         v\n        +----------+-br         -\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{(w-2*r)*(h-2*r)}{w*h} \\ge {iou} \\quad\\Rightarrow\\quad\n        {4r^2-2(w+h)r+(1-iou)*w*h} \\ge 0 \\\\\n        {a} = 4,\\quad {b} = {-2(w+h)},\\quad {c} = {(1-iou)*w*h}\n        {r} \\le \\cfrac{-b-\\sqrt{b^2-4*a*c}}{2*a}\n\n    - Case3: both two corners are outside the gt box.\n\n    .. code:: text\n\n           |<   width   >|\n\n        x--+----------------+\n        |  |                |\n        +-lt-------------+  |   -\n        |  |             |  |   ^\n        |  |             |  |\n        |  |   overlap   |  | height\n        |  |             |  |\n        |  |             |  |   v\n        |  +------------br--+   -\n        |                |  |\n        +----------------+--x\n\n    To ensure IoU of generated box and gt box is larger than ``min_overlap``:\n\n    .. math::\n        \\cfrac{w*h}{(w+2*r)*(h+2*r)} \\ge {iou} \\quad\\Rightarrow\\quad\n        {4*iou*r^2+2*iou*(w+h)r+(iou-1)*w*h} \\le 0 \\\\\n        {a} = {4*iou},\\quad {b} = {2*iou*(w+h)},\\quad {c} = {(iou-1)*w*h} \\\\\n        {r} \\le \\cfrac{-b+\\sqrt{b^2-4*a*c}}{2*a}\n\n    Args:\n        det_size (list[int]): Shape of object.\n        min_overlap (float): Min IoU with ground truth for boxes generated by\n            keypoints inside the gaussian kernel.\n\n    Returns:\n        radius (int): Radius of gaussian kernel.\n    \"\"\"\n    height, width = det_size\n\n    a1 = 1\n    b1 = (height + width)\n    c1 = width * height * (1 - min_overlap) / (1 + min_overlap)\n    sq1 = sqrt(b1**2 - 4 * a1 * c1)\n    r1 = (b1 - sq1) / (2 * a1)\n\n    a2 = 4\n    b2 = 2 * (height + width)\n    c2 = (1 - min_overlap) * width * height\n    sq2 = sqrt(b2**2 - 4 * a2 * c2)\n    r2 = (b2 - sq2) / (2 * a2)\n\n    a3 = 4 * min_overlap\n    b3 = -2 * min_overlap * (height + width)\n    c3 = (min_overlap - 1) * width * height\n    sq3 = sqrt(b3**2 - 4 * a3 * c3)\n    r3 = (b3 + sq3) / (2 * a3)\n    return min(r1, r2, r3)\n\n\n\nh = gaussian2D(radius=2, sigma=1, dtype=torch.float32, device='cpu')\n\n\nh\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\nOur radius was 2 so we can see that at (2,2) the magnitude is 1 and in a gaussian kernel way,it decreases around.Now this heatmap will be copied to the center as required, but if the center is at the corners then cropping of the heatmap might be requried as needed 1. As in the begining assume that the heatmap is of shape 8,8 and lets assume that the object is located at the center (3,3). 2. So we need to copy the ground truth heatMap to that position as is as shown in the code below\n\nradius = 2\nheatmap = torch.zeros((8,8))\nheight, width = heatmap.shape[:2]\ncenter=(3,3)\nx, y = center\n# we are doing this because the kernel may lie outside the heatmap for example near corners\nleft, right = min(x, radius), min(width - x, radius + 1)\ntop, bottom = min(y, radius), min(height - y, radius + 1)\n\nmasked_heatmap = heatmap[y - top:y + bottom, x - left:x + right]\nmasked_gaussian = h[radius - top:radius + bottom,\n                                      radius - left:radius + right]\n\n\nmasked_gaussian\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\n\nmasked_heatmap\n\ntensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n\n\n\nout_heatmap = heatmap\ntorch.max(\n        masked_heatmap,\n        masked_gaussian ,\n        out=out_heatmap[y - top:y + bottom, x - left:x + right])\n\ntensor([[0.0183, 0.0821, 0.1353, 0.0821, 0.0183],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.1353, 0.6065, 1.0000, 0.6065, 0.1353],\n        [0.0821, 0.3679, 0.6065, 0.3679, 0.0821],\n        [0.0183, 0.0821, 0.1353, 0.0821, 0.0183]])\n\n\n\nout_heatmap\n\ntensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n        [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n        [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n        [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n        [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n\n\n\nout_heatmap.shape\n\ntorch.Size([8, 8])\n\n\nWe can see that the heatmap has been placed with a value of 1 as (3,3) which is the center we gave.\nNow look at how the losses will work.\n\n\nHeatMap loss\n\nSuppose we have 4 classes, 2 batches and feature map size be 8, then the predicted heatmap will be of shape 2,4,8,8 . (batch,num_classes,height,width)\nSuppose we have two images and one bounding box each in two images, let the first image have class id 0 and the second image have classid 2 . So the corresponding depth will have the heatmap gaussian as the ground truth with center having one and the gaussian kernel spread around.\nFor now we will use the above geneareted heatmap as the object , that is we have object at center 3,3 at id 0 in first image and at id 2 in second image.\n\n\ngroundTruth = torch.zeros((2,4,8,8),dtype=torch.float32)\nprint(\"GroundTruth shape\",groundTruth.shape)\n# now we need to copy the heat map we generated above to the positions of the class ids,\n# here we have assumed the in the first image the class id 0 is having the bounding box\n# and in the image 2 the classid 2 is having the object, for simplicity we are assuming\n# that both the images have same heat map an center, the assignment is as follows then\ngroundTruth[0,0,:,:] = out_heatmap.clone()\ngroundTruth[1,2,:,:] = out_heatmap.clone()\nprint(\"Ground Truth \\n\",groundTruth)\n\nGroundTruth shape torch.Size([2, 4, 8, 8])\nGround Truth \n tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.1353, 0.6065, 1.0000, 0.6065, 0.1353, 0.0000, 0.0000],\n          [0.0000, 0.0821, 0.3679, 0.6065, 0.3679, 0.0821, 0.0000, 0.0000],\n          [0.0000, 0.0183, 0.0821, 0.1353, 0.0821, 0.0183, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]])\n\n\nNow we will make a random prediction and see how we calculate the losses\n\npred = torch.randn(2,4,8,8,dtype=torch.float32).sigmoid()\n\n\n# the loss is as the described in the paper and is a modified focal loss\nalpha = 2.0\ngamma = 4.0\neps = 1e-12\npos_weights = groundTruth.eq(1)\nneg_weigths = (1 - groundTruth).pow(gamma)\n\nFrom the heat map we can see that the negative samples will be much more and therefore they introduced this modified version of focal loss to counteract that\n\npos_loss = -(pred+eps).log()*(1 - pred).pow(alpha)*pos_weights\nneg_loss = -(1 - pred+ eps).log()*pred.pow(alpha)*neg_weigths\nfinal_loss = pos_loss + neg_loss\nprint(final_loss.sum())\n\ntensor(181.3507)\n\n\n\n\nGroundTruth WH and WH offset\n\nThe the head predicts the wh in the shape of batch,2,height,widht and the same is the shape of offset head\nFor wh head the bbox scaled down widht and height will be placed at the indexes 0 and 1.\nFor offset head the the corressponding difference in offset will be placed at the corresponding indexes.\nTo understand it better, initially we have assumed that we have an image of shape (64,64) and after scaling down it becomes (8,8). And suppose in the original image the bounding box center is at (28,28) so when we scale it down to feature map level it becomem 28/8 = 3.5 and we have to take the int of that so the offset difference is (3.5 -3) which is 0.5. Its as shown below.\n\n\n#so cx and cy will be \nctx,cty = 28/8 ,28/8\nctx_int,cty_int = int(ctx),int(cty)\n\nprint(\"orginal scaled down \",(ctx, cty) )\nprint(\"floored version \" ,(ctx_int,cty_int))\nprint(\"offset is \",(ctx- ctx_int,cty-cty_int))\n\norginal scaled down  (3.5, 3.5)\nfloored version  (3, 3)\noffset is  (0.5, 0.5)\n\n\n\n# now we generate the ground truth\ngroundTruthWH =torch.zeros((2,2,8,8),dtype=torch.float32)\ngroundTruthWHOffset = torch.zeros((2,2,8,8),dtype=torch.float32)\ngroundTruthWHOffsetWeights = torch.zeros((2,2,8,8),dtype=torch.float32)\n\n\n# so for the we have said that the object is same position in both the images, so when\n# groundTruth is set the to be predicted width and height at the same position\n\ngroundTruthWHOffset[0,0,ctx_int,cty_int] = ctx- ctx_int\ngroundTruthWHOffset[0,1,ctx_int,cty_int] = cty-cty_int\n\n\n# we are asuming the object is at the same position in both the images so the\n# above will be the same for batchid 1\ngroundTruthWHOffset[1,0,ctx_int,cty_int] = ctx- ctx_int\ngroundTruthWHOffset[1,1,ctx_int,cty_int] = cty-cty_int\n\nWe need set weights because we need to consider loss only from the places there was an object\n\ngroundTruthWHOffsetWeights[0,:,ctx_int,cty_int] = 1\n#since the second batch image is also at the same place\ngroundTruthWHOffsetWeights[1,:,ctx_int,cty_int] = 1\n\nLets make a random prediction to calculate the loss\n\npredWH = torch.randn((2,2,8,8),dtype=torch.float32)\npredWHOffset = torch.randn((2,2,8,8),dtype=torch.float32)\n\nThe loss we use for the wh and wh_offset are the same and is the l1_loss\n\ndef l1_loss(pred, target):\n    \"\"\"L1 loss.\n\n    Args:\n        pred (torch.Tensor): The prediction.\n        target (torch.Tensor): The learning target of the prediction.\n\n    Returns:\n        torch.Tensor: Calculated loss\n    \"\"\"\n    if target.numel() == 0:\n        return pred.sum() * 0\n\n    assert pred.size() == target.size()\n    loss = torch.abs(pred - target)\n    return loss\n\n\n#Note we are multiplying by the weights in the end to get the loss from required poistion only\nWHLoss = l1_loss(predWH,groundTruthWH)*groundTruthWHOffsetWeights\nWHOffsetLoss = l1_loss(predWHOffset,groundTruthWHOffset)*groundTruthWHOffsetWeights\n\nWHLoss = WHLoss.sum()\nWHOffsetLoss = WHOffsetLoss.sum()\n\n\nprint(\"Ground Truth Loss \",WHLoss)\nprint(\"Ground Offset Loss \",WHOffsetLoss)\n\nGround Truth Loss  tensor(3.6995)\nGround Offset Loss  tensor(2.0938)\n\n\n\nFinal Loss\nThe final loss is the weighted sum of the heapmap loss, wh loss and wh_offset loss.There is a little more things involved and in the repo i have showed how these are actually done in a real implemenation. Hope this was helpful"
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html",
    "title": "Simple Online RealTime Tracking",
    "section": "",
    "text": "This is an overview of the implementation details of SORT tracking algorithh. The official implemenation of the paper is present this repo . The paper pretty much explains its straightforward , i will be more walking through the implemenation details. In a top level SORT is a tracking algorithm which falls in the class of tracking by detection and the detection, assoaciation , tracking cycle is happening in the 2D image domain."
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html#detection",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html#detection",
    "title": "Simple Online RealTime Tracking",
    "section": "Detection",
    "text": "Detection\nSort is a tracking by detection algorithm. So the quality of the tracking will inturn depends on the quality of detector. In the officiall implementation repo, the author have already porvided detections from the MOT Benchmark. So in the implementation present in the repo we can treat the detector as a blackbox and use the detection information already present.\n\nMotion Model and why we need it\nSo we get detections in the current frame and we need to some how associate it to the detections from the previous frame. Seems like a place where we can put a Kalman filter into good use. So we use a kalman filter with constant velocity motion model and then we will treat the detections as measurements. The official implementation uses filterpy which is a python library with different kalman filter implementations.\n\nState Variables in the constant velocity model\nSo what are the state variables\nu –> the horizontal pixel location of center of target\nv –> the vertical pixel location of the center of target\ns –> area (width_bbox*height_bbox)\nr –> aspect ration (width_bbox/height_bbox)\nWe assume a constant velocity model and also assume the aspect ratio also remains constant, our process and measurement models will be based on that.\nThe state variables are [u,v,s,r,u_dot,v_dot,s_dot] where u_dot,v_dot and s_dot represent the corresponding velocities. Since we are assuming a constant velocity model we can use the normal newtons equations.\nu = u + u_dot * t\nv = v + v_dot * t\ns = s + s_dot * t\nr = r\nu_dot = u_dot\nv_dot = v_dot\ns_dot = s_dot\nThe final process model will look like\nx_(t+1) = F * x_(t) + ProcessNoise , For constant velocity model like above the process model will look something like shown below. As its shown from the output of the dot product, we get what we expected\n\nfrom sympy import symbols , Matrix\nu,v,s,r,u_dot,v_dot,s_dot = symbols('u,v,s,r,u_dot,v_dot,s_dot')\nF  = Matrix([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]])\nx_ = Matrix([u,v,s,r,u_dot,v_dot,s_dot])\nout = F.dot(x_)\n\n\nF\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1\\end{matrix}\\right]\\)\n\n\n\nout\n\n[u + u_dot, v + v_dot, s + s_dot, r, u_dot, v_dot, s_dot]\n\n\n\n\nMeasurement Model\nWe use the variables u,v,s and r as measurements. We get these from the bounding box coordinates of each detections. So we need the measurement model to convert form the state space to the measurement space and the model is very simple 4x7 matrix with\n\nH = Matrix([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]])\nH\n\n\\(\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\end{matrix}\\right]\\)\n\n\n\nout = H.dot(x_)\nout\n\n[u, v, s, r]\n\n\nSo thats about the kalman filter motion and measurement model and regarding the process noise since we are not observing velocties they are given high variances in the process matrix"
  },
  {
    "objectID": "posts/2022-11-23-simpleonlinerealtimetracking.html#core-sort-loop",
    "href": "posts/2022-11-23-simpleonlinerealtimetracking.html#core-sort-loop",
    "title": "Simple Online RealTime Tracking",
    "section": "Core Sort Loop",
    "text": "Core Sort Loop\n    #create instance of the SORT tracker , the min_hits are the minimum times the object needed to be redetected to be considered as a valid object\n    # max_age is the maximum age above which the object is ignored\n    mot_tracker = Sort(max_age=args.max_age, \n                       min_hits=args.min_hits,\n                       iou_threshold=args.iou_threshold) \n\n    \n      # we loop through each frame\n      for frame in range(int(seq_dets[:,0].max())):\n        frame += 1 #detection and frame numbers begin at 1\n        dets = seq_dets[seq_dets[:, 0]==frame, 2:7]\n        dets[:, 2:4] += dets[:, 0:2] #convert to [x1,y1,w,h] to [x1,y1,x2,y2]\n        total_frames += 1\n    \n        # this part is only needed if we are displaying\n        if(display):\n          fn = os.path.join('mot_benchmark', phase, seq, 'img1', '%06d.jpg'%(frame))\n          im =io.imread(fn)\n          ax1.imshow(im)\n          plt.title(seq + ' Tracked Targets')\n\n        start_time = time.time()\n        #The the tracker update, the kalman predict and update are done within this update method.\n        trackers = mot_tracker.update(dets)\n        cycle_time = time.time() - start_time\n        total_time += cycle_time\nHere we initially create an instance of the tracker and then loop through each frame and get the detections in those frames ,those detections are passed to the update method of the SORT. One point to note here is that within this update method the actual predict and update of the kalman is called.\n\nUpdate Sort\n\nFor each unmatched detections an new kalmanfitler will be created,and in the very first loop all the kalman tracks will be created from the detections since we are not having any trackers to match against, but from the next frame onwards these trackers will be used to match against them. When a new kalman filter object is created for an unmatched detection the kalmans state vector (the first four states which we get from measurement) is with the intial measurement itself.\nIf already initialized trackers are there the predict method for each of the existing trackers is called which is explained in detail below.\n\n for t, trk in enumerate(trks):\n      pos = self.trackers[t].predict()[0]\n      trk[:] = [pos[0], pos[1], pos[2], pos[3], 0]\n      if np.any(np.isnan(pos)):\n        to_del.append(t)\n\npredict method of Sort ```python\n\ndef predict(self):\n\"\"\"\nAdvances the state vector and returns the predicted bounding box estimate.\n\"\"\"\nif((self.kf.x[6]+self.kf.x[2])<=0):\n  self.kf.x[6] *= 0.0\nself.kf.predict()\nself.age += 1\nif(self.time_since_update>0):\n  self.hit_streak = 0\nself.time_since_update += 1\nself.history.append(convert_x_to_bbox(self.kf.x))\nreturn self.history[-1]\n  \n\n\n>  a.Initially we check for negative area and if so we set the rate of change of area as zero  \n>  b.Then we do the kalman predict method which is F@x_state, and here the covariance also gets udpated.  \n>  c.Then we increase the age of the tracker by one and check for time since the last update was called, if it was called in the last cycle we set the hit_streak to 0.   \n>  d.Increase the time_since_update by 1. We set this back to zero in the update method of the kalman, this is means to know the if we had a valid kalman update for this tracker or not.  \n>  e.We return the converted bounding box from measurement space x_center,y_center,scale,aspect ratio to x_top,y_top,x_bottom,y_bottom.\n\n\n3. Now we associate the predicted trackers to detections.  \n  > *Associate predicted tracks to detections in the current frame*  \n  ```python\n      def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3): \n      \"\"\"\n      Assigns detections to tracked object (both represented as bounding boxes)\n\n      Returns 3 lists of matches, unmatched_detections and unmatched_trackers\n      \"\"\"\n      # if the trackers is empty(happens in the begining of the cycle), we return all detections as unmatched\n      if(len(trackers)==0):\n          return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n\n      #here we find the iou between the detections and tracker\n      iou_matrix = iou_batch(detections, trackers)\n      \n      #the iou_matrix will be a one with shape detection_number x tracker_number\n      if min(iou_matrix.shape) > 0:\n          a = (iou_matrix > iou_threshold).astype(np.int32)\n      \n          #if all detection is only associated with only one tracker we can simply return where\n          if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n              matched_indices = np.stack(np.where(a), axis=1)\n          else:\n              #if more than one traker is associated with any detection we use the hungarian algo and find the indexes\n              matched_indices = linear_assignment(-iou_matrix)\n      else:\n          matched_indices = np.empty(shape=(0,2))\n\n      unmatched_detections = []\n      #here we loop through the detections and see if there any unmatched detections\n      for d, det in enumerate(detections):\n          if(d not in matched_indices[:,0]):\n              unmatched_detections.append(d)\n      unmatched_trackers = []\n      \n       #here we loop through the trackers  and see if there any unmatched detections\n      for t, trk in enumerate(trackers):\n          if(t not in matched_indices[:,1]):\n              unmatched_trackers.append(t)\n\n      #filter out matched with low IOU\n      matches = []\n      for m in matched_indices:\n          if(iou_matrix[m[0], m[1]]<iou_threshold):\n              unmatched_detections.append(m[0])\n              unmatched_trackers.append(m[1])\n      else:\n          matches.append(m.reshape(1,2))\n      if(len(matches)==0):\n          matches = np.empty((0,2),dtype=int)\n      else:\n          matches = np.concatenate(matches,axis=0)\n\n      return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n  ```\n  > a. We first find the iou_matrix , this will have detections along the row and trackers along the column  \n  > b.Now if every row and column of the iou matrix only have one value above the iou_threshold then that row,col pair will be the match with with row for      detection id and col for tracked id.\n  > c.But if more than one mathces are there in every column then we do the linear_assignment using the hungarian alogrithm\n  > d.Then we check for the unmatched detection by seeing if there are any rows in the iou matches without the detection.\n  > e.Similarly we look for the unmatched tracks and see if there any column in the iou matches that are not there.\n  > f.Then we check for the iou_threshold and see and add to mathces and non matches accordingly , then finally return the matches((det,tracker) as (row,col)), unmatched_detection,unmatched_trackers\n\n\n\n4. Now update each of the tracker with the corresponding matched detections , the update method is explained in detail below.\n  > *update*\n  ```python\n     def update(self,bbox):\n          \"\"\"\n          Updates the state vector with observed bbox.\n          \"\"\"\n          self.time_since_update = 0\n          self.history = []\n          self.hits += 1\n          self.hit_streak += 1\n          self.kf.update(convert_bbox_to_z(bbox))\n\n  ```\n  > a.Initially we set the time since update to zero.    \n  > b.Then we set the history as an empty list  \n  > c.Then we increase the hits by 1  \n  > d. Then we call the kalman update but first have the change the bounding box form x_top,y_top,x_bottom,y_bottom to the x_center,y_center,scale,aspectRation   \n  \n\n5. Now we reverse the trackers and  get the state, then we check if time_since_update is < 1 , we set it to zero in the update part so here we are checking whether we have done update and only if we have done an update we append it to the output, also we check if the hit_streak(which also increase by one in the update method) is greater than the minimum hit streak unless its the begining frames.\n```python\n  i = len(self.trackers)\n  for trk in reversed(self.trackers):\n      d = trk.get_state()[0]\n      if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n           ret.append(np.concatenate((d,[trk.id+1])).reshape(1,-1)) # +1 as MOT benchmark requires positive\n      i -= 1\n      # remove dead tracklet\n      if(trk.time_since_update > self.max_age):\n            self.trackers.pop(i)\n  if(len(ret)>0):\n        return np.concatenate(ret)\n  return np.empty((0,5))\n\nWe remove the dead trackers, meaning trackers that have not been assigned to any detections, by checking the time_since_update, the time_since_update is set to zero in the udpate method and is incremented in the predict method, so if we are only doing prediction without any update the time_since_update will increase and pass the maximum age and we will pop it from the trackers\nFinally we concatenate the detections and give them as out.\nLike this we keep updating looping through each frame and detection in it and the same detections should ideally have the same id until they disappear from the frame."
  }
]