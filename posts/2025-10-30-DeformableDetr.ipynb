{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "86a36c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e3218",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. I assume the reader have good understanding of [DETR](https://arxiv.org/abs/2005.12872)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb71c2f",
   "metadata": {},
   "source": [
    "Deformable detr is  an improvement to DETR which was one of the first attempts in using transformers for the task of object detection. The main problems with DETR where\n",
    "### 1. Not able to detect small objects properly. \n",
    "    Usually all the new object detection networks use some kind of Feature Pyramind to detect the object across differenet scales. In DETR we cannot use that because the self attention if across the entire feature map and this could lead to a computational explotion .Deformable Detr tends to solve this problem by attending to features from feature maps of different levels.\n",
    "###  2. Too much training time.\n",
    "    This is because the network needs to learn the actually features to attend to from the full feature map and this will take a good time to converge to. Deformable detr solves introducing a Deformable Attention module , that doesnt look into all the keys , but only a subset of them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fbe6b1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced feature map 1 shape: torch.Size([4, 256, 134, 134])\n",
      "Reduced feature map 2 shape: torch.Size([4, 256, 67, 67])\n",
      "Reduced feature map 3 shape: torch.Size([4, 256, 34, 34])\n",
      "Reduced feature map 4 shape: torch.Size([4, 256, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "pixel_values = torch.randn(4,3,1065,1066) \n",
    "\n",
    "# 1 .Initially we need to pass the images through the FPN and get features across different layers,\n",
    "# 2. Also we need to get positional embedding for each of the feature map, the positional embedding is similar to the normal sine-cosine positional embedding in the original paper,\n",
    "#  the only difference here is that since we have HxW in the feature domain , suppose if our embedding dim is 256, we will have them alingned in such a way that the first 128 corresponds to vertical and the next 128 corresponds\n",
    "# to vertical so that in the end we end up with 256 and that encodes both vertical and horizontal positions. https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55\n",
    "# 3. Suppose we get feature map from 4 layers and let them be (4,512,134,134) ,(4,1024,67,67) , (4,2048,34,34) ,(4,2048,17,17) [Note the actual feature map in the paper is created by an additional conv+group norm] and there positional embeddings w\n",
    "# will have the same size as well. but with the corresponding embedding dim, so they will be of size (4,256,134,134) ,(4,256,67,67) ,(4,256,34,34) ,(4,256,17,17)\n",
    "\n",
    "\n",
    "feature_shapes = [\n",
    "    (4, 512, 134, 134),\n",
    "    (4, 1024, 67, 67),\n",
    "    (4, 2048, 34, 34),\n",
    "    (4, 2048, 17, 17)\n",
    "]\n",
    "\n",
    "# Positional embedding shapes (same spatial dims, but channel dim = 256)\n",
    "embedding_shapes = [\n",
    "    (4, 256, 134, 134),\n",
    "    (4, 256, 67, 67),\n",
    "    (4, 256, 34, 34),\n",
    "    (4, 256, 17, 17)\n",
    "]\n",
    "\n",
    "# original implementation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/backbone.py#L71\n",
    "feature_maps = [torch.randn(shape) for shape in feature_shapes]\n",
    "\n",
    "# original implemenation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55\n",
    "positional_embeddings = [torch.randn(shape) for shape in embedding_shapes]\n",
    "\n",
    "# 4 . Now we have to have a 1x1 conv layer to reduce the channel dimension of the feature so that they match the embedding dimension of 256\n",
    "conv_layers = nn.ModuleList([\n",
    "    nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=2048, out_channels=256, kernel_size=1),\n",
    "    nn.Conv2d(in_channels=2048, out_channels=256, kernel_size=1)\n",
    "])\n",
    "\n",
    "# Apply the 1x1 conv layers\n",
    "reduced_feature_maps = [conv(feature) for conv, feature in zip(conv_layers, feature_maps)]\n",
    "\n",
    "for i, (fmap,pos_emb) in enumerate(zip(reduced_feature_maps,positional_embeddings)):\n",
    "    print(f\"Reduced feature map {i+1} shape:\", fmap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f8f37015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1:\n",
      "  Feature shape: torch.Size([4, 17956, 256])\n",
      "  Positional + Level Embedding shape: torch.Size([4, 17956, 256])\n",
      "Level 2:\n",
      "  Feature shape: torch.Size([4, 4489, 256])\n",
      "  Positional + Level Embedding shape: torch.Size([4, 4489, 256])\n",
      "Level 3:\n",
      "  Feature shape: torch.Size([4, 1156, 256])\n",
      "  Positional + Level Embedding shape: torch.Size([4, 1156, 256])\n",
      "Level 4:\n",
      "  Feature shape: torch.Size([4, 289, 256])\n",
      "  Positional + Level Embedding shape: torch.Size([4, 289, 256])\n"
     ]
    }
   ],
   "source": [
    "# 5 . Also we need a learnable Level embedding for each levels , since here we are using 4 layers, and 256 embedding dim , the size of the level embedding will be (4,256)\n",
    "# Learnable level embedding (in actual model this would be nn.Parameter)\n",
    "level_embedding = torch.randn((4, 256))  # shape: (num_levels, embedding_dim)\n",
    "\n",
    "\n",
    "#6. Now we need to flatten and transpose the features and positional embedding so they become the similar shape like token_len X embedding_dim , for example the first feature map will become (4,134*134,256) ,similarly we have do this \n",
    "# for all the feature maps and the positional embedding. and one additional thing to do is to add the level embedding to the positional embedding.\n",
    "\n",
    "features_flatten = []\n",
    "positional_and_level_embedding_flattened = []\n",
    "\n",
    "for level, (feature, pos_emb) in enumerate(zip(reduced_feature_maps, positional_embeddings)):\n",
    "    # Flatten and transpose: (B, C, H, W) -> (B, HW, C)\n",
    "    feature_flatten = feature.flatten(2).transpose(1, 2)\n",
    "    positional_plus_level_embed = pos_emb.flatten(2).transpose(1, 2) + level_embedding[level].view(1, 1, -1)\n",
    "\n",
    "    features_flatten.append(feature_flatten)\n",
    "    positional_and_level_embedding_flattened.append(positional_plus_level_embed)\n",
    "\n",
    "    # Print shapes\n",
    "    print(f\"Level {level + 1}:\")\n",
    "    print(f\"  Feature shape: {feature_flatten.shape}\")\n",
    "    print(f\"  Positional + Level Embedding shape: {positional_plus_level_embed.shape}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "da3098e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated Inputs Embeds shape: torch.Size([4, 23890, 256])\n",
      "Concatenated Position Embeddings shape: torch.Size([4, 23890, 256])\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Concatenate along sequence dimension (dim=1)\n",
    "inputs_embeds = torch.cat(features_flatten, dim=1)  # shape: (B, total_seq_len, 256)\n",
    "position_embeddings = torch.cat(positional_and_level_embedding_flattened, dim=1)  # shape: (B, total_seq_len, 256)\n",
    "\n",
    "print(\"Concatenated Inputs Embeds shape:\", inputs_embeds.shape)\n",
    "print(\"Concatenated Position Embeddings shape:\", position_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2cf651a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference points shape input to encoder  torch.Size([4, 23890, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 8. we need to apply a initial dropout before passing it to the encoder \n",
    "inputs_embeds = nn.functional.dropout(inputs_embeds, p=0.1)\n",
    "batch_size = inputs_embeds.shape[0]\n",
    "#9. Generating the reference points, so this is a concept that is similar to the deformable convolution , so basically for each feature_point/query in the feature map we need to look into the corresponding point in the other feature\n",
    "# map as well, feature maps a re normilized  based on their height and width, so we can look for the corresponding point for each query in different points as well, here\n",
    "#original implemenation https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/deformable_transformer.py#L238 \n",
    "spatial_shapes_list = [(134, 134), (67, 67), (34, 34), (17, 17)]\n",
    "\n",
    "reference_points_list = []\n",
    "for H_, W_ in spatial_shapes_list:\n",
    "        # Create meshgrid of normalized coordinates\n",
    "        ref_y, ref_x = torch.meshgrid(\n",
    "            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32),\n",
    "            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32),\n",
    "            indexing='ij'  # Important for correct axis ordering\n",
    "        )\n",
    "        # Normalize\n",
    "        ref_y = ref_y.reshape(-1) / H_\n",
    "        ref_x = ref_x.reshape(-1) / W_\n",
    "\n",
    "        # Stack and expand to batch size\n",
    "        ref = torch.stack((ref_x, ref_y), dim=-1)  # shape: (H_*W_, 2)\n",
    "        ref = ref[None].expand(batch_size, -1, -1)  # shape: (B, H_*W_, 2)\n",
    "        reference_points_list.append(ref)\n",
    "\n",
    "\n",
    "# Concatenate all levels\n",
    "reference_points = torch.cat(reference_points_list, dim=1)  # shape: (B, total_seq_len, 2)\n",
    "# Expand to include level dimension\n",
    "reference_points = reference_points[:, :, None, :]  # shape: (B, total_seq_len, 1, 2)\n",
    "\n",
    "# Repeat across levels\n",
    "num_levels = len(spatial_shapes_list)\n",
    "reference_points = reference_points.expand(-1, -1, num_levels, -1)  # shape: (B, total_seq_len, L, 2)\n",
    "print(\"Reference points shape input to encoder \",reference_points.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "937677b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value shape = torch.Size([4, 23890, 8, 32])\n",
      "Sampling offset shape = torch.Size([4, 23890, 8, 4, 4, 2])\n",
      "Attention weights shape = torch.Size([4, 23890, 8, 4, 4]) \n",
      "\n",
      "Reference points with unsqueezed dimension for head and levels = torch.Size([4, 23890, 1, 4, 1, 2])\n",
      "Final sampling locations = torch.Size([4, 23890, 8, 4, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "#so for now each query we have 4 positions (x,y) across 4 different channels, now this will be passed to the encoder.\n",
    "## DEFORMABLE MULTI SCALE ATTENTION.\n",
    "\n",
    "# params\n",
    "num_heads = 8\n",
    "num_levels  = 4\n",
    "n_points  = 4\n",
    "embdedding_dim = inputs_embeds.shape[-1]\n",
    "batch_size, num_queries, _ = inputs_embeds.shape\n",
    "\n",
    "fc1 = nn.Linear(embdedding_dim, 512)\n",
    "fc2 = nn.Linear(512, embdedding_dim)\n",
    "\n",
    "layer_norm1 = nn.LayerNorm(embdedding_dim)\n",
    "layer_norm2 = nn.LayerNorm(embdedding_dim)\n",
    "\n",
    "final_layer_norm = nn.LayerNorm(embdedding_dim)\n",
    "\n",
    "# learnable parameters in the layer\n",
    "sampling_offsets_layer = nn.Linear(embdedding_dim, num_heads * num_levels * n_points *2)\n",
    "attention_weights_layer = nn.Linear(embdedding_dim,num_heads * num_levels * n_points)\n",
    "value_projection_layer  = nn.Linear(embdedding_dim,embdedding_dim)\n",
    "output_projection_layer = nn.Linear(embdedding_dim,embdedding_dim)\n",
    "\n",
    "#initially we add the poistional_embedding to the input_embeds\n",
    "hidden_states = inputs_embeds + position_embeddings\n",
    "value = value_projection_layer(inputs_embeds)\n",
    "value = value.view(batch_size,num_queries, num_heads,embdedding_dim//num_heads)\n",
    "print(f\"Value shape = {value.shape}\")\n",
    "\n",
    "# note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.\n",
    "sampling_offsets = sampling_offsets_layer(hidden_states)\n",
    "sampling_offsets = sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,2) # NOTE : We actually need to normalize this wrt to spatial size of each feature, but omitting here for simplicity\n",
    "attention_weights = attention_weights_layer(hidden_states)\n",
    "attention_weights = attention_weights.view(batch_size,num_queries,num_heads,num_levels*n_points)\n",
    "attention_weights = torch.nn.functional.softmax(attention_weights, -1).view(batch_size,num_queries,num_heads,num_levels,n_points) # note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels\n",
    "print(f\"Sampling offset shape = {sampling_offsets.shape}\")\n",
    "print(f\"Attention weights shape = {attention_weights.shape} \\n\")\n",
    "\n",
    "# Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points , we need to look into 4 more points across 8 different heads\n",
    "# so initially we had for each query 1 points account each feature dimension making it total 4 and now when we add this sampling offsets it makes 4 more across 8 differenet heads\n",
    "reference_points = reference_points[:,:,None,:,None,:]\n",
    "print(f\"Reference points with unsqueezed dimension for head and levels = {reference_points.shape}\")\n",
    "sampling_location = reference_points + sampling_offsets\n",
    "print(f\"Final sampling locations = {sampling_location.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "960fe384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted feature at level 0 --> torch.Size([4, 17956, 8, 32])\n",
      "Splitted feature at level 1 --> torch.Size([4, 4489, 8, 32])\n",
      "Splitted feature at level 2 --> torch.Size([4, 1156, 8, 32])\n",
      "Splitted feature at level 3 --> torch.Size([4, 289, 8, 32])\n",
      "\n",
      "Sampling grid shape  = torch.Size([4, 23890, 8, 4, 4, 2]) \n",
      "\n",
      "Value at level 0 torch.Size([32, 32, 134, 134])\n",
      "Value at level 1 torch.Size([32, 32, 67, 67])\n",
      "Value at level 2 torch.Size([32, 32, 34, 34])\n",
      "Value at level 3 torch.Size([32, 32, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the value tensor into per-level chunks based on spatial shapes\n",
    "value_list = value.split([h * w for h, w in spatial_shapes_list], dim=1)\n",
    "batch_size, _, num_heads, hidden_dim = value.shape\n",
    "\n",
    "# Print the shape of each level's value tensor\n",
    "for level, feature in enumerate(value_list):\n",
    "    print(f\"Splitted feature at level {level} --> {feature.shape}\")\n",
    "\n",
    "# Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample\n",
    "sampling_grids = 2 * sampling_location - 1\n",
    "print(f\"\\nSampling grid shape  = {sampling_grids.shape} \\n\")\n",
    "\n",
    "sampling_value_list = []\n",
    "\n",
    "for level_id, (height, width) in enumerate(spatial_shapes_list):\n",
    "    # Reshape value tensor for grid sampling:\n",
    "    # (B, H*W, num_heads, C) → (B, num_heads, H*W, C) → (B*num_heads, C, H, W)\n",
    "    value_l = (\n",
    "        value_list[level_id]\n",
    "        .flatten(2)               # (B, H*W, num_heads * C)\n",
    "        .transpose(1, 2)          # (B, num_heads * C, H*W)\n",
    "        .reshape(batch_size * num_heads, hidden_dim, height, width)\n",
    "    )\n",
    "    print(f\"Value at level {level_id} {value_l.shape}\")\n",
    "\n",
    "    # Reshape sampling grid:\n",
    "    # (B, num_queries, num_heads, num_levels, num_points, 2)\n",
    "    # → (B, num_heads, num_queries, num_points, 2)\n",
    "    # → (B*num_heads, num_queries, num_points, 2)\n",
    "    sampling_grid_l = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n",
    "\n",
    "    # Sample values using bilinear interpolation\n",
    "    sampling_value_l = nn.functional.grid_sample(\n",
    "        value_l,\n",
    "        sampling_grid_l,\n",
    "        mode=\"bilinear\",\n",
    "        padding_mode=\"zeros\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    sampling_value_list.append(sampling_value_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "787449fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 23890, 4])\n",
      "torch.Size([32, 32, 23890, 4])\n",
      "torch.Size([32, 32, 23890, 4])\n",
      "torch.Size([32, 32, 23890, 4])\n"
     ]
    }
   ],
   "source": [
    "for f in sampling_value_list:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d4c7b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked value matrix shape before flattening = torch.Size([32, 32, 23890, 4, 4])\n",
      "Stacked value matrix shape after flattening = torch.Size([32, 32, 23890, 16])\n"
     ]
    }
   ],
   "source": [
    "final_key_matrix = torch.stack(sampling_value_list, dim=-2)\n",
    "print(f\"Stacked value matrix shape before flattening = {final_key_matrix.shape}\")\n",
    "final_key_matrix = final_key_matrix.flatten(-2)\n",
    "print(f\"Stacked value matrix shape after flattening = {final_key_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a55ae744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 23890, 16])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = attention_weights.transpose(1, 2).reshape(\n",
    "            batch_size * num_heads, 1, num_queries, num_levels * n_points\n",
    "        )\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dee924ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 23890, 16])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = final_key_matrix*attention_weights\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4955f2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 23890])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.sum(-1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6ed7d2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23890, 256])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.view(batch_size,num_heads*hidden_dim,num_queries).transpose(1,2)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67284cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23890, 256])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bde3f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = nn.functional.dropout(hidden_states,p=0.1)\n",
    "hidden_states = inputs_embeds + hidden_states # residual\n",
    "hidden_states = layer_norm1(hidden_states)\n",
    "\n",
    "residual = hidden_states\n",
    "hidden_states = nn.ReLU()(fc1(hidden_states))\n",
    "hidden_states = nn.functional.dropout(hidden_states,p=0.1)\n",
    "hidden_states = fc2(hidden_states)\n",
    "hidden_states  = nn.functional.dropout(hidden_states,p=0.1)\n",
    "\n",
    "hidden_states = residual+hidden_states\n",
    "hidden_states = layer_norm2(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "23f6f7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23890, 256])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851db98",
   "metadata": {},
   "source": [
    "## Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e29ffd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode Reference points shape torch.Size([4, 300, 2])\n"
     ]
    }
   ],
   "source": [
    "encoder_output = hidden_states.clone()\n",
    "num_query = 300\n",
    "embedding_dim = encoder_output.shape[-1]\n",
    "num_levels\n",
    "# Learnable query and positional embeddings\n",
    "position_embeddings = nn.Parameter(torch.randn(num_query, embedding_dim)) #(num_query,embedding_dim)\n",
    "position_embeddings = position_embeddings[None].expand(batch_size,-1,-1) # (batch_size,num_query,embedding_dim)\n",
    "input_query = nn.Parameter(torch.randn(num_query, embedding_dim)) #(num_query,embedding_dim)\n",
    "input_query = input_query[None].expand(batch_size,-1,-1) # (batch_size,num_query,embedding_dim) \n",
    "\n",
    "fc1 = nn.Linear(embdedding_dim, 512)\n",
    "fc2 = nn.Linear(512, embdedding_dim)\n",
    "\n",
    "layer_norm1 = nn.LayerNorm(embdedding_dim)\n",
    "layer_norm2 = nn.LayerNorm(embdedding_dim)\n",
    "layer_norm3 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "# Linear layer to generate reference points from positional embeddings\n",
    "decoder_reference_point_layer = nn.Linear(embedding_dim, 2)\n",
    "\n",
    "# Generate normalized reference points in [0, 1] range\n",
    "reference_points = decoder_reference_point_layer(position_embeddings).sigmoid()  # shape: (num_query, 2)\n",
    "print(f\"Encode Reference points shape {reference_points.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5ed83498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 4, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_points_input = reference_points[:,:,None,:].expand(batch_size,num_query,num_levels,2)\n",
    "reference_points_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "970b4940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state shape input to cross attention  torch.Size([4, 300, 256])\n"
     ]
    }
   ],
   "source": [
    "# Initially here we will have the normal self attention.\n",
    "residual = input_query\n",
    "multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads)\n",
    "self_attn_output, _ = multihead_attn(input_query+position_embeddings, input_query+position_embeddings, input_query)\n",
    "hidden_state_after_self_attention = self_attn_output + residual # residual connection. \n",
    "hidden_state_after_self_attention = layer_norm1(hidden_state_after_self_attention)\n",
    "second_residual = hidden_state_after_self_attention \n",
    "print(f\"Hidden state shape input to cross attention  {self_attn_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3923392b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 300, 256]),\n",
       " torch.Size([4, 300, 256]),\n",
       " torch.Size([4, 23890, 256]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings.shape,hidden_state_after_self_attention.shape,encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6a7a3ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value shape = torch.Size([4, 23890, 8, 32])\n",
      "Sampling offset shape = torch.Size([4, 300, 8, 4, 4, 2])\n",
      "Attention weights shape = torch.Size([4, 300, 8, 4, 4]) \n",
      "\n",
      "Reference points with unsqueezed dimension for head and levels = torch.Size([4, 300, 1, 4, 1, 2])\n",
      "Final sampling locations = torch.Size([4, 300, 8, 4, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "## DEFORMABLE MULTI SCALE ATTENTION.\n",
    "num_heads = 8\n",
    "num_levels  = 4\n",
    "n_points  = 4\n",
    "embdedding_dim = hidden_state_after_self_attention.shape[-1]\n",
    "batch_size, num_queries, _ = hidden_state_after_self_attention.shape\n",
    "\n",
    "# learnable parameters in the layer\n",
    "sampling_offsets_layer = nn.Linear(embdedding_dim, num_heads * num_levels * n_points *2)\n",
    "attention_weights_layer = nn.Linear(embdedding_dim,num_heads * num_levels * n_points)\n",
    "value_projection_layer  = nn.Linear(embdedding_dim,embdedding_dim)\n",
    "output_projection_layer = nn.Linear(embdedding_dim,embdedding_dim)\n",
    "\n",
    "#initially we add the poistional_embedding to the input_embeds\n",
    "hidden_states = hidden_state_after_self_attention + position_embeddings\n",
    "value = value_projection_layer(encoder_output)\n",
    "_,encoder_sequence_length,_ = value.shape\n",
    "value = value.view(batch_size,encoder_sequence_length, num_heads,embdedding_dim//num_heads)\n",
    "print(f\"Value shape = {value.shape}\")\n",
    "\n",
    "# note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.\n",
    "sampling_offsets = sampling_offsets_layer(hidden_states)\n",
    "sampling_offsets = sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,2) # NOTE : We actually need to normalize this wrt to spatial size of each feature, but omitting here for simplicity\n",
    "attention_weights = attention_weights_layer(hidden_states)\n",
    "attention_weights = attention_weights.view(batch_size,num_queries,num_heads,num_levels*n_points)\n",
    "attention_weights = torch.nn.functional.softmax(attention_weights, -1).view(batch_size,num_queries,num_heads,num_levels,n_points) # note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels\n",
    "print(f\"Sampling offset shape = {sampling_offsets.shape}\")\n",
    "print(f\"Attention weights shape = {attention_weights.shape} \\n\")\n",
    "\n",
    "\n",
    "# Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points , we need to look into 4 more points across 8 different heads\n",
    "# so initially we had for each query 1 points account each feature dimension making it total 4 and now when we add this sampling offsets it makes 4 more across 8 differenet heads\n",
    "reference_points_input = reference_points_input[:,:,None,:,None,:]\n",
    "print(f\"Reference points with unsqueezed dimension for head and levels = {reference_points_input.shape}\")\n",
    "sampling_location = reference_points_input + sampling_offsets\n",
    "print(f\"Final sampling locations = {sampling_location.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "86fa0133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted feature at level 0 --> torch.Size([4, 17956, 8, 32])\n",
      "Splitted feature at level 1 --> torch.Size([4, 4489, 8, 32])\n",
      "Splitted feature at level 2 --> torch.Size([4, 1156, 8, 32])\n",
      "Splitted feature at level 3 --> torch.Size([4, 289, 8, 32])\n",
      "\n",
      "Sampling grid shape  = torch.Size([4, 300, 8, 4, 4, 2]) \n",
      "\n",
      "Value at level 0 torch.Size([32, 32, 134, 134])\n",
      "Value at level 1 torch.Size([32, 32, 67, 67])\n",
      "Value at level 2 torch.Size([32, 32, 34, 34])\n",
      "Value at level 3 torch.Size([32, 32, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "# Split the value tensor into per-level chunks based on spatial shapes\n",
    "value_list = value.split([h * w for h, w in spatial_shapes_list], dim=1)\n",
    "batch_size, _, num_heads, hidden_dim = value.shape\n",
    "\n",
    "# Print the shape of each level's value tensor\n",
    "for level, feature in enumerate(value_list):\n",
    "    print(f\"Splitted feature at level {level} --> {feature.shape}\")\n",
    "\n",
    "# Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample\n",
    "sampling_grids = 2 * sampling_location - 1\n",
    "print(f\"\\nSampling grid shape  = {sampling_grids.shape} \\n\")\n",
    "\n",
    "sampling_value_list = []\n",
    "\n",
    "for level_id, (height, width) in enumerate(spatial_shapes_list):\n",
    "    # Reshape value tensor for grid sampling:\n",
    "    # (B, H*W, num_heads, C) → (B, num_heads, H*W, C) → (B*num_heads, C, H, W)\n",
    "    value_l = (\n",
    "        value_list[level_id]\n",
    "        .flatten(2)               # (B, H*W, num_heads * C)\n",
    "        .transpose(1, 2)          # (B, num_heads * C, H*W)\n",
    "        .reshape(batch_size * num_heads, hidden_dim, height, width)\n",
    "    )\n",
    "    print(f\"Value at level {level_id} {value_l.shape}\")\n",
    "\n",
    "    # Reshape sampling grid:\n",
    "    # (B, num_queries, num_heads, num_levels, num_points, 2)\n",
    "    # → (B, num_heads, num_queries, num_points, 2)\n",
    "    # → (B*num_heads, num_queries, num_points, 2)\n",
    "    sampling_grid_l = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n",
    "\n",
    "    # Sample values using bilinear interpolation\n",
    "    sampling_value_l = nn.functional.grid_sample(\n",
    "        value_l,\n",
    "        sampling_grid_l,\n",
    "        mode=\"bilinear\",\n",
    "        padding_mode=\"zeros\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    sampling_value_list.append(sampling_value_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "537666fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling points from each layer 0 torch.Size([32, 32, 300, 4])\n",
      "Sampling points from each layer 1 torch.Size([32, 32, 300, 4])\n",
      "Sampling points from each layer 2 torch.Size([32, 32, 300, 4])\n",
      "Sampling points from each layer 3 torch.Size([32, 32, 300, 4])\n"
     ]
    }
   ],
   "source": [
    "for i,f in enumerate(sampling_value_list):\n",
    "    print(f\"Sampling points from each layer {i} {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "bbfa786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked value matrix shape before flattening = torch.Size([32, 32, 300, 4, 4])\n",
      "Stacked value matrix shape after flattening = torch.Size([32, 32, 300, 16])\n"
     ]
    }
   ],
   "source": [
    "final_key_matrix = torch.stack(sampling_value_list, dim=-2)\n",
    "print(f\"Stacked value matrix shape before flattening = {final_key_matrix.shape}\")\n",
    "final_key_matrix = final_key_matrix.flatten(-2)\n",
    "print(f\"Stacked value matrix shape after flattening = {final_key_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4ef2f515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 300, 16])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights = attention_weights.transpose(1, 2).reshape(\n",
    "            batch_size * num_heads, 1, num_queries, num_levels * n_points\n",
    "        )\n",
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cac02ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after attention torch.Size([32, 32, 300, 16])\n",
      "Final output after summation torch.Size([32, 32, 300])\n",
      " Output reshaped --> torch.Size([4, 300, 256])\n"
     ]
    }
   ],
   "source": [
    "output = final_key_matrix*attention_weights\n",
    "print(f\"Output after attention {output.shape}\")\n",
    "output = output.sum(dim=-1)\n",
    "print(f\"Final output after summation {output.shape}\")\n",
    "output = output.view(batch_size,num_heads*hidden_dim,num_queries).transpose(1,2)\n",
    "print(f\" Output reshaped --> {output.shape}\")\n",
    "output = output_projection_layer(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47091739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 256])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7f35de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = nn.functional.dropout(output,p=0.1)\n",
    "hidden_states = second_residual + hidden_states\n",
    "hidden_states = layer_norm2(hidden_states)\n",
    "\n",
    "# Fully connected\n",
    "residual = hidden_states\n",
    "hidden_states = nn.ReLU()(fc1(hidden_states))\n",
    "hidden_states = fc2(hidden_states)\n",
    "hidden_states = hidden_states + residual\n",
    "hidden_states = layer_norm3(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dabeac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 256])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output = hidden_states.clone()\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a104ffb",
   "metadata": {},
   "source": [
    "# Final Box and class prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cd6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 300, 2])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is needed because  model predicts an offset in unconstrained space \n",
    "# By applying inverse_sigmoid(reference_points), we map the reference points from [0, 1] to unconstrained space,which is  the same space as the predicted offset.\n",
    "# Then bring it back to the constrained space , by appling sigmoid, this making learning faster.\n",
    "\n",
    "reference_points_with_inverse_sigmoid = torch.special.logit(encoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2a9e8cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final box head shape torch.Size([4, 300, 10])\n",
      "Final pred boxes head shape torch.Size([4, 300, 4]) \n"
     ]
    }
   ],
   "source": [
    "# This is needed because  model predicts an offset in unconstrained space \n",
    "# By applying inverse_sigmoid(reference_points), we map the reference points from [0, 1] to unconstrained space,which is  the same space as the predicted offset.\n",
    "# Then bring it back to the constrained space , by appling sigmoid, this making learning faster.\n",
    "\n",
    "reference_points_with_inverse_sigmoid = torch.special.logit(reference_points)\n",
    "\n",
    "# say we have 10 classes\n",
    "num_class = 10 \n",
    "class_pred = nn.Linear(embdedding_dim,num_class)\n",
    "box_head  = nn.Sequential(\n",
    "    nn.Linear(embdedding_dim, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(512, 4),\n",
    ")\n",
    "\n",
    "output_classes = class_pred(encoder_output)\n",
    "box_pred = box_head(encoder_output)\n",
    "box_pred[...,:2] += reference_points_with_inverse_sigmoid\n",
    "pred_boxes = box_pred.sigmoid()\n",
    "print(f\"Final box head shape {output_classes.shape}\")\n",
    "print(f\"Final pred boxes head shape {pred_boxes.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "471f3a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses are similart to DETR, only difference is Deformable detr uses focal loss for  classification and for pred boxes, it uses the same loss like DETR where the losses are a combination of l1 loss and Generalized IOU loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa09a8e",
   "metadata": {},
   "source": [
    "# Deformable Attention: Compute and Memory Complexity\n",
    "\n",
    "We derive the compute and memory complexity for each attention type, specializing to the **encoder** and **decoder** settings, and show how $L$ (levels) and $K$ (points per level) enter the formulas.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- $N_q$: number of queries  \n",
    "- $N_k$: number of keys  \n",
    "- $M$: number of heads  \n",
    "- $C$: channel dimension  \n",
    "- $C_v = C/M$: per-head dimension  \n",
    "- $H \\times W$: spatial size of a single feature map  \n",
    "- $H_l \\times W_l$: spatial size at level $l$  \n",
    "- $S = \\sum_{l=1}^L H_l W_l$: total token count across levels  \n",
    "- $K$: sampled points per head per level  \n",
    "- $L$: number of feature levels  \n",
    "\n",
    "## 0) Preliminaries: What Contributes to Cost?\n",
    "\n",
    "For any attention block, there are four compute buckets:\n",
    "\n",
    "1. **Linear projections** to form $Q, K, V$ (and the output projection): costs scale like $\\mathcal{O}(\\cdot C^2)$.\n",
    "2. **Score computation** (e.g., $QK^\\top$ or its sparse substitute): costs scale like $\\mathcal{O}(N_q N_k C_v M) = \\mathcal{O}(N_q N_k C)$ for dense attention.\n",
    "3. **Softmax + weighting**: typically $\\mathcal{O}(N_q N_k M)$ for softmax, and $\\mathcal{O}(N_q N_k C)$ for multiplying by $V$; the latter usually dominates.\n",
    "4. **Sampling / Interpolation** *(deformable attention only)*: adds a term of approximately $\\mathcal{O}(N_q \\cdot \\text{\\#samples} \\cdot C)$; Appendix A.1 counts this as a constant **\"5\"** times per sample for bilinear interpolation + reduct\n",
    "\n",
    "Appendix A.1 counts this as a constant **\"5\"** times per sample for bilinear interpolation + reduction.\n",
    "\n",
    "Memory is dominated by storing the attention weights: $\\mathcal{O}(N_q N_k M)$ for dense vs. $\\mathcal{O}(N_q M K)$ (single-scale) or $\\mathcal{O}(N_q M L K)$ (multi-scale).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Standard Multi-Head Attention (Eq. (1))\n",
    "\n",
    "$$\n",
    "\\text{MultiHeadAttn}(z_q, x) = \\sum_{m=1}^M W_m \\sum_{k \\in \\mathcal{K}} A_{mqk} W'_m x_k\n",
    "$$\n",
    "\n",
    "**Compute:**\n",
    "\n",
    "- Projections:\n",
    "    - $Q$: $\\mathcal{O}(N_q C^2)$\n",
    "    - $K, V$: $\\mathcal{O}(N_k C^2)$\n",
    "- Scores ($QK^\\top$): $\\mathcal{O}(M \\cdot N_q N_k C_v) = \\mathcal{O}(N_q N_k C)$\n",
    "- Softmax: $\\mathcal{O}(M \\cdot N_q N_k)$\n",
    "- Weighted sum (AV): $\\mathcal{O}(M \\cdot N_q N_k C_v) = \\mathcal{O}(N_q N_k C)$\n",
    "- Output projection: $\\mathcal{O}(N_q C^2)$\n",
    "\n",
    "**Total (dense attention):**\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal{O}\\big(N_q C^2 + N_k C^2 + N_q N_k C\\big)}\n",
    "$$\n",
    "\n",
    "**Memory:**\n",
    "\n",
    "- Attention weights: $\\mathcal{O}(M N_q N_k)$ (dominant)\n",
    "- Key/value caches: $\\mathcal{O}(N_k C)$\n",
    "\n",
    "**Specializations:**\n",
    "\n",
    "- **DETR encoder** (self-attention over pixels): $N_q = N_k = S$\n",
    "\n",
    "    $$\n",
    "    \\mathcal{O}(S^2 C) + \\mathcal{O}(S C^2) \\quad \\text{(dominated by $S^2C$)}\n",
    "    $$\n",
    "\n",
    "- **DETR decoder cross-attention**: $N_q = N$ queries, $N_k = S$ pixels\n",
    "\n",
    "    $$\n",
    "    \\mathcal{O}(N S C) + \\mathcal{O}((N+S)C^2)\n",
    "    $$\n",
    "\n",
    "- **DETR decoder self-attention** (queries only):\n",
    "\n",
    "    $$\n",
    "    \\mathcal{O}(2 N C^2 + N^2 C)\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Single-Scale Deformable Attention (Eq. (2))\n",
    "\n",
    "$$\n",
    "\\text{DeformAttn}(z_q, p_q, x) = \\sum_{m=1}^M W_m \\sum_{k=1}^K A_{mqk} W'_m x(p_q + p_{mqk})\n",
    "$$\n",
    "\n",
    "Each query attends $K$ sampled points per head around reference $p_q$. Sampling uses bilinear interpolation.\n",
    "\n",
    "**Compute (Appendix A.1 derivation):**\n",
    "\n",
    "- Predict offsets + weights (a single linear with $3MK$ output channels over $z_q$): $\\mathcal{O}(3 N_q C M K)$\n",
    "- Value projection ($W'_m x$): two possible ways\n",
    "    - Precompute once on the whole map: $\\mathcal{O}(H W C^2)$\n",
    "    - Or do per sampled value: $\\mathcal{O}(N_q K C^2)$\n",
    "- Sampling + weighted sum (bilinear + reduce): approx 5 ops per sample per channel: $\\mathcal{O}(5 N_q K C)$\n",
    "- Output projection: $\\mathcal{O}(N_q C^2)$\n",
    "\n",
    "Putting it together (App. A.1):\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathcal{O}\\Big(N_q C^2 + \\min(H W C^2, N_q K C^2) + 5 N_q K C + 3 N_q C M K\\Big)\n",
    "}\n",
    "$$\n",
    "\n",
    "For typical settings ($M=8$, $K \\leq 4$, $C=256$), the paper notes $5K + 3MK \\ll C$, yielding the simplification:\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal{O}\\big(2 N_q C^2 + \\min(H W C^2, N_q K C^2)\\big)}\n",
    "$$\n",
    "\n",
    "**Memory:**\n",
    "\n",
    "- Attention weights: $\\mathcal{O}(M N_q K)$\n",
    "- Offsets: $\\mathcal{O}(M N_q K \\cdot 2)$\n",
    "- No dense $(N_q \\times N_k)$ matrix---this is the major win.\n",
    "\n",
    "**Specializations:**\n",
    "\n",
    "- **Encoder (single-scale, queries are pixels):** $N_q = HW$  \n",
    "  With precomputation ($W'_m x$): complexity becomes $\\mathcal{O}(HW C^2)$, i.e. **linear in spatial size** (vs. quadratic for dense).\n",
    "- **Decoder cross-attention (single-scale):** $N_q = N$  \n",
    "  With per-query sampled values: $\\mathcal{O}(N K C^2)$ (independent of $HW$).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Multi-Scale Deformable Attention (Eq. (3))\n",
    "\n",
    "$$\n",
    "\\text{MSDeformAttn}(z_q, \\hat{p}_q, \\{x_l\\}_{l=1}^L)\n",
    "= \\sum_{m=1}^M W_m \\sum_{l=1}^L \\sum_{k=1}^K\n",
    "A_{mlqk} W'_m x_l(\\phi_l(\\hat{p}_q) + p_{mlqk})\n",
    "$$\n",
    "\n",
    "Each query samples $(L \\times K)$ points total.\n",
    "\n",
    "**Compute:**\n",
    "\n",
    "- Predict offsets + weights: $\\mathcal{O}(3 N_q C M L K)$\n",
    "- Value projections (choose one):\n",
    "    - Precompute on all levels: $\\sum_{l=1}^L \\mathcal{O}(H_l W_l C^2) = \\mathcal{O}(S C^2)$\n",
    "    - Or per sampled value: $\\mathcal{O}(N_q L K C^2)$\n",
    "- Sampling + weighted sum: $\\mathcal{O}(5 N_q L K C)$\n",
    "- Output projection: $\\mathcal{O}(N_q C^2)$\n",
    "\n",
    "**Total (multi-scale):**\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathcal{O}\\Big(N_q C^2 + \\min(S C^2, N_q L K C^2) + 5 N_q L K C + 3 N_q C M L K\\Big)\n",
    "}\n",
    "$$\n",
    "\n",
    "Under the same \"small $(M, K, L)$\" assumption as the paper (App. A.1):\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathcal{O}\\big(2 N_q C^2 + \\min(S C^2, N_q L K C^2)\\big)\n",
    "}\n",
    "$$\n",
    "\n",
    "**Memory:**\n",
    "\n",
    "- Attention weights: $\\mathcal{O}(M N_q L K)$\n",
    "- Offsets: $\\mathcal{O}(M N_q L K \\cdot 2)$\n",
    "- Again, no dense $(N_q \\times S)$ matrix.\n",
    "\n",
    "**Specializations:**\n",
    "\n",
    "- **Deformable DETR encoder** (multi-scale, queries are pixels across all levels): $N_q = S$  \n",
    "  Precompute values per level $\\rightarrow$\n",
    "  $$\n",
    "  \\boxed{\\mathcal{O}(S C^2)} \\quad \\text{(linear in total tokens across scales)}\n",
    "  $$\n",
    "  This is the paper's claim that encoder complexity becomes linear in spatial size (Section 4.1).\n",
    "- **Deformable DETR decoder cross-attention:** $N_q = N$ queries  \n",
    "  Use per-query samples $\\rightarrow$\n",
    "  $$\n",
    "  \\boxed{\\mathcal{O}(N L K C^2)} \\quad \\text{(independent of spatial resolution)}\n",
    "  $$\n",
    "- **Decoder self-attention:** unchanged from standard: $\\mathcal{O}(2 N C^2 + N^2 C)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Side-by-side Summary (Dominant Terms)\n",
    "\n",
    "| Block                   | Dense MHA (DETR)                                   | Deformable (single-scale)                        | Deformable (multi-scale)                        |\n",
    "|-------------------------|----------------------------------------------------|--------------------------------------------------|-------------------------------------------------|\n",
    "| **Generic**             | $\\mathcal{O}(N_q C^2 + N_k C^2 + N_q N_k C)$       | $\\mathcal{O}(2 N_q C^2 + \\min(HW C^2, N_q K C^2))$ | $\\mathcal{O}(2 N_q C^2 + \\min(S C^2, N_q L K C^2))$ |\n",
    "| **Encoder**             | $N_q = N_k = S \\Rightarrow \\mathcal{O}(S^2 C)$     | $N_q = HW \\Rightarrow \\mathcal{O}(HW C^2)$       | $N_q = S \\Rightarrow \\boxed{\\mathcal{O}(S C^2)}$ |\n",
    "| **Decoder cross-attn**  | $N_q = N, N_k = S \\Rightarrow \\mathcal{O}(N S C)$  | $\\mathcal{O}(N K C^2)$                           | $\\boxed{\\mathcal{O}(N L K C^2)}$                |\n",
    "| **Decoder self-attn**   | $\\mathcal{O}(2 N C^2 + N^2 C)$                     | same                                             | same                                            |\n",
    "| **Attention memory**    | $\\mathcal{O}(M N_q N_k)$                           | $\\mathcal{O}(M N_q K)$                           | $\\mathcal{O}(M N_q L K)$                        |\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Practical Takeaways\n",
    "\n",
    "- **Encoder:** dense self-attention is quadratic in spatial tokens; deformable makes it **linear** in the total number of tokens across scales ($S$).\n",
    "- **Decoder cross-attention:** deformable cost depends on $(L K)$ (small, fixed hyperparameters), not on image size, so it scales with the number of queries ($N$) and channel dimension ($C$), **not** with $H, W$.\n",
    "- **Memory:** deformable avoids the $\\mathcal{O}(N_q N_k)$ attention matrix, replacing it with $\\mathcal{O}(N_q L K)$ structures---crucial for speed and convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd426337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8eaf0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
