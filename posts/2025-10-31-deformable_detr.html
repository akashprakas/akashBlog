<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-10-31">

<title>Deformable DETR a tiny Implemenation â€“ Akashâ€™s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-6d7e0e055978adcb0dc3bbe98a6d8351.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c4a17ede6a1e121a6ac7785b1d8267d8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-ef2ad32b6c2cbdb1d3e607ed113afe70.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-c4a17ede6a1e121a6ac7785b1d8267d8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Akashâ€™s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/akashprakas"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/akashaapz"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deformable DETR a tiny Implemenation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">jupyter</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 31, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview">Overview</a></li>
  <li><a href="#why-was-deformable-detr-needed" id="toc-why-was-deformable-detr-needed" class="nav-link" data-scroll-target="#why-was-deformable-detr-needed">Why was Deformable DETR needed?</a></li>
  <li><a href="#convolutional-backbone" id="toc-convolutional-backbone" class="nav-link" data-scroll-target="#convolutional-backbone">Convolutional backbone</a></li>
  <li><a href="#transformer-encoder" id="toc-transformer-encoder" class="nav-link" data-scroll-target="#transformer-encoder">Transformer Encoder</a></li>
  <li><a href="#transformer-decoder" id="toc-transformer-decoder" class="nav-link" data-scroll-target="#transformer-decoder">Transformer Decoder</a></li>
  <li><a href="#final-box-and-class-prediction" id="toc-final-box-and-class-prediction" class="nav-link" data-scroll-target="#final-box-and-class-prediction">Final Box and class prediction</a></li>
  <li><a href="#deformable-attention-compute-and-memory-complexity" id="toc-deformable-attention-compute-and-memory-complexity" class="nav-link" data-scroll-target="#deformable-attention-compute-and-memory-complexity">Deformable Attention: Compute and Memory Complexity</a></li>
  <li><a href="#key-improvements-of-deformable-detr-over-detr" id="toc-key-improvements-of-deformable-detr-over-detr" class="nav-link" data-scroll-target="#key-improvements-of-deformable-detr-over-detr">Key Improvements of Deformable DETR over DETR</a></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<div id="86a36c34" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'torch version </span><span class="sc">{</span>torch<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">'</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch version 2.9.0+cpu</code></pre>
</div>
</div>
<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This blog provides a code-level walkthrough of the internals of Deformable DETR and its core componentâ€”deformable attentionâ€”to understand how it works in practice. Iâ€™ll explore the full pipeline: starting from how a batch of images flows through the convolutional backbone, then into the encoder, and finally how deformable attention operates within the architecture both in encoder side and decoder cross attention. Along the way, Iâ€™ll highlight where Deformable DETR aligns with the original DETR and where it diverges. All examples shown below are executable in the accompanying notebook and have been tested to work end-to-end.</p>
<section id="prerequisites" class="level5">
<h5 class="anchored" data-anchor-id="prerequisites">Prerequisites</h5>
<p>I assume the reader have good understanding of <a href="https://arxiv.org/abs/2005.12872">DETR</a></p>
</section>
</section>
<section id="why-was-deformable-detr-needed" class="level2">
<h2 class="anchored" data-anchor-id="why-was-deformable-detr-needed">Why was Deformable DETR needed?</h2>
<p><a href="https://arxiv.org/abs/2010.04159">Deformable DETR</a> is an enhancement of DETR, which was one of the first approaches to apply transformers to object detection. While DETR introduced a novel paradigm, it faced two major challenges</p>
<pre><code>1. Difficulty Detecting Small Objects</code></pre>
<p>Most modern object detection networks leverage Feature Pyramid Networks (FPN) to handle objects at multiple scales. However, DETR cannot easily incorporate FPN because its global self-attention operates over the entire feature map, making multi-scale attention computationally expensive. Deformable DETR addresses this by introducing multi-scale deformable attention, which selectively attends to a small set of key points across different feature levels instead of the entire map. This enables efficient multi-scale feature aggregation without exploding computational cost.</p>
<pre><code>2. Long Training Time</code></pre>
<p>DETR requires extensive training because the model must learn which parts of the feature map to attend to from scratch, which is slow to converge. Deformable DETR solves this by using a Deformable Attention Module, which focuses on a sparse set of relevant keys rather than all possible keys. This reduces complexity and accelerates convergence significantly.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ðŸ“º Watch the Explanation in Youtube
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://youtu.be/RY54XtV8SzM">Introduction to DETR</a></p>
</div>
</div>
</section>
<section id="convolutional-backbone" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-backbone">Convolutional backbone</h2>
<p><img src="images/deformDetrBackbone.png" class="img-fluid"></p>
<ol type="1">
<li><p><strong>Feature Extraction via FPN</strong><br>
The input image is passed through a Feature Pyramid Network (FPN), which extracts multi-scale feature maps from different layers of the backbone. These feature maps capture hierarchical representations at varying resolutions.</p></li>
<li><p><strong>Positional Embedding for Feature Maps</strong><br>
For each feature map, we compute a positional embedding using sine-cosine encoding, similar to the method described in the original Transformer paper.<br>
However, in this setup: (which is same as DETR)</p>
<ul>
<li>The feature maps are 2D (height Ã— width), so the positional encoding must reflect both spatial dimensions.</li>
<li>If the embedding dimension is 256:
<ul>
<li>The first 128 dimensions encode <strong>horizontal (x-axis)</strong> positions.</li>
<li>The next 128 dimensions encode <strong>vertical (y-axis)</strong> positions.</li>
</ul></li>
<li>This results in a positional embedding tensor of shape <code>(B, 256, H, W)</code> for each feature map, where <code>B</code> is the batch size.</li>
</ul></li>
<li><p><strong>Level Embeddings</strong> In addition to positional embeddings, DETR introduces a learnable level embedding for each feature scale. For example, with 4 feature levels, we have a tensor of shape (4, 256). This embedding is added to the corresponding feature map after positional encoding, helping the model distinguish between different scales.</p></li>
</ol>
<div id="fbe6b1b4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>pixel_values <span class="op">=</span> torch.randn(<span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">1065</span>,<span class="dv">1066</span>) </span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 .Initially we need to pass the images through the FPN and get features across different layers,</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Also we need to get positional embedding for each of the feature map, the positional embedding</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  is similar to the normal sine-cosine positional embedding in the original paper,</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#  the only difference here is that since we have HxW in the feature domain , </span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># suppose if our embedding dim is 256,</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#  we will have them alingned in such a way that the first 128 corresponds</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">#  to vertical and the next 128 corresponds</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># to vertical so that in the end we end up with 256 and that encodes both vertical </span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># and horizontal positions. https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Suppose we get feature map from 4 layers and let </span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># them be (4,512,134,134) ,(4,1024,67,67) , (4,2048,34,34) ,(4,2048,17,17) </span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># [Note the actual feature map in the paper is created by </span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># an additional conv+group norm] and there positional embeddings w</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># will have the same size as well. but with the corresponding embedding dim, </span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># so they will be of size (4,256,134,134) ,(4,256,67,67) ,(4,256,34,34) ,(4,256,17,17)</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>feature_shapes <span class="op">=</span> [</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">512</span>, <span class="dv">134</span>, <span class="dv">134</span>),</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">1024</span>, <span class="dv">67</span>, <span class="dv">67</span>),</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">2048</span>, <span class="dv">34</span>, <span class="dv">34</span>),</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">2048</span>, <span class="dv">17</span>, <span class="dv">17</span>)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Positional embedding shapes (same spatial dims, but channel dim = 256)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>embedding_shapes <span class="op">=</span> [</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">256</span>, <span class="dv">134</span>, <span class="dv">134</span>),</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">256</span>, <span class="dv">67</span>, <span class="dv">67</span>),</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">256</span>, <span class="dv">34</span>, <span class="dv">34</span>),</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="dv">256</span>, <span class="dv">17</span>, <span class="dv">17</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co"># original implementation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/backbone.py#L71</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>feature_maps <span class="op">=</span> [torch.randn(shape) <span class="cf">for</span> shape <span class="kw">in</span> feature_shapes]</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># original implemenation here https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/position_encoding.py#L55</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>positional_embeddings <span class="op">=</span> [torch.randn(shape) <span class="cf">for</span> shape <span class="kw">in</span> embedding_shapes]</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 4 . Now we have to have a 1x1 conv layer to reduce the channel dimension of the feature so that they match the embedding dimension of 256</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>conv_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">512</span>, out_channels<span class="op">=</span><span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">1024</span>, out_channels<span class="op">=</span><span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">2048</span>, out_channels<span class="op">=</span><span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(in_channels<span class="op">=</span><span class="dv">2048</span>, out_channels<span class="op">=</span><span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the 1x1 conv layers</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>reduced_feature_maps <span class="op">=</span> [conv(feature) <span class="cf">for</span> conv, feature <span class="kw">in</span> <span class="bu">zip</span>(conv_layers, feature_maps)]</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (fmap,pos_emb) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(reduced_feature_maps,positional_embeddings)):</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Reduced feature map </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> shape:"</span>, fmap.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Reduced feature map 1 shape: torch.Size([4, 256, 134, 134])
Reduced feature map 2 shape: torch.Size([4, 256, 67, 67])
Reduced feature map 3 shape: torch.Size([4, 256, 34, 34])
Reduced feature map 4 shape: torch.Size([4, 256, 17, 17])</code></pre>
</div>
</div>
<div id="f8f37015" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 . Also we need a learnable Level embedding for each levels , since here we are using 4 layers, </span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and 256 embedding dim , the size of the level embedding will be (4,256)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Learnable level embedding (in actual model this would be nn.Parameter)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>num_levels <span class="op">=</span> <span class="bu">len</span>(reduced_feature_maps)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>level_embedding <span class="op">=</span> nn.Parameter(torch.randn((num_levels, embedding_dim)))  <span class="co"># shape: (num_levels, embedding_dim)</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">#6. Now we need to flatten and transpose the features and positional embedding </span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># so they become the similar shape like token_len X embedding_dim , </span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># for example the first feature map will become (4,134*134,256) ,similarly we have do this </span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># for all the feature maps and the positional embedding. </span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># and one additional thing to do is to add the level embedding to the positional embedding.</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>features_flatten <span class="op">=</span> []</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>positional_and_level_embedding_flattened <span class="op">=</span> []</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> level, (feature, pos_emb) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(reduced_feature_maps, positional_embeddings)):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Flatten and transpose: (B, C, H, W) -&gt; (B, HW, C)</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    feature_flatten <span class="op">=</span> feature.flatten(<span class="dv">2</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    positional_plus_level_embed <span class="op">=</span> pos_emb.flatten(<span class="dv">2</span>).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="op">+</span> level_embedding[level].view(<span class="dv">1</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    features_flatten.append(feature_flatten)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    positional_and_level_embedding_flattened.append(positional_plus_level_embed)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print shapes</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Level </span><span class="sc">{</span>level <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Feature shape: </span><span class="sc">{</span>feature_flatten<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Positional + Level Embedding shape: </span><span class="sc">{</span>positional_plus_level_embed<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    </span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Level 1:
  Feature shape: torch.Size([4, 17956, 256])
  Positional + Level Embedding shape: torch.Size([4, 17956, 256])
Level 2:
  Feature shape: torch.Size([4, 4489, 256])
  Positional + Level Embedding shape: torch.Size([4, 4489, 256])
Level 3:
  Feature shape: torch.Size([4, 1156, 256])
  Positional + Level Embedding shape: torch.Size([4, 1156, 256])
Level 4:
  Feature shape: torch.Size([4, 289, 256])
  Positional + Level Embedding shape: torch.Size([4, 289, 256])</code></pre>
</div>
</div>
<div id="da3098e3" class="cell" data-execution_count="27">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 7: Concatenate along sequence dimension (dim=1)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>inputs_embeds <span class="op">=</span> torch.cat(features_flatten, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># shape: (B, total_seq_len, 256)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>position_embeddings <span class="op">=</span> torch.cat(positional_and_level_embedding_flattened, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># shape: (B, total_seq_len, 256)</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Concatenated Inputs Embeds shape:"</span>, inputs_embeds.shape)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Concatenated Position Embeddings shape:"</span>, position_embeddings.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Concatenated Inputs Embeds shape: torch.Size([4, 23890, 256])
Concatenated Position Embeddings shape: torch.Size([4, 23890, 256])</code></pre>
</div>
</div>
</section>
<section id="transformer-encoder" class="level2">
<h2 class="anchored" data-anchor-id="transformer-encoder">Transformer Encoder</h2>
<p>Once we have the flattened feature maps from multiple scales (typically 4 levels), these serve as the <strong>queries</strong> in the encoder. In standard Transformer attention, each query interacts with all keys via dot-product attention, which becomes computationally infeasible when the sequence length is large (e.g., 23,890 tokens). This quadratic complexity is a major bottleneck.</p>
<p>To address this, <strong>Deformable DETR introduces a sparse attention mechanism</strong>. Instead of attending to all positions, each query attends to a small, fixed number of sampling pointsâ€”<strong>K = 4 per feature level</strong>, across <strong>L = 4 levels</strong>, resulting in <strong>16 sampling points per query</strong>. These points are not fixed but are <strong>learned dynamically</strong>.</p>
<p>Hereâ€™s how it works:</p>
<ul>
<li>Each query has a <strong>reference point</strong> (normalized coordinates in [0, 1]Â²), typically its own spatial location.</li>
<li>Instead of using separate query and key projection matrices, Deformable DETR uses:
<ul>
<li>A <strong>sampling offset prediction layer</strong>: a feed-forward network that predicts offsets from the reference point for each attention head and feature level.</li>
<li>An <strong>attention weight prediction layer</strong>: another feed-forward network that assigns attention scores to each of the sampled points.</li>
</ul></li>
<li>The sampled points are fractional, so <strong>bilinear interpolation</strong> is used to extract features from the input maps.</li>
<li>The attention weights are normalized across all sampled points (total of 16 per query), and the final output is a weighted sum of the interpolated features.</li>
</ul>
<p>This design drastically reduces computational complexity while maintaining the ability to model spatial relationships. It also enables efficient multi-scale feature aggregation without relying on FPNs.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ðŸ“º Watch the Explanation in Youtube
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://youtu.be/40Qs_y63Bx4">Deformable Attention</a></p>
</div>
</div>
<div id="2cf651a8" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 8. we need to apply a initial dropout before passing it to the encoder </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>inputs_embeds <span class="op">=</span> nn.functional.dropout(inputs_embeds, p<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> inputs_embeds.shape[<span class="dv">0</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co">#9. Generating the reference points, so this is a concept that is similar to the deformable convolution ,</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># so basically for each feature_point/query</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># in the feature map we need to look into the corresponding point in the other feature</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># map as well, feature maps a re normilized  based on their height and width, </span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># so we can look for the corresponding point for each query in different points as well, here</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">#original implemenation https://github.com/fundamentalvision/Deformable-DETR/blob/11169a60c33333af00a4849f1808023eba96a931/models/deformable_transformer.py#L238 </span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>spatial_shapes_list <span class="op">=</span> [(<span class="dv">134</span>, <span class="dv">134</span>), (<span class="dv">67</span>, <span class="dv">67</span>), (<span class="dv">34</span>, <span class="dv">34</span>), (<span class="dv">17</span>, <span class="dv">17</span>)]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>reference_points_list <span class="op">=</span> []</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> H_, W_ <span class="kw">in</span> spatial_shapes_list:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create meshgrid of normalized coordinates</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        ref_y, ref_x <span class="op">=</span> torch.meshgrid(</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>            torch.linspace(<span class="fl">0.5</span>, H_ <span class="op">-</span> <span class="fl">0.5</span>, H_, dtype<span class="op">=</span>torch.float32),</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>            torch.linspace(<span class="fl">0.5</span>, W_ <span class="op">-</span> <span class="fl">0.5</span>, W_, dtype<span class="op">=</span>torch.float32),</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>            indexing<span class="op">=</span><span class="st">'ij'</span>  <span class="co"># Important for correct axis ordering</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Normalize</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        ref_y <span class="op">=</span> ref_y.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> H_</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        ref_x <span class="op">=</span> ref_x.reshape(<span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> W_</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stack and expand to batch size</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        ref <span class="op">=</span> torch.stack((ref_x, ref_y), dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># shape: (H_*W_, 2)</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        ref <span class="op">=</span> ref[<span class="va">None</span>].expand(batch_size, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># shape: (B, H_*W_, 2)</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        reference_points_list.append(ref)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate all levels</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>reference_points <span class="op">=</span> torch.cat(reference_points_list, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># shape: (B, total_seq_len, 2)</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Expand to include level dimension</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>reference_points <span class="op">=</span> reference_points[:, :, <span class="va">None</span>, :]  <span class="co"># shape: (B, total_seq_len, 1, 2)</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat across levels</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>num_levels <span class="op">=</span> <span class="bu">len</span>(spatial_shapes_list)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>reference_points <span class="op">=</span> reference_points.expand(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, num_levels, <span class="op">-</span><span class="dv">1</span>)  <span class="co"># shape: (B, total_seq_len, L, 2)</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Reference points shape input to encoder "</span>,reference_points.shape)</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Reference points shape input to encoder  torch.Size([4, 23890, 4, 2])</code></pre>
</div>
</div>
<div id="937677b6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#so for now each query we have 4 positions (x,y) across 4 different channels, now this will be passed to the encoder.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co">## DEFORMABLE MULTI SCALE </span><span class="al">ATTENTION</span><span class="co">.</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># params</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>num_levels  <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>n_points  <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>embdedding_dim <span class="op">=</span> inputs_embeds.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>batch_size, num_queries, _ <span class="op">=</span> inputs_embeds.shape</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>fc1 <span class="op">=</span> nn.Linear(embdedding_dim, <span class="dv">512</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, embdedding_dim)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>layer_norm1 <span class="op">=</span> nn.LayerNorm(embdedding_dim)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>layer_norm2 <span class="op">=</span> nn.LayerNorm(embdedding_dim)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>final_layer_norm <span class="op">=</span> nn.LayerNorm(embdedding_dim)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># learnable parameters in the layer</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>sampling_offsets_layer <span class="op">=</span> nn.Linear(embdedding_dim, num_heads <span class="op">*</span> num_levels <span class="op">*</span> n_points <span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>attention_weights_layer <span class="op">=</span> nn.Linear(embdedding_dim,num_heads <span class="op">*</span> num_levels <span class="op">*</span> n_points)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>value_projection_layer  <span class="op">=</span> nn.Linear(embdedding_dim,embdedding_dim)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>output_projection_layer <span class="op">=</span> nn.Linear(embdedding_dim,embdedding_dim)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a><span class="co">#initially we add the poistional_embedding to the input_embeds</span></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> inputs_embeds <span class="op">+</span> position_embeddings</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> value_projection_layer(inputs_embeds)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> value.view(batch_size,num_queries, num_heads,embdedding_dim<span class="op">//</span>num_heads)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Value shape = </span><span class="sc">{</span>value<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="co"># note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets_layer(hidden_states)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,<span class="dv">2</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling_offsets are predicted in a normalized, unitless space (not tied to any particular feature map size).</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Each feature map (level) can have a different spatial resolution (height, width).</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a><span class="co"># To convert the offsets into actual positions on each feature map, they must be scaled relative to that map's size.</span></span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>offset_normalizer <span class="op">=</span> torch.tensor(spatial_shapes_list)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>offset_normalizer <span class="op">=</span> offset_normalizer[<span class="va">None</span>,<span class="va">None</span>,<span class="va">None</span>,:,<span class="va">None</span>,:]</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets<span class="op">/</span>offset_normalizer</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Offset Normalizer </span><span class="sc">{</span>offset_normalizer<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights_layer(hidden_states)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights.view(batch_size,num_queries,num_heads,num_levels<span class="op">*</span>n_points)</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a><span class="co"># note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> torch.nn.functional.softmax(attention_weights, <span class="op">-</span><span class="dv">1</span>).view(batch_size,num_queries,num_heads,num_levels,n_points) </span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sampling offset shape = </span><span class="sc">{</span>sampling_offsets<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights shape = </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points ,</span></span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to look into 4 more points across 8 different heads</span></span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a><span class="co"># so initially we had for each query 1 points account each feature dimension making it total 4 and</span></span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a><span class="co">#  now when we add this sampling offsets it makes 4 more across 8 differenet heads</span></span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>reference_points <span class="op">=</span> reference_points[:,:,<span class="va">None</span>,:,<span class="va">None</span>,:]</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reference points with unsqueezed dimension for head and levels = </span><span class="sc">{</span>reference_points<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>sampling_location <span class="op">=</span> reference_points <span class="op">+</span> sampling_offsets</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final sampling locations = </span><span class="sc">{</span>sampling_location<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Value shape = torch.Size([4, 23890, 8, 32])
Offset Normalizer torch.Size([1, 1, 1, 4, 1, 2])
Sampling offset shape = torch.Size([4, 23890, 8, 4, 4, 2])
Attention weights shape = torch.Size([4, 23890, 8, 4, 4]) 

Reference points with unsqueezed dimension for head and levels = torch.Size([4, 23890, 1, 4, 1, 2])
Final sampling locations = torch.Size([4, 23890, 8, 4, 4, 2])</code></pre>
</div>
</div>
<section id="deformable-attention" class="level4">
<h4 class="anchored" data-anchor-id="deformable-attention">Deformable Attention</h4>
<p><img src="images/defor_detrn_attn.png" class="img-fluid"></p>
<div id="960fe384" class="cell" data-execution_count="30">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the value tensor into per-level chunks based on spatial shapes</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>value_list <span class="op">=</span> value.split([h <span class="op">*</span> w <span class="cf">for</span> h, w <span class="kw">in</span> spatial_shapes_list], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>batch_size, _, num_heads, hidden_dim <span class="op">=</span> value.shape</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of each level's value tensor</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> level, feature <span class="kw">in</span> <span class="bu">enumerate</span>(value_list):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Splitted feature at level </span><span class="sc">{</span>level<span class="sc">}</span><span class="ss"> --&gt; </span><span class="sc">{</span>feature<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>sampling_grids <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> sampling_location <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sampling grid shape  = </span><span class="sc">{</span>sampling_grids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>sampling_value_list <span class="op">=</span> []</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> level_id, (height, width) <span class="kw">in</span> <span class="bu">enumerate</span>(spatial_shapes_list):</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape value tensor for grid sampling:</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (B, H*W, num_heads, C) â†’ (B, num_heads, H*W, C) â†’ (B*num_heads, C, H, W)</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    value_l <span class="op">=</span> (</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        value_list[level_id]</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        .flatten(<span class="dv">2</span>)               <span class="co"># (B, H*W, num_heads * C)</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        .transpose(<span class="dv">1</span>, <span class="dv">2</span>)          <span class="co"># (B, num_heads * C, H*W)</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        .reshape(batch_size <span class="op">*</span> num_heads, hidden_dim, height, width)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Value at level </span><span class="sc">{</span>level_id<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>value_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape sampling grid:</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (B, num_queries, num_heads, num_levels, num_points, 2)</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># â†’ (B, num_heads, num_queries, num_points, 2)</span></span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># â†’ (B*num_heads, num_queries, num_points, 2)</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    sampling_grid_l <span class="op">=</span> sampling_grids[:, :, :, level_id].transpose(<span class="dv">1</span>, <span class="dv">2</span>).flatten(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample values using bilinear interpolation</span></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    sampling_value_l <span class="op">=</span> nn.functional.grid_sample(</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        value_l,</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        sampling_grid_l,</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>        padding_mode<span class="op">=</span><span class="st">"zeros"</span>,</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        align_corners<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    sampling_value_list.append(sampling_value_l)</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Splitted feature at level 0 --&gt; torch.Size([4, 17956, 8, 32])
Splitted feature at level 1 --&gt; torch.Size([4, 4489, 8, 32])
Splitted feature at level 2 --&gt; torch.Size([4, 1156, 8, 32])
Splitted feature at level 3 --&gt; torch.Size([4, 289, 8, 32])

Sampling grid shape  = torch.Size([4, 23890, 8, 4, 4, 2]) 

Value at level 0 torch.Size([32, 32, 134, 134])
Value at level 1 torch.Size([32, 32, 67, 67])
Value at level 2 torch.Size([32, 32, 34, 34])
Value at level 3 torch.Size([32, 32, 17, 17])</code></pre>
</div>
</div>
<div id="787449fd" class="cell" data-execution_count="31">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> sampling_value_list:</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(f.shape)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 32, 23890, 4])
torch.Size([32, 32, 23890, 4])
torch.Size([32, 32, 23890, 4])
torch.Size([32, 32, 23890, 4])</code></pre>
</div>
</div>
<div id="5d4c7b29" class="cell" data-execution_count="32">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>final_key_matrix <span class="op">=</span> torch.stack(sampling_value_list, dim<span class="op">=-</span><span class="dv">2</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Stacked value matrix shape before flattening = </span><span class="sc">{</span>final_key_matrix<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>final_key_matrix <span class="op">=</span> final_key_matrix.flatten(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Stacked value matrix shape after flattening = </span><span class="sc">{</span>final_key_matrix<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Stacked value matrix shape before flattening = torch.Size([32, 32, 23890, 4, 4])
Stacked value matrix shape after flattening = torch.Size([32, 32, 23890, 16])</code></pre>
</div>
</div>
<div id="a55ae744" class="cell" data-execution_count="33">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>            batch_size <span class="op">*</span> num_heads, <span class="dv">1</span>, num_queries, num_levels <span class="op">*</span> n_points</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>attention_weights.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>torch.Size([32, 1, 23890, 16])</code></pre>
</div>
</div>
<div id="dee924ad" class="cell" data-execution_count="34">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> final_key_matrix<span class="op">*</span>attention_weights</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>torch.Size([32, 32, 23890, 16])</code></pre>
</div>
</div>
<div id="4955f2ef" class="cell" data-execution_count="35">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>torch.Size([32, 32, 23890])</code></pre>
</div>
</div>
<div id="6ed7d2a8" class="cell" data-execution_count="36">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output.view(batch_size,num_heads<span class="op">*</span>hidden_dim,num_queries).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output_projection_layer(output)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>torch.Size([4, 23890, 256])</code></pre>
</div>
</div>
<div id="bde3f2ac" class="cell" data-execution_count="37">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed forward layers </span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> nn.functional.dropout(hidden_states,p<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> inputs_embeds <span class="op">+</span> hidden_states <span class="co"># residual</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> layer_norm1(hidden_states)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>residual <span class="op">=</span> hidden_states</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> nn.ReLU()(fc1(hidden_states))</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> nn.functional.dropout(hidden_states,p<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> fc2(hidden_states)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>hidden_states  <span class="op">=</span> nn.functional.dropout(hidden_states,p<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> residual<span class="op">+</span>hidden_states</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> layer_norm2(hidden_states)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="23f6f7fc" class="cell" data-execution_count="38">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>hidden_states.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>torch.Size([4, 23890, 256])</code></pre>
</div>
</div>
<p>The above hidden state acts as the input to the next decoder layer, here i am showing only one decoder layer and in the paper they used 8, the output of the final decoder layer will be passed as input to the decoder for cross attention.</p>
</section>
</section>
<section id="transformer-decoder" class="level2">
<h2 class="anchored" data-anchor-id="transformer-decoder">Transformer Decoder</h2>
<p>The decoder follows the same overall design as DETR but with key modifications for efficiency:</p>
<section id="learnable-object-queries" class="level4">
<h4 class="anchored" data-anchor-id="learnable-object-queries"><strong>1. Learnable Object Queries</strong></h4>
<ul>
<li>The decoder starts with a fixed set of <strong>learnable queries</strong> (300 in the paper) and <strong>learnable positional embeddings.</strong></li>
<li>These queries represent potential objects and are refined through multiple decoder layers.</li>
</ul>
</section>
<section id="self-attention" class="level4">
<h4 class="anchored" data-anchor-id="self-attention"><strong>2. Self-Attention</strong></h4>
<ul>
<li>The first step is <strong>self-attention</strong> among the 300 queries, this is similar to the DETR paper.</li>
</ul>
</section>
<section id="cross-attention-with-deformable-attention" class="level4">
<h4 class="anchored" data-anchor-id="cross-attention-with-deformable-attention"><strong>3. Cross-Attention with Deformable Attention</strong></h4>
<ul>
<li>The second step is <strong>cross-attention</strong> between the decoder queries and the encoderâ€™s multi-scale feature maps.</li>
<li>Instead of standard dense attention (which was the case in DETR), <strong>Deformable DETR uses multi-scale deformable attention</strong>, similar to the encoder:
<ul>
<li>Each query predicts a <strong>reference point</strong> in normalized coordinates.</li>
<li>Two lightweight networks predict:
<ul>
<li><strong>Sampling offsets</strong> for each attention head and feature level.</li>
<li><strong>Attention weights</strong> for the sampled points.</li>
</ul></li>
<li>For each query:
<ul>
<li><strong>K = 4 sampling points per level</strong>, across <strong>L = 4 levels</strong>, giving <strong>16 points total</strong>.</li>
<li>Features at these fractional locations are extracted using <strong>bilinear interpolation</strong>.</li>
<li>The weighted sum of these sampled features forms the cross-attention output.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="why-this-matters" class="level4">
<h4 class="anchored" data-anchor-id="why-this-matters"><strong>Why This Matters</strong></h4>
<ul>
<li>This design avoids attending to all pixels, reducing complexity from quadratic to linear in spatial size.</li>
<li>It also aligns attention with predicted object locations, improving convergence and detection accuracy.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ðŸ“º Watch the Explanation in Youtube
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://youtu.be/bvzp4X26SOs">Deformable Decoder</a></p>
</div>
</div>
<div id="e29ffd1b" class="cell" data-execution_count="39">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>encoder_output <span class="op">=</span> hidden_states.clone()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>num_query <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> encoder_output.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>num_levels</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Learnable query and positional embeddings</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>position_embeddings <span class="op">=</span> nn.Parameter(torch.randn(num_query, embedding_dim)) <span class="co">#(num_query,embedding_dim)</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>position_embeddings <span class="op">=</span> position_embeddings[<span class="va">None</span>].expand(batch_size,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>) <span class="co"># (batch_size,num_query,embedding_dim)</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>input_query <span class="op">=</span> nn.Parameter(torch.randn(num_query, embedding_dim)) <span class="co">#(num_query,embedding_dim)</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>input_query <span class="op">=</span> input_query[<span class="va">None</span>].expand(batch_size,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>) <span class="co"># (batch_size,num_query,embedding_dim) </span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>fc1 <span class="op">=</span> nn.Linear(embdedding_dim, <span class="dv">512</span>)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>fc2 <span class="op">=</span> nn.Linear(<span class="dv">512</span>, embdedding_dim)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>layer_norm1 <span class="op">=</span> nn.LayerNorm(embdedding_dim)</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>layer_norm2 <span class="op">=</span> nn.LayerNorm(embdedding_dim)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>layer_norm3 <span class="op">=</span> nn.LayerNorm(embedding_dim)</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear layer to generate reference points from positional embeddings</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>decoder_reference_point_layer <span class="op">=</span> nn.Linear(embedding_dim, <span class="dv">2</span>)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate normalized reference points in [0, 1] range</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>reference_points <span class="op">=</span> decoder_reference_point_layer(position_embeddings).sigmoid()  <span class="co"># shape: (num_query, 2)</span></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encode Reference points shape </span><span class="sc">{</span>reference_points<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Encode Reference points shape torch.Size([4, 300, 2])</code></pre>
</div>
</div>
<div id="5ed83498" class="cell" data-execution_count="40">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>reference_points_input <span class="op">=</span> reference_points[:,:,<span class="va">None</span>,:].expand(batch_size,num_query,num_levels,<span class="dv">2</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>reference_points_input.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>torch.Size([4, 300, 4, 2])</code></pre>
</div>
</div>
<div id="970b4940" class="cell" data-execution_count="41">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initially here we will have the normal self attention.</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>residual <span class="op">=</span> input_query</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>multihead_attn <span class="op">=</span> nn.MultiheadAttention(embedding_dim, num_heads)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>self_attn_output, _ <span class="op">=</span> multihead_attn(input_query<span class="op">+</span>position_embeddings, input_query<span class="op">+</span>position_embeddings, input_query)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>hidden_state_after_self_attention <span class="op">=</span> self_attn_output <span class="op">+</span> residual <span class="co"># residual connection. </span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>hidden_state_after_self_attention <span class="op">=</span> layer_norm1(hidden_state_after_self_attention)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>second_residual <span class="op">=</span> hidden_state_after_self_attention </span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Hidden state shape input to cross attention  </span><span class="sc">{</span>self_attn_output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hidden state shape input to cross attention  torch.Size([4, 300, 256])</code></pre>
</div>
</div>
<div id="3923392b" class="cell" data-execution_count="42">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>position_embeddings.shape,hidden_state_after_self_attention.shape,encoder_output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>(torch.Size([4, 300, 256]),
 torch.Size([4, 300, 256]),
 torch.Size([4, 23890, 256]))</code></pre>
</div>
</div>
<div id="6a7a3ff1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co">## DEFORMABLE MULTI SCALE </span><span class="al">ATTENTION</span><span class="co">.</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>num_levels  <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>n_points  <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>embdedding_dim <span class="op">=</span> hidden_state_after_self_attention.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>batch_size, num_queries, _ <span class="op">=</span> hidden_state_after_self_attention.shape</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># learnable parameters in the layer</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>sampling_offsets_layer <span class="op">=</span> nn.Linear(embdedding_dim, num_heads <span class="op">*</span> num_levels <span class="op">*</span> n_points <span class="op">*</span><span class="dv">2</span>)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>attention_weights_layer <span class="op">=</span> nn.Linear(embdedding_dim,num_heads <span class="op">*</span> num_levels <span class="op">*</span> n_points)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>value_projection_layer  <span class="op">=</span> nn.Linear(embdedding_dim,embdedding_dim)</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>output_projection_layer <span class="op">=</span> nn.Linear(embdedding_dim,embdedding_dim)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="co">#initially we add the poistional_embedding to the input_embeds</span></span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> hidden_state_after_self_attention <span class="op">+</span> position_embeddings</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> value_projection_layer(encoder_output)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>_,encoder_sequence_length,_ <span class="op">=</span> value.shape</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> value.view(batch_size,encoder_sequence_length, num_heads,embdedding_dim<span class="op">//</span>num_heads)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Value shape = </span><span class="sc">{</span>value<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a><span class="co"># note for the below sampling offset and attention weights we are using the hidden state which have positional embedding information in it.</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets_layer(hidden_states)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets.view(batch_size,num_queries,num_heads,num_levels,n_points,<span class="dv">2</span>) </span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># sampling_offsets are predicted in a normalized, unitless space (not tied to any particular feature map size).</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Each feature map (level) can have a different spatial resolution (height, width).</span></span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a><span class="co"># To convert the offsets into actual positions on each feature map, they must be scaled relative to that map's size.</span></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>offset_normalizer <span class="op">=</span> torch.tensor(spatial_shapes_list)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>offset_normalizer <span class="op">=</span> offset_normalizer[<span class="va">None</span>,<span class="va">None</span>,<span class="va">None</span>,:,<span class="va">None</span>,:]</span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>sampling_offsets <span class="op">=</span> sampling_offsets<span class="op">/</span>offset_normalizer</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Offset Normalizer </span><span class="sc">{</span>offset_normalizer<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights_layer(hidden_states)</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights.view(batch_size,num_queries,num_heads,num_levels<span class="op">*</span>n_points)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a><span class="co"># note here the softmax is along a row of size 16 ,intuitively this means there 4 points from 4 feature levels</span></span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> torch.nn.functional.softmax(attention_weights, <span class="op">-</span><span class="dv">1</span>).view(batch_size,num_queries,num_heads,num_levels,n_points) </span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sampling offset shape = </span><span class="sc">{</span>sampling_offsets<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Attention weights shape = </span><span class="sc">{</span>attention_weights<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we have to modify the refrence points with these sampling points, what this means is that for each of the reference points ,</span></span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a><span class="co"># we need to look into 4 more points across 8 different heads</span></span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a><span class="co"># so initially we had for each query 1 points account each feature dimension making it total 4 and </span></span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a><span class="co"># now when we add this sampling offsets it makes 4 more across 8 differenet heads</span></span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>reference_points_input <span class="op">=</span> reference_points_input[:,:,<span class="va">None</span>,:,<span class="va">None</span>,:]</span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Reference points with unsqueezed dimension for head and levels = </span><span class="sc">{</span>reference_points_input<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>sampling_location <span class="op">=</span> reference_points_input <span class="op">+</span> sampling_offsets</span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final sampling locations = </span><span class="sc">{</span>sampling_location<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Value shape = torch.Size([4, 23890, 8, 32])
Offset Normalizer torch.Size([1, 1, 1, 4, 1, 2])
Sampling offset shape = torch.Size([4, 300, 8, 4, 4, 2])
Attention weights shape = torch.Size([4, 300, 8, 4, 4]) 

Reference points with unsqueezed dimension for head and levels = torch.Size([4, 300, 1, 4, 1, 2])
Final sampling locations = torch.Size([4, 300, 8, 4, 4, 2])</code></pre>
</div>
</div>
</section>
<section id="deformable-attention-1" class="level4">
<h4 class="anchored" data-anchor-id="deformable-attention-1">Deformable Attention</h4>
<div id="86fa0133" class="cell" data-execution_count="44">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the value tensor into per-level chunks based on spatial shapes</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>value_list <span class="op">=</span> value.split([h <span class="op">*</span> w <span class="cf">for</span> h, w <span class="kw">in</span> spatial_shapes_list], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>batch_size, _, num_heads, hidden_dim <span class="op">=</span> value.shape</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of each level's value tensor</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> level, feature <span class="kw">in</span> <span class="bu">enumerate</span>(value_list):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Splitted feature at level </span><span class="sc">{</span>level<span class="sc">}</span><span class="ss"> --&gt; </span><span class="sc">{</span>feature<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert normalized sampling locations from [0, 1] to [-1, 1] for grid_sample</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>sampling_grids <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> sampling_location <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sampling grid shape  = </span><span class="sc">{</span>sampling_grids<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>sampling_value_list <span class="op">=</span> []</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> level_id, (height, width) <span class="kw">in</span> <span class="bu">enumerate</span>(spatial_shapes_list):</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape value tensor for grid sampling:</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (B, H*W, num_heads, C) â†’ (B, num_heads, H*W, C) â†’ (B*num_heads, C, H, W)</span></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>    value_l <span class="op">=</span> (</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>        value_list[level_id]</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>        .flatten(<span class="dv">2</span>)               <span class="co"># (B, H*W, num_heads * C)</span></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>        .transpose(<span class="dv">1</span>, <span class="dv">2</span>)          <span class="co"># (B, num_heads * C, H*W)</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>        .reshape(batch_size <span class="op">*</span> num_heads, hidden_dim, height, width)</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Value at level </span><span class="sc">{</span>level_id<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>value_l<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reshape sampling grid:</span></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (B, num_queries, num_heads, num_levels, num_points, 2)</span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># â†’ (B, num_heads, num_queries, num_points, 2)</span></span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># â†’ (B*num_heads, num_queries, num_points, 2)</span></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>    sampling_grid_l <span class="op">=</span> sampling_grids[:, :, :, level_id].transpose(<span class="dv">1</span>, <span class="dv">2</span>).flatten(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample values using bilinear interpolation</span></span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>    sampling_value_l <span class="op">=</span> nn.functional.grid_sample(</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>        value_l,</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>        sampling_grid_l,</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">"bilinear"</span>,</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>        padding_mode<span class="op">=</span><span class="st">"zeros"</span>,</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>        align_corners<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    sampling_value_list.append(sampling_value_l)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Splitted feature at level 0 --&gt; torch.Size([4, 17956, 8, 32])
Splitted feature at level 1 --&gt; torch.Size([4, 4489, 8, 32])
Splitted feature at level 2 --&gt; torch.Size([4, 1156, 8, 32])
Splitted feature at level 3 --&gt; torch.Size([4, 289, 8, 32])

Sampling grid shape  = torch.Size([4, 300, 8, 4, 4, 2]) 

Value at level 0 torch.Size([32, 32, 134, 134])
Value at level 1 torch.Size([32, 32, 67, 67])
Value at level 2 torch.Size([32, 32, 34, 34])
Value at level 3 torch.Size([32, 32, 17, 17])</code></pre>
</div>
</div>
<div id="537666fc" class="cell" data-execution_count="45">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,f <span class="kw">in</span> <span class="bu">enumerate</span>(sampling_value_list):</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Sampling points from each layer </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>f<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sampling points from each layer 0 torch.Size([32, 32, 300, 4])
Sampling points from each layer 1 torch.Size([32, 32, 300, 4])
Sampling points from each layer 2 torch.Size([32, 32, 300, 4])
Sampling points from each layer 3 torch.Size([32, 32, 300, 4])</code></pre>
</div>
</div>
<div id="bbfa786c" class="cell" data-execution_count="46">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>final_key_matrix <span class="op">=</span> torch.stack(sampling_value_list, dim<span class="op">=-</span><span class="dv">2</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Stacked value matrix shape before flattening = </span><span class="sc">{</span>final_key_matrix<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>final_key_matrix <span class="op">=</span> final_key_matrix.flatten(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Stacked value matrix shape after flattening = </span><span class="sc">{</span>final_key_matrix<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Stacked value matrix shape before flattening = torch.Size([32, 32, 300, 4, 4])
Stacked value matrix shape after flattening = torch.Size([32, 32, 300, 16])</code></pre>
</div>
</div>
<div id="4ef2f515" class="cell" data-execution_count="47">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> attention_weights.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>            batch_size <span class="op">*</span> num_heads, <span class="dv">1</span>, num_queries, num_levels <span class="op">*</span> n_points</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>attention_weights.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>torch.Size([32, 1, 300, 16])</code></pre>
</div>
</div>
<div id="cac02ec2" class="cell" data-execution_count="48">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> final_key_matrix<span class="op">*</span>attention_weights</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Output after attention </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output.<span class="bu">sum</span>(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final output after summation </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output.view(batch_size,num_heads<span class="op">*</span>hidden_dim,num_queries).transpose(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f" Output reshaped --&gt; </span><span class="sc">{</span>output<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> output_projection_layer(output)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output after attention torch.Size([32, 32, 300, 16])
Final output after summation torch.Size([32, 32, 300])
 Output reshaped --&gt; torch.Size([4, 300, 256])</code></pre>
</div>
</div>
<div id="47091739" class="cell" data-execution_count="49">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>torch.Size([4, 300, 256])</code></pre>
</div>
</div>
<div id="7f35de42" class="cell" data-execution_count="50">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> nn.functional.dropout(output,p<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> second_residual <span class="op">+</span> hidden_states</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> layer_norm2(hidden_states)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Fully connected</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>residual <span class="op">=</span> hidden_states</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> nn.ReLU()(fc1(hidden_states))</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> fc2(hidden_states)</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> hidden_states <span class="op">+</span> residual</span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a>hidden_states <span class="op">=</span> layer_norm3(hidden_states)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="dabeac69" class="cell" data-execution_count="51">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>encoder_output <span class="op">=</span> hidden_states.clone()</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>encoder_output.shape</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>torch.Size([4, 300, 256])</code></pre>
</div>
</div>
</section>
</section>
<section id="final-box-and-class-prediction" class="level2">
<h2 class="anchored" data-anchor-id="final-box-and-class-prediction">Final Box and class prediction</h2>
<p>After the decoder produces object query embeddings, two heads operate in parallel:</p>
<ol type="1">
<li><p><strong>Classification Head</strong><br>
A linear layer maps each query embedding to class logits (e.g., 10 classes). The authors uses <strong>Sigmoid + Focal Loss</strong> for multi-label classification</p></li>
<li><p><strong>Box Regression Head</strong><br>
A small feed-forward network predicts four values: offsets for ((x, y, w, h)) in <strong>logit space</strong> (unconstrained).</p>
<ul>
<li>The queryâ€™s <strong>reference point</strong> (normalized in ([0,1])) is converted to logit space using <strong>inverse sigmoid</strong>.</li>
<li>Offsets are added to this reference point for the center coordinates.</li>
<li>Finally, a <strong>sigmoid</strong> maps the result back to normalized coordinates ([0,1]^4), giving the final bounding box.</li>
</ul></li>
</ol>
<p>This design stabilizes training because the network learns relative offsets in an unconstrained space rather than directly regressing normalized coordinates, which accelerates convergence and improves accuracy.</p>
<div id="2a9e8cfa" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is needed because  model predicts an offset in unconstrained space </span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="co"># By applying inverse_sigmoid(reference_points), we map the reference points from [0, 1] to unconstrained space,</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># which is  the same space as the predicted offset.</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Then bring it back to the constrained space , by appling sigmoid, this making learning faster.</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>reference_points_with_inverse_sigmoid <span class="op">=</span> torch.special.logit(reference_points)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a><span class="co"># say we have 10 classes</span></span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>num_class <span class="op">=</span> <span class="dv">10</span> </span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>class_pred <span class="op">=</span> nn.Linear(embdedding_dim,num_class)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>box_head  <span class="op">=</span> nn.Sequential(</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    nn.Linear(embdedding_dim, <span class="dv">512</span>),</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">512</span>, <span class="dv">4</span>),</span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>output_classes <span class="op">=</span> class_pred(encoder_output)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>box_pred <span class="op">=</span> box_head(encoder_output)</span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>box_pred[...,:<span class="dv">2</span>] <span class="op">+=</span> reference_points_with_inverse_sigmoid</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>pred_boxes <span class="op">=</span> box_pred.sigmoid()</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final box head shape </span><span class="sc">{</span>output_classes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final pred boxes head shape </span><span class="sc">{</span>pred_boxes<span class="sc">.</span>shape<span class="sc">}</span><span class="ss"> "</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Final box head shape torch.Size([4, 300, 10])
Final pred boxes head shape torch.Size([4, 300, 4]) </code></pre>
</div>
</div>
<div id="471f3a4d" class="cell" data-execution_count="54">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Losses are similart to DETR, only difference is Deformable detr uses focal loss for  classification and for pred boxes,</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="co"># it uses the same loss like DETR where the losses are a combination of l1 loss and Generalized IOU loss</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="deformable-attention-compute-and-memory-complexity" class="level2">
<h2 class="anchored" data-anchor-id="deformable-attention-compute-and-memory-complexity">Deformable Attention: Compute and Memory Complexity</h2>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ðŸ“º Watch the Explanation in Youtube
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://youtu.be/62XbGHl_DEE">Deformable Decoder</a></p>
</div>
</div>
<p>The paper have a good summary of the complexity of the computation here I will show it bit in detail, specializing to the <strong>encoder</strong> and <strong>decoder</strong> settings, and show how <span class="math inline">\(L\)</span> (levels) and <span class="math inline">\(K\)</span> (points per level) enter the formulas.</p>
<section id="notation" class="level4">
<h4 class="anchored" data-anchor-id="notation">Notation</h4>
<ul>
<li><span class="math inline">\(N_q\)</span>: number of queries<br>
</li>
<li><span class="math inline">\(N_k\)</span>: number of keys<br>
</li>
<li><span class="math inline">\(M\)</span>: number of heads<br>
</li>
<li><span class="math inline">\(C\)</span>: channel dimension<br>
</li>
<li><span class="math inline">\(C_v = C/M\)</span>: per-head dimension<br>
</li>
<li><span class="math inline">\(H \times W\)</span>: spatial size of a single feature map<br>
</li>
<li><span class="math inline">\(H_l \times W_l\)</span>: spatial size at level <span class="math inline">\(l\)</span><br>
</li>
<li><span class="math inline">\(S = \sum_{l=1}^L H_l W_l\)</span>: total token count across levels<br>
</li>
<li><span class="math inline">\(K\)</span>: sampled points per head per level<br>
</li>
<li><span class="math inline">\(L\)</span>: number of feature levels</li>
</ul>
</section>
<section id="preliminaries-what-contributes-to-cost" class="level4">
<h4 class="anchored" data-anchor-id="preliminaries-what-contributes-to-cost">0) Preliminaries: What Contributes to Cost?</h4>
<p>For any attention block, there are four compute buckets:</p>
<ol type="1">
<li><strong>Linear projections</strong> to form <span class="math inline">\(Q, K, V\)</span> (and the output projection): costs scale like <span class="math inline">\(\mathcal{O}(N C^2)\)</span>.</li>
<li><strong>Score computation</strong> (e.g., <span class="math inline">\(QK^\top\)</span> or its sparse substitute): costs scale like <span class="math inline">\(\mathcal{O}(N_q N_k C_v M) = \mathcal{O}(N_q N_k C)\)</span> for dense attention.</li>
<li><strong>Softmax + weighting</strong>: typically <span class="math inline">\(\mathcal{O}(N_q N_k M)\)</span> for softmax, and <span class="math inline">\(\mathcal{O}(N_q N_k C)\)</span> for multiplying by <span class="math inline">\(V\)</span>; the latter usually dominates.</li>
<li><strong>Sampling / Interpolation</strong> <em>(deformable attention only)</em>: adds a term of approximately <span class="math inline">\(\mathcal{O}(N_q \cdot \text{\#samples} \cdot C)\)</span>; Appendix of the paper counts this as a constant <strong>â€œ5â€</strong> times per sample for bilinear interpolation + reduct</li>
</ol>
<p>Memory is dominated by storing the attention weights: <span class="math inline">\(\mathcal{O}(N_q N_k M)\)</span> for dense vs.&nbsp;<span class="math inline">\(\mathcal{O}(N_q M K)\)</span> (single-scale) or <span class="math inline">\(\mathcal{O}(N_q M L K)\)</span> (multi-scale).</p>
<hr>
</section>
<section id="standard-multi-head-attention-expression-is-directly-from-the-paper-eq1" class="level4">
<h4 class="anchored" data-anchor-id="standard-multi-head-attention-expression-is-directly-from-the-paper-eq1">1) Standard Multi-Head Attention (Expression is directly from the paper Eq1)</h4>
<p><span class="math display">\[
\text{MultiHeadAttn}(z_q, x) = \sum_{m=1}^M W_m \sum_{k \in \mathcal{K}} A_{mqk} W'_m x_k
\]</span></p>
<p><strong>Compute:</strong></p>
<ul>
<li>Projections:
<ul>
<li><span class="math inline">\(Q\)</span>: <span class="math inline">\(\mathcal{O}(N_q C^2)\)</span></li>
<li><span class="math inline">\(K, V\)</span>: <span class="math inline">\(\mathcal{O}(N_k C^2)\)</span></li>
</ul></li>
<li>Scores (<span class="math inline">\(QK^\top\)</span>): <span class="math inline">\(\mathcal{O}(M \cdot N_q N_k C_v) = \mathcal{O}(N_q N_k C)\)</span></li>
<li>Softmax: <span class="math inline">\(\mathcal{O}(M \cdot N_q N_k)\)</span></li>
<li>Weighted sum (AV): <span class="math inline">\(\mathcal{O}(M \cdot N_q N_k C_v) = \mathcal{O}(N_q N_k C)\)</span></li>
<li>Output projection: <span class="math inline">\(\mathcal{O}(N_q C^2)\)</span></li>
</ul>
<p><strong>Total (dense attention):</strong></p>
<p><span class="math display">\[
\boxed{\mathcal{O}\big(N_q C^2 + N_k C^2 + N_q N_k C\big)}
\]</span></p>
<p><strong>Memory:</strong></p>
<ul>
<li>Attention weights: <span class="math inline">\(\mathcal{O}(M N_q N_k)\)</span> (dominant)</li>
<li>Key/value caches: <span class="math inline">\(\mathcal{O}(N_k C)\)</span></li>
</ul>
<p><strong>Specializations:</strong></p>
<ul>
<li><p><strong>DETR encoder</strong> (self-attention over pixels): <span class="math inline">\(N_q = N_k = S\)</span></p>
<p><span class="math display">\[
  \mathcal{O}(S^2 C) + \mathcal{O}(S C^2) \quad \text{(dominated by $S^2C$)}
  \]</span></p></li>
<li><p><strong>DETR decoder cross-attention</strong>: <span class="math inline">\(N_q = N\)</span> queries, <span class="math inline">\(N_k = S\)</span> pixels</p>
<p><span class="math display">\[
  \mathcal{O}(N S C) + \mathcal{O}((N+S)C^2)
  \]</span></p></li>
<li><p><strong>DETR decoder self-attention</strong> (queries only):</p>
<p><span class="math display">\[
  \mathcal{O}(2 N C^2 + N^2 C)
  \]</span></p></li>
</ul>
<hr>
</section>
<section id="single-scale-deformable-attention-expression-is-directly-from-the-paper-eq2" class="level4">
<h4 class="anchored" data-anchor-id="single-scale-deformable-attention-expression-is-directly-from-the-paper-eq2">2) Single-Scale Deformable Attention (Expression is directly from the paper Eq2)</h4>
<p><span class="math display">\[
\text{DeformAttn}(z_q, p_q, x) = \sum_{m=1}^M W_m \sum_{k=1}^K A_{mqk} W'_m x(p_q + p_{mqk})
\]</span></p>
<p>Each query attends <span class="math inline">\(K\)</span> sampled points per head around reference <span class="math inline">\(p_q\)</span>. Sampling uses bilinear interpolation.</p>
<ul>
<li>Predict offsets + weights (a single linear with <span class="math inline">\(3MK\)</span> output channels over <span class="math inline">\(z_q\)</span>): <span class="math inline">\(\mathcal{O}(3 N_q C M K)\)</span></li>
<li>Value projection (<span class="math inline">\(W'_m x\)</span>): two possible ways
<ul>
<li>Precompute once on the whole map: <span class="math inline">\(\mathcal{O}(H W C^2)\)</span></li>
<li>Or do per sampled value: <span class="math inline">\(\mathcal{O}(N_q K C^2)\)</span></li>
</ul></li>
<li>Sampling + weighted sum (bilinear + reduce): approx 5 ops per sample per channel: <span class="math inline">\(\mathcal{O}(5 N_q K C)\)</span></li>
<li>Output projection: <span class="math inline">\(\mathcal{O}(N_q C^2)\)</span></li>
</ul>
<p>Putting it together (App. A.1):</p>
<p><span class="math display">\[
\boxed{
\mathcal{O}\Big(N_q C^2 + \min(H W C^2, N_q K C^2) + 5 N_q K C + 3 N_q C M K\Big)
}
\]</span></p>
<p>For typical settings (<span class="math inline">\(M=8\)</span>, <span class="math inline">\(K \leq 4\)</span>, <span class="math inline">\(C=256\)</span>), the paper notes <span class="math inline">\(5K + 3MK \ll C\)</span>, yielding the simplification:</p>
<p><span class="math display">\[
\boxed{\mathcal{O}\big(2 N_q C^2 + \min(H W C^2, N_q K C^2)\big)}
\]</span></p>
<p><strong>Memory:</strong></p>
<ul>
<li>Attention weights: <span class="math inline">\(\mathcal{O}(M N_q K)\)</span></li>
<li>Offsets: <span class="math inline">\(\mathcal{O}(M N_q K \cdot 2)\)</span></li>
<li>No dense <span class="math inline">\((N_q \times N_k)\)</span> matrixâ€”this is the major win.</li>
</ul>
<p><strong>Specializations:</strong></p>
<ul>
<li><strong>Encoder (single-scale, queries are pixels):</strong> <span class="math inline">\(N_q = HW\)</span><br>
With precomputation (<span class="math inline">\(W'_m x\)</span>): complexity becomes <span class="math inline">\(\mathcal{O}(HW C^2)\)</span>, i.e.&nbsp;<strong>linear in spatial size</strong> (vs.&nbsp;quadratic for dense).</li>
<li><strong>Decoder cross-attention (single-scale):</strong> <span class="math inline">\(N_q = N\)</span><br>
With per-query sampled values: <span class="math inline">\(\mathcal{O}(N K C^2)\)</span> (independent of <span class="math inline">\(HW\)</span>).</li>
</ul>
<hr>
</section>
<section id="multi-scale-deformable-attention-expression-is-directly-from-the-paper-eq.-3" class="level4">
<h4 class="anchored" data-anchor-id="multi-scale-deformable-attention-expression-is-directly-from-the-paper-eq.-3">3) Multi-Scale Deformable Attention ( Expression is directly from the paper Eq. (3))</h4>
<p><span class="math display">\[
\text{MSDeformAttn}(z_q, \hat{p}_q, \{x_l\}_{l=1}^L)
= \sum_{m=1}^M W_m \sum_{l=1}^L \sum_{k=1}^K
A_{mlqk} W'_m x_l(\phi_l(\hat{p}_q) + p_{mlqk})
\]</span></p>
<p>Each query samples <span class="math inline">\((L \times K)\)</span> points total.</p>
<p><strong>Compute:</strong></p>
<ul>
<li>Predict offsets + weights: <span class="math inline">\(\mathcal{O}(3 N_q C M L K)\)</span></li>
<li>Value projections (choose one):
<ul>
<li>Precompute on all levels: <span class="math inline">\(\sum_{l=1}^L \mathcal{O}(H_l W_l C^2) = \mathcal{O}(S C^2)\)</span></li>
<li>Or per sampled value: <span class="math inline">\(\mathcal{O}(N_q L K C^2)\)</span></li>
</ul></li>
<li>Sampling + weighted sum: <span class="math inline">\(\mathcal{O}(5 N_q L K C)\)</span></li>
<li>Output projection: <span class="math inline">\(\mathcal{O}(N_q C^2)\)</span></li>
</ul>
<p><strong>Total (multi-scale):</strong></p>
<p><span class="math display">\[
\boxed{
\mathcal{O}\Big(N_q C^2 + \min(S C^2, N_q L K C^2) + 5 N_q L K C + 3 N_q C M L K\Big)
}
\]</span></p>
<p>Under the same â€œsmall <span class="math inline">\((M, K, L)\)</span>â€ assumption as the paper (App. A.1):</p>
<p><span class="math display">\[
\boxed{
\mathcal{O}\big(2 N_q C^2 + \min(S C^2, N_q L K C^2)\big)
}
\]</span></p>
<p><strong>Memory:</strong></p>
<ul>
<li>Attention weights: <span class="math inline">\(\mathcal{O}(M N_q L K)\)</span></li>
<li>Offsets: <span class="math inline">\(\mathcal{O}(M N_q L K \cdot 2)\)</span></li>
<li>Again, no dense <span class="math inline">\((N_q \times S)\)</span> matrix.</li>
</ul>
<p><strong>Specializations:</strong></p>
<ul>
<li><strong>Deformable DETR encoder</strong> (multi-scale, queries are pixels across all levels): <span class="math inline">\(N_q = S\)</span><br>
Precompute values per level <span class="math inline">\(\rightarrow\)</span> <span class="math display">\[
\boxed{\mathcal{O}(S C^2)} \quad \text{(linear in total tokens across scales)}
\]</span> This is the paperâ€™s claim that encoder complexity becomes linear in spatial size (Section 4.1).</li>
<li><strong>Deformable DETR decoder cross-attention:</strong> <span class="math inline">\(N_q = N\)</span> queries<br>
Use per-query samples <span class="math inline">\(\rightarrow\)</span> <span class="math display">\[
\boxed{\mathcal{O}(N L K C^2)} \quad \text{(independent of spatial resolution)}
\]</span></li>
<li><strong>Decoder self-attention:</strong> unchanged from standard: <span class="math inline">\(\mathcal{O}(2 N C^2 + N^2 C)\)</span>.</li>
</ul>
<hr>
</section>
<section id="side-by-side-summary-dominant-terms" class="level4">
<h4 class="anchored" data-anchor-id="side-by-side-summary-dominant-terms">4) Side-by-side Summary (Dominant Terms)</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 29%">
<col style="width: 28%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Block</th>
<th>Dense MHA (DETR)</th>
<th>Deformable (single-scale)</th>
<th>Deformable (multi-scale)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Generic</strong></td>
<td><span class="math inline">\(\mathcal{O}(N_q C^2 + N_k C^2 + N_q N_k C)\)</span></td>
<td><span class="math inline">\(\mathcal{O}(2 N_q C^2 + \min(HW C^2, N_q K C^2))\)</span></td>
<td><span class="math inline">\(\mathcal{O}(2 N_q C^2 + \min(S C^2, N_q L K C^2))\)</span></td>
</tr>
<tr class="even">
<td><strong>Encoder</strong></td>
<td><span class="math inline">\(N_q = N_k = S \Rightarrow \mathcal{O}(S^2 C)\)</span></td>
<td><span class="math inline">\(N_q = HW \Rightarrow \mathcal{O}(HW C^2)\)</span></td>
<td><span class="math inline">\(N_q = S \Rightarrow \boxed{\mathcal{O}(S C^2)}\)</span></td>
</tr>
<tr class="odd">
<td><strong>Decoder cross-attn</strong></td>
<td><span class="math inline">\(N_q = N, N_k = S \Rightarrow \mathcal{O}(N S C)\)</span></td>
<td><span class="math inline">\(\mathcal{O}(N K C^2)\)</span></td>
<td><span class="math inline">\(\boxed{\mathcal{O}(N L K C^2)}\)</span></td>
</tr>
<tr class="even">
<td><strong>Decoder self-attn</strong></td>
<td><span class="math inline">\(\mathcal{O}(2 N C^2 + N^2 C)\)</span></td>
<td>same</td>
<td>same</td>
</tr>
<tr class="odd">
<td><strong>Attention memory</strong></td>
<td><span class="math inline">\(\mathcal{O}(M N_q N_k)\)</span></td>
<td><span class="math inline">\(\mathcal{O}(M N_q K)\)</span></td>
<td><span class="math inline">\(\mathcal{O}(M N_q L K)\)</span></td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="practical-takeaways" class="level4">
<h4 class="anchored" data-anchor-id="practical-takeaways">5) Practical Takeaways</h4>
<ul>
<li><strong>Encoder:</strong> dense self-attention is quadratic in spatial tokens; deformable makes it <strong>linear</strong> in the total number of tokens across scales (<span class="math inline">\(S\)</span>).</li>
<li><strong>Decoder cross-attention:</strong> deformable cost depends on <span class="math inline">\((L K)\)</span> (small, fixed hyperparameters), not on image size, so it scales with the number of queries (<span class="math inline">\(N\)</span>) and channel dimension (<span class="math inline">\(C\)</span>), <strong>not</strong> with <span class="math inline">\(H, W\)</span>.</li>
<li><strong>Memory:</strong> deformable avoids the <span class="math inline">\(\mathcal{O}(N_q N_k)\)</span> attention matrix, replacing it with <span class="math inline">\(\mathcal{O}(N_q L K)\)</span> structuresâ€”crucial for speed and convergence.</li>
</ul>
</section>
</section>
<section id="key-improvements-of-deformable-detr-over-detr" class="level2">
<h2 class="anchored" data-anchor-id="key-improvements-of-deformable-detr-over-detr">Key Improvements of Deformable DETR over DETR</h2>
<ul>
<li>DETR needs <strong>500 epochs</strong> to reach ~42 AP, while Deformable DETR achieves <strong>43.8 AP in just 50 epochs</strong>.</li>
<li>Training time drops drastically: 325 GPU hours vs.&nbsp;2000+ for DETR.</li>
<li>Inference speed: Deformable DETR runs at <strong>19 FPS</strong>, faster than DETR-DC5 (12 FPS).</li>
<li>Deformable DETR converges <strong>10Ã— faster</strong> than DETR-DC5.</li>
</ul>
</section>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>Most of the implementation closely follow the below two, so all credtits to them!!</p>
<ul>
<li><a href="https://github.com/fundamentalvision/Deformable-DETR">Original Detr Paper Implementation</a></li>
<li><a href="https://github.com/huggingface/transformers/tree/main">Hugging Face Transformers</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/akashprakas\.github\.io\/akashBlog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>