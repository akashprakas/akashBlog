<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-11-15">

<title>Lift Splat Shoot Explained – Akash’s Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-6d7e0e055978adcb0dc3bbe98a6d8351.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a14e345711173c227b21482e2eb4cc70.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-c4a17ede6a1e121a6ac7785b1d8267d8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-ef2ad32b6c2cbdb1d3e607ed113afe70.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-c4a17ede6a1e121a6ac7785b1d8267d8.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Akash’s Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/akashprakas"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/akashaapz"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Lift Splat Shoot Explained</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">jupyter</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#problem-with-simple-approach" id="toc-problem-with-simple-approach" class="nav-link active" data-scroll-target="#problem-with-simple-approach"><strong>Problem with simple approach</strong></a></li>
  <li><a href="#how-lift-splat-solves-this" id="toc-how-lift-splat-solves-this" class="nav-link" data-scroll-target="#how-lift-splat-solves-this"><strong>How Lift-Splat solves this</strong></a></li>
  <li><a href="#creating-the-dataset" id="toc-creating-the-dataset" class="nav-link" data-scroll-target="#creating-the-dataset"><strong>Creating the Dataset</strong></a></li>
  <li><a href="#lifting-image-coordinates-with-depth" id="toc-lifting-image-coordinates-with-depth" class="nav-link" data-scroll-target="#lifting-image-coordinates-with-depth">Lifting image coordinates with depth</a>
  <ul class="collapse">
  <li><a href="#the-grid-should-be-alinged-with-feature-map" id="toc-the-grid-should-be-alinged-with-feature-map" class="nav-link" data-scroll-target="#the-grid-should-be-alinged-with-feature-map">The grid should be alinged with feature map</a></li>
  </ul></li>
  <li><a href="#frustum-creation" id="toc-frustum-creation" class="nav-link" data-scroll-target="#frustum-creation">Frustum Creation</a></li>
  <li><a href="#camera-encode" id="toc-camera-encode" class="nav-link" data-scroll-target="#camera-encode">Camera Encode</a></li>
  <li><a href="#lifting-image-features-to-grid" id="toc-lifting-image-features-to-grid" class="nav-link" data-scroll-target="#lifting-image-features-to-grid">Lifting image features to grid</a></li>
  <li><a href="#splat" id="toc-splat" class="nav-link" data-scroll-target="#splat">Splat</a></li>
  <li><a href="#bevencodeshoot" id="toc-bevencodeshoot" class="nav-link" data-scroll-target="#bevencodeshoot">BevEncode/shoot</a></li>
  <li><a href="#loss" id="toc-loss" class="nav-link" data-scroll-target="#loss">Loss</a></li>
  <li><a href="#gradient-flow-insight-through-voxel-pooling" id="toc-gradient-flow-insight-through-voxel-pooling" class="nav-link" data-scroll-target="#gradient-flow-insight-through-voxel-pooling">Gradient Flow Insight through voxel pooling</a>
  <ul class="collapse">
  <li><a href="#how-gradients-flow-through-voxel-pooling" id="toc-how-gradients-flow-through-voxel-pooling" class="nav-link" data-scroll-target="#how-gradients-flow-through-voxel-pooling"><strong>How Gradients Flow Through Voxel Pooling</strong></a></li>
  <li><a href="#how-depth-probabilities-interact" id="toc-how-depth-probabilities-interact" class="nav-link" data-scroll-target="#how-depth-probabilities-interact"><strong>How Depth Probabilities Interact</strong></a></li>
  <li><a href="#multi-camera-fusion-effect" id="toc-multi-camera-fusion-effect" class="nav-link" data-scroll-target="#multi-camera-fusion-effect"><strong>Multi-Camera Fusion Effect</strong></a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><strong>Summary</strong></a></li>
  </ul></li>
  <li><a href="#credits" id="toc-credits" class="nav-link" data-scroll-target="#credits">Credits</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<p>Lift-Splat-Shoot is one of the pioneering models that demonstrated how to effectively use multi-camera inputs—a common setup in autonomous vehicles (for example, the six-camera rig in the nuScenes dataset)—along with their corresponding intrinsic and extrinsic calibration parameters to build an end-to-end differentiable perception system. By carefully designing the architecture to preserve key geometric symmetries, the model enables direct learning from downstream planning feedback, allowing the perception module to improve in a data-driven manner.</p>
<section id="problem-with-simple-approach" class="level3">
<h3 class="anchored" data-anchor-id="problem-with-simple-approach"><strong>Problem with simple approach</strong></h3>
<p>Suppose we were to use a simple approach of single image detector on each camera view and the convert those detections to ego frame using camera extrinsics/intrinsics</p>
<pre><code>Image → Single-image detector → 2D detections → Transform to ego frame → Output</code></pre>
<p>The transformation to ego frame is <strong>post-processing</strong>, not part of the neural network. Therefore, the network <strong>does not know</strong> how its predictions will be fused across cameras and mostly the 2d detector involves some post processing steps that and putting it all together this approach is not end to end differentiable.</p>
</section>
<section id="how-lift-splat-solves-this" class="level3">
<h3 class="anchored" data-anchor-id="how-lift-splat-solves-this"><strong>How Lift-Splat solves this</strong></h3>
<ul>
<li>Lift-Splat makes the entire process differentiable enabling end-to-end learning.
<ul>
<li>It <strong>lifts</strong> image features into 3D frustums.</li>
<li><strong>Splats</strong> them into a bird’s-eye-view grid.</li>
<li>Runs a BEV CNN for final predictions for say vehicle segmentation in BEV frame.</li>
<li>This is called a <strong>forward projection</strong> method because you go <strong>from image → 3D → BEV</strong> by pushing features outward along rays. And this is geometry driven. (Ofcourse there is backward projection and forward-backward projection methods as well, which might be covered in another blog)</li>
</ul></li>
</ul>
<p>All of the steps will be explained in detail with the corresponding code below and the notebook is end to end executable.</p>
<div id="8d5b352c" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> T</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn, torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">:  You can comment out the below if you dont have nuscenes data available locally and can directly load the preprocessed data later as i have cached it in this repo</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyquaternion <span class="im">import</span> Quaternion</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nuscenes.nuscenes <span class="im">import</span> NuScenes</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nuscenes.utils.data_classes <span class="im">import</span> Box</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nuscenes.nuscenes <span class="im">import</span> NuScenes</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="96990de2" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>]</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>std  <span class="op">=</span> [<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>denormalize_img <span class="op">=</span> T.Compose([</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    T.Normalize(</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        mean<span class="op">=</span>[<span class="op">-</span>m<span class="op">/</span>s <span class="cf">for</span> m, s <span class="kw">in</span> <span class="bu">zip</span>(mean, std)],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        std<span class="op">=</span>[<span class="dv">1</span><span class="op">/</span>s <span class="cf">for</span> s <span class="kw">in</span> std],</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    T.Lambda(<span class="kw">lambda</span> x: x.clamp(<span class="dv">0</span>, <span class="dv">1</span>)),  </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    T.ToPILImage(),</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>normalize_img <span class="op">=</span> T.Compose([</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    T.ToTensor(),                      </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    T.Normalize(mean<span class="op">=</span>mean, std<span class="op">=</span>std),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>cams <span class="op">=</span> [<span class="st">'CAM_FRONT_LEFT'</span>,<span class="st">'CAM_FRONT'</span>, <span class="st">'CAM_FRONT_RIGHT'</span>, <span class="st">'CAM_BACK_LEFT'</span>, <span class="st">'CAM_BACK'</span>, <span class="st">'CAM_BACK_RIGHT'</span>]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the image size the input CNN expects , so basically we will resize the images to this size</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>input_image_height <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>input_image_width <span class="op">=</span> <span class="dv">352</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>nx <span class="op">=</span> np.array([<span class="dv">200</span>, <span class="dv">200</span>,<span class="dv">1</span>]) <span class="co"># number of cells in x,y,z in the bev grid, note that z is 1 since we are doing a 2D bev</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>bx <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">49.75</span>, <span class="op">-</span><span class="fl">49.75</span>, <span class="fl">0.0</span>]) <span class="co"># bottom right corner x,y,z of the bev grid</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">20.0</span>]) <span class="co"># x,y,z resolution of the bev grid</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="creating-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-dataset"><strong>Creating the Dataset</strong></h2>
<p>To create the dataset, I am using the <strong>nuScenes</strong> dataset. Each input sample consists of:</p>
<ul>
<li><strong>6 camera images</strong></li>
<li><strong>Camera intrinsics and extrinsics matrices</strong></li>
<li><strong>Augmentation transforms</strong> (This is not directly from the nuscenes dataset)</li>
</ul>
<p>A crucial detail when applying augmentations such as resizing , cropping or rotation is that the camera intrinsics and extrinsics are defined for the original image dimensions. Therefore, any transformations applied to the images must be cached so that their inverse transforms can be used later during projection to the ego vehicle coordinate frame. This ensures that the image features are correctly aligned with the camera parameters (K, R, and T) and that the spatial consistency of the pipeline is preserved.</p>
<div id="3a17f471" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: If you dont have the nuscenes dataset, you can skip running this code block</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and load the preprocessed data saved as 'nuscenes_mini_sample_data_batch.pt' which is the repo and I load it in later code blocks</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>nusc <span class="op">=</span> NuScenes(version<span class="op">=</span><span class="st">'v1.0-mini'</span>, dataroot<span class="op">=</span><span class="st">'/home/ap/dataset/nuscenes'</span>, verbose<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>last_sample_token <span class="op">=</span> <span class="st">'63c24b51feb94f14bec29022dae4975d'</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>second_last_sample_token  <span class="op">=</span> <span class="st">'de9432d4fc7a4e5a985e2bc628eb614c'</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>last_sample <span class="op">=</span> nusc.get(<span class="st">'sample'</span>, last_sample_token)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>second_last_sample <span class="op">=</span> nusc.get(<span class="st">'sample'</span>, second_last_sample_token)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> [second_last_sample, last_sample]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>images_per_batch <span class="op">=</span> []</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>rotations_per_batch <span class="op">=</span> []</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>translations_per_batch <span class="op">=</span> []</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>intrinsics_per_batch <span class="op">=</span> []</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>resize_transforms_per_batch <span class="op">=</span> []</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>crop_transforms_per_batch <span class="op">=</span> []</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>gt_binary_images_per_batch <span class="op">=</span> []</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> my_sample <span class="kw">in</span> samples:</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> []</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    rotations <span class="op">=</span> []</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    translations <span class="op">=</span> []</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    intrinsics <span class="op">=</span> []</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    resize_transforms <span class="op">=</span> []</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    crop_transforms <span class="op">=</span> []</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> cam <span class="kw">in</span> cams:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        cam_data <span class="op">=</span> nusc.get(<span class="st">'sample_data'</span>, my_sample[<span class="st">'data'</span>][cam])</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        image_name <span class="op">=</span> os.path.join(nusc.dataroot, cam_data[<span class="st">'filename'</span>])</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        img  <span class="op">=</span> Image.<span class="bu">open</span>(image_name)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Image from {cam}: size {img.size}")</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        calibrated_sensor_token <span class="op">=</span> cam_data[<span class="st">'calibrated_sensor_token'</span>]</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        calibrated_sensor <span class="op">=</span> nusc.get(<span class="st">'calibrated_sensor'</span>, calibrated_sensor_token)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        rotation <span class="op">=</span> torch.tensor(Quaternion(calibrated_sensor[<span class="st">'rotation'</span>]).rotation_matrix)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        translation <span class="op">=</span> torch.tensor(calibrated_sensor[<span class="st">'translation'</span>])</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        camera_intrinsic <span class="op">=</span> torch.tensor(calibrated_sensor[<span class="st">'camera_intrinsic'</span>])</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        H,W <span class="op">=</span> img.size[<span class="dv">1</span>], img.size[<span class="dv">0</span>]</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Rotation:\n{rotation}\nTranslation:\n{translation}\nIntrinsics:\n{camera_intrinsic}\nImage Height: {H}, Width: {W}\n")</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print("=============================================================================================================================================")</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        resize <span class="op">=</span> <span class="bu">max</span>(input_image_height <span class="op">/</span> H, input_image_width <span class="op">/</span> W) <span class="co">#this is to maintain aspect ratio</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        resize_dims <span class="op">=</span> (<span class="bu">int</span>(W <span class="op">*</span> resize), <span class="bu">int</span>(H <span class="op">*</span> resize))</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Resize dimensions to fit feature map: {resize_dims}\n\n")</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>        newW, newH <span class="op">=</span> resize_dims</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># cropping from the top since the top usually has more irrelevant info (sky) </span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in the orginal implementation, cropping is done from bottom + top randomly with more weight on top</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        crop_h <span class="op">=</span> newH <span class="op">-</span> input_image_height</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        crop_w <span class="op">=</span> newW <span class="op">-</span> input_image_width</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        crop <span class="op">=</span> (crop_w, crop_h, crop_w <span class="op">+</span> input_image_width, crop_h <span class="op">+</span> input_image_height)</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Crop box to get feature map size: {crop}\n")</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> img.resize(resize_dims).crop(crop)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">#post homography transformations,</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># we can consider resize and crop as two homography transformations and need to make matrices for them, because we want to undo later during projection because the intrinscs</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and extrinsics are for original image size and this needs to be taken care</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ideally these two should be two dimensional but now making it 3 work numerically well so</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for resize transform for z we multiply by 1, which is a no change and for crop_transform for z we subtract 0 which is also a no change. </span></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        resize_transform <span class="op">=</span> torch.diag(torch.tensor([resize,resize, <span class="fl">1.0</span>])) </span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        crop_transform   <span class="op">=</span> <span class="op">-</span>torch.tensor([[crop_w,crop_h,<span class="fl">0.0</span>]])</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Resize transform:\n{resize_transform}")</span></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># print(f"Crop transform:\n{crop_transform}")</span></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>        images.append(normalize_img(img))</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>        rotations.append(rotation)</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>        translations.append(translation)</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>        intrinsics.append(camera_intrinsic)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>        resize_transforms.append(resize_transform)</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>        crop_transforms.append(crop_transform)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>    images_per_batch.append(torch.stack(images))</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    rotations_per_batch.append(torch.stack(rotations))  </span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>    translations_per_batch.append(torch.stack(translations))</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    intrinsics_per_batch.append(torch.stack(intrinsics))</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    resize_transforms_per_batch.append(torch.stack(resize_transforms))</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    crop_transforms_per_batch.append(torch.stack(crop_transforms))</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    sd_lidar <span class="op">=</span> nusc.get(<span class="st">'sample_data'</span>, my_sample[<span class="st">'data'</span>][<span class="st">'LIDAR_TOP'</span>])</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get ego pose to transform from global to ego vehicle frame</span></span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    egopose <span class="op">=</span> nusc.get(<span class="st">'ego_pose'</span>, sd_lidar[<span class="st">'ego_pose_token'</span>])</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    trans <span class="op">=</span> <span class="op">-</span>np.array(egopose[<span class="st">'translation'</span>])</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    rot <span class="op">=</span> Quaternion(egopose[<span class="st">'rotation'</span>]).inverse</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    target_binary_image <span class="op">=</span> np.zeros((nx[<span class="dv">0</span>], nx[<span class="dv">1</span>]))</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tok <span class="kw">in</span> my_sample[<span class="st">'anns'</span>]:</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>        inst <span class="op">=</span> nusc.get(<span class="st">'sample_annotation'</span>, tok)</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># add category for lyft</span></span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> inst[<span class="st">'category_name'</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>] <span class="op">==</span> <span class="st">'vehicle'</span>:</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>        box <span class="op">=</span> Box(inst[<span class="st">'translation'</span>], inst[<span class="st">'size'</span>], Quaternion(inst[<span class="st">'rotation'</span>]))</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># bring box to ego frame</span></span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        box.translate(trans)</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>        box.rotate(rot)</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>        pts <span class="op">=</span> box.bottom_corners()[:<span class="dv">2</span>].T</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># bring points to grid coords</span></span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>        pts <span class="op">=</span> np.<span class="bu">round</span>(</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>            (pts <span class="op">-</span> bx[:<span class="dv">2</span>] <span class="op">+</span> dx[:<span class="dv">2</span>]<span class="op">/</span><span class="fl">2.</span>) <span class="op">/</span> dx[:<span class="dv">2</span>]</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>            ).astype(np.int32)</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>        pts[:, [<span class="dv">1</span>, <span class="dv">0</span>]] <span class="op">=</span> pts[:, [<span class="dv">0</span>, <span class="dv">1</span>]]</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>        cv2.fillPoly(target_binary_image, [pts], <span class="fl">1.0</span>)</span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>    gt_binary_images_per_batch.append(torch.tensor(target_binary_image))</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a><span class="co">#Saving the data for  the data for tutorial usage</span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {<span class="st">"images"</span>:torch.stack(images_per_batch),  <span class="co"># (B, num_cams, 3, H, W)</span></span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>        <span class="st">"rotations"</span>:torch.stack(rotations_per_batch),  <span class="co"># (B, num_cams, 3, 3)</span></span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>        <span class="st">"translations"</span>:torch.stack(translations_per_batch),  <span class="co"># (B, num_cams, 3)</span></span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>        <span class="st">"intrinsics"</span>:torch.stack(intrinsics_per_batch),  <span class="co"># (B, num_cams, 3, 3),</span></span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>        <span class="st">"resize_transforms"</span>:torch.stack(resize_transforms_per_batch),  <span class="co"># (B, num_cams, 3, 3)</span></span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>        <span class="st">"crop_transforms"</span>:torch.stack(crop_transforms_per_batch),  <span class="co"># (B, num_cams, 1, 3)</span></span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>        <span class="st">"gt_binary_images"</span>:torch.stack(gt_binary_images_per_batch)  <span class="co"># (B, H, W)</span></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a><span class="co">#torch.save(data,"nuscenes_mini_sample_data_batch.pt")</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>======
Loading NuScenes tables for version v1.0-mini...
23 category,
8 attribute,
4 visibility,
911 instance,
12 sensor,
120 calibrated_sensor,
31206 ego_pose,
8 log,
10 scene,
404 sample,
31206 sample_data,
18538 sample_annotation,
4 map,
Done loading in 0.330 seconds.
======
Reverse indexing ...
Done reverse indexing in 0.1 seconds.
======</code></pre>
</div>
</div>
<div id="d3ad558c" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co"> If you dont have nuscenes dataset, you can directly load the preprocessed data saved as 'nuscenes_mini_sample_data_batch.pt' which is in the repo</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> torch.load(<span class="st">"nuscenes_mini_sample_data_batch.pt"</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>images, rotations, translations, intrinsics, resize_transforms, crop_transforms, target_binary_image <span class="op">=</span> (</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'images'</span>],</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'rotations'</span>],</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'translations'</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'intrinsics'</span>],</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'resize_transforms'</span>],</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'crop_transforms'</span>],</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    data[<span class="st">'gt_binary_images'</span>]</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="e93809b9" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_all_cams(images_batch, cams):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    imgs <span class="op">=</span> images_batch </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(cams)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    rows, cols <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>, <span class="dv">9</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        plt.subplot(rows, cols, i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        img <span class="op">=</span> denormalize_img(imgs[i])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        plt.imshow(img)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        plt.title(cams[i], fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">'off'</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.suptitle(<span class="st">'All camera images resized and cropped'</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>plot_all_cams(images[<span class="dv">1</span>], cams)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_bev(target_binary_image, nx, bx, dx):</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    bev <span class="op">=</span> target_binary_image.detach().cpu().numpy() <span class="cf">if</span> torch.is_tensor(target_binary_image) <span class="cf">else</span> target_binary_image</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    bev <span class="op">=</span> bev.T </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    x_min, y_min <span class="op">=</span> bx[<span class="dv">0</span>], bx[<span class="dv">1</span>]</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    x_max <span class="op">=</span> x_min <span class="op">+</span> nx[<span class="dv">0</span>] <span class="op">*</span> dx[<span class="dv">0</span>]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    y_max <span class="op">=</span> y_min <span class="op">+</span> nx[<span class="dv">1</span>] <span class="op">*</span> dx[<span class="dv">1</span>]</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    plt.imshow(bev, cmap<span class="op">=</span><span class="st">'gray'</span>, origin<span class="op">=</span><span class="st">'lower'</span>, extent<span class="op">=</span>[x_min, x_max, y_min, y_max], interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'X (m) — forward'</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Y (m) — left'</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'BEV target '</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    plt.grid(color<span class="op">=</span><span class="st">'w'</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, linestyle<span class="op">=</span><span class="st">':'</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    plt.colorbar(label<span class="op">=</span><span class="st">'occupancy'</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ego at (0,0)</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    plt.scatter([<span class="dv">0</span>], [<span class="dv">0</span>], c<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">30</span>, label<span class="op">=</span><span class="st">'Ego'</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>plot_bev(target_binary_image[<span class="dv">0</span>],nx,bx,dx)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2025-11-15-LiftSplatShoot_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2025-11-15-LiftSplatShoot_files/figure-html/cell-6-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="lifting-image-coordinates-with-depth" class="level2">
<h2 class="anchored" data-anchor-id="lifting-image-coordinates-with-depth">Lifting image coordinates with depth</h2>
<p><strong>Lifting</strong> refers to the process of converting 2D image cordinates into 3D space by associating each pixel (or feature map location) with a set of depth hypotheses. This is a pure geometric step.</p>
<ul>
<li>Each pixel (or feature map cell) has coordinates ((u, v)) in image space.</li>
<li>Since we don’t know the true depth (d) of each pixel, we hypothesize multiple depths:<br>
</li>
<li>For each ((u, v)), we create a set of 3D points ((u, v, d)) for different values of (d).</li>
</ul>
<p>This gives us a <strong>3D volume</strong> of 3D points extending from the camera into the scene.</p>
<section id="the-grid-should-be-alinged-with-feature-map" class="level3">
<h3 class="anchored" data-anchor-id="the-grid-should-be-alinged-with-feature-map">The grid should be alinged with feature map</h3>
<p>The feature map from the backbone (EfficientNet used below) has a lower resolution than the input image (here stride=16). So instead of lifting every pixel in the original image, we lift <strong>each cell in the feature map</strong>, which corresponds to a patch in the original image.</p>
<ul>
<li>For example, if the input image is (128 x 352), and the stride is 16, the feature map is (8 x 22).</li>
<li>For each of these 8×22 positions, we hypothesize (D) depth values, and network will predict say 64 channel features</li>
<li>So grid cell cordinates should be divided such that they have the similar shape as feature map, for example here the lifted grid has shape ((D, H, W,3)) = ((41, 8, 22,3)) and the lifted feature map will have shape (41,8,22,64)</li>
<li>Each feature map cell has its corresponding 3D grid points</li>
</ul>
<p>This alignment is crucial because:</p>
<ul>
<li>You want to <strong>associate the feature vector</strong> at each ((h, w)) with the corresponding 3D points along the ray.</li>
<li>Later, when we <strong>splat</strong> these 3D points into the BEV grid, we use the depth distribution (learned by the network) to weigh how much each depth contributes to the final BEV cell.</li>
</ul>
<div id="5688f417" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>stride <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># feature map size after feature extraction from the input image</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>feature_map_height <span class="op">=</span> input_image_height<span class="op">//</span>stride</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>feature_map_width <span class="op">=</span> input_image_width<span class="op">//</span>stride  <span class="co">#feature map size, (8, 22)</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>depth_bin <span class="op">=</span> [<span class="fl">4.0</span>,<span class="fl">45.0</span>,<span class="fl">1.0</span>] <span class="co"># start,stop,step </span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> torch.arange(<span class="op">*</span>depth_bin,dtype<span class="op">=</span>torch.<span class="bu">float</span>).view(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>).expand(<span class="op">-</span><span class="dv">1</span>,feature_map_height,feature_map_width)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>D,_,_ <span class="op">=</span> ds.shape</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> torch.linspace(<span class="dv">0</span>, input_image_width <span class="op">-</span> <span class="dv">1</span>, feature_map_width, dtype<span class="op">=</span>torch.<span class="bu">float</span>).view(<span class="dv">1</span>, <span class="dv">1</span>, feature_map_width).expand(D, feature_map_height, feature_map_width)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> torch.linspace(<span class="dv">0</span>, input_image_height <span class="op">-</span> <span class="dv">1</span>, feature_map_height, dtype<span class="op">=</span>torch.<span class="bu">float</span>).view(<span class="dv">1</span>, feature_map_height, <span class="dv">1</span>).expand(D, feature_map_height, feature_map_width)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates <span class="op">=</span> torch.stack((xs,ys,ds),<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lifted coordinates shape is </span><span class="sc">{</span>lifted_image_coordinates<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Lifted coordinates shape is torch.Size([41, 8, 22, 3])</code></pre>
</div>
</div>
</section>
</section>
<section id="frustum-creation" class="level2">
<h2 class="anchored" data-anchor-id="frustum-creation">Frustum Creation</h2>
<p><img src="images/Frustum.jpg" class="img-fluid" width="600"></p>
<p>Now we have to project the lifted image points into the <strong>ego frame</strong> which forms a frustum . Initially, the lifted image coordinates are in <strong>post-augmentation space</strong> (after resize and crop), but the camera intrinsics are defined in the <strong>original image space</strong>. To ensure geometric consistency, we first <strong>undo the augmentations</strong> by applying the inverse of the crop translation and resize transform. This maps the points back to the original image dimensions.</p>
<p>Next, we compute <strong>unnormalized pixel coordinates scaled by depth</strong> ((u , v , Z)), which is necessary for converting 2D image points into 3D camera-frame coordinates using the pinhole camera model. We then multiply these coordinates by the <strong>inverse of the camera intrinsics</strong> to obtain 3D points in the camera frame.</p>
<p>Finally, we transform these camera-frame points into the <strong>ego frame</strong> using the camera-to-ego rotation and translation matrices.</p>
<section id="augmentation-inverse-mapping" class="level4">
<h4 class="anchored" data-anchor-id="augmentation-inverse-mapping">1. Augmentation Inverse Mapping</h4>
<p><span class="math display">\[
X = R^{-1} (X' - T)
\]</span></p>
</section>
<section id="unnormalize-pixel-coordinates" class="level4">
<h4 class="anchored" data-anchor-id="unnormalize-pixel-coordinates">2. Unnormalize Pixel Coordinates</h4>
<p>Given normalized pixel coordinates <span class="math inline">\((u, v)\)</span> and depth <span class="math inline">\(Z_c\)</span> from pinhole camera principle</p>
<p><span class="math display">\[
\begin{bmatrix}
u' \\\\
v'
\end{bmatrix}
=
\begin{bmatrix}
u \cdot Z_c \\\\
v \cdot Z_c
\end{bmatrix}
\]</span></p>
</section>
<section id="camera-intrinsics-inverse" class="level4">
<h4 class="anchored" data-anchor-id="camera-intrinsics-inverse">3. Camera Intrinsics Inverse</h4>
<p>To get 3D camera coordinates from image coordinates and depth:</p>
<p><span class="math display">\[
\begin{bmatrix}
X_c \\\\
Y_c \\\\
Z_c
\end{bmatrix}
=
K^{-1}
\begin{bmatrix}
u' \\\\
v' \\\\
Z_c
\end{bmatrix}
\]</span></p>
<p>Where <span class="math inline">\(K\)</span> is the camera intrinsics matrix.</p>
</section>
<section id="camera-to-ego-transformation" class="level4">
<h4 class="anchored" data-anchor-id="camera-to-ego-transformation">4. Camera-to-Ego Transformation</h4>
<p>To transform from camera frame to ego (vehicle) frame:</p>
<p><span class="math display">\[
\begin{bmatrix}
X_{ego} \\\\
Y_{ego} \\\\
Z_{ego}
\end{bmatrix}
=
R_{c2e}
\begin{bmatrix}
X_c \\\\
Y_c \\\\
Z_c
\end{bmatrix}
+
T_{c2e}
\]</span></p>
</section>
<section id="combined-transformation" class="level4">
<h4 class="anchored" data-anchor-id="combined-transformation">5. Combined Transformation</h4>
<p><span class="math display">\[
\text{Combined} = R_{c2e} \cdot K^{-1}
\]</span></p>
<p>So the full transformation is:</p>
<p><span class="math display">\[
\begin{bmatrix}
X_{ego} \\\\
Y_{ego} \\\\
Z_{ego}
\end{bmatrix}
=
\text{Combined}
\begin{bmatrix}
u' \\\\
v' \\\\
Z_c
\end{bmatrix}
+
T_{c2e}
\]</span></p>
<div id="3b9974cb" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># so the lifted image coordinates are in post augmented space, but the intrinsics are pre-augmented</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># so we should map the points back, in the example i used resize and crop, so we should apply inverse transforms here</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># before converting them to 3D points in ego frame</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># so you can think of the transformation as </span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># X_cropped_and_resized = Resize_transform*X + T # where R is the resize scale matrix and T is the crop translation</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># X = Resize_transform @ (X_cropped_and_resized - T)</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>batch_size,num_cameras_per_batch, _,_ <span class="op">=</span> crop_transforms.shape</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a> <span class="co"># the last dimension is for x,y,depth and first two are batch and camera index and the D,H,W need to be broadcasted</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates_in_orginal_image_dimension <span class="op">=</span> lifted_image_coordinates <span class="op">-</span> crop_transforms.view(batch_size,num_cameras_per_batch,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates_in_orginal_image_dimension <span class="op">=</span>torch.inverse(resize_transforms).view(batch_size,num_cameras_per_batch,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>).matmul(lifted_image_coordinates_in_orginal_image_dimension.unsqueeze(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates_in_orginal_image_dimension <span class="op">=</span> lifted_image_coordinates_in_orginal_image_dimension.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_xy <span class="op">=</span> lifted_image_coordinates_in_orginal_image_dimension[...,:<span class="dv">2</span>]<span class="op">*</span>lifted_image_coordinates_in_orginal_image_dimension[...,<span class="dv">2</span>:<span class="dv">3</span>]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_x_y_z <span class="op">=</span> torch.cat((lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_xy, lifted_image_coordinates_in_orginal_image_dimension[...,<span class="dv">2</span>:<span class="dv">3</span>]),<span class="op">-</span><span class="dv">1</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="co"># first we have to multiply with inverse intrinsics to get the 3D points in camera frame</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="co"># [X_c, Y_c, Z_c] = inv(K) * [u*Z_c, v*Z_c, Z_c]</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Then we can transform to ego frame using rotation and translation</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co"># [X_ego, Y_ego, Z_ego] = R_c2e * [X_c, Y_c, Z_c] + T_c2e</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># we can combine these two steps into one matrix multiplication and one addition</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>combined_k_inverse_and_rotation <span class="op">=</span> rotations.to(dtype<span class="op">=</span>intrinsics.dtype).matmul(torch.inverse(intrinsics))</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>frustum_3d_points_in_ego_frame <span class="op">=</span> combined_k_inverse_and_rotation.view(batch_size,num_cameras_per_batch,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">3</span>).matmul(lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_x_y_z).squeeze(<span class="op">-</span><span class="dv">1</span>) <span class="op">+</span> translations.view(batch_size,num_cameras_per_batch,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Frustum 3D points in ego frame shape: </span><span class="sc">{</span>frustum_3d_points_in_ego_frame<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Frustum 3D points in ego frame shape: torch.Size([2, 6, 41, 8, 22, 3])</code></pre>
</div>
</div>
</section>
</section>
<section id="camera-encode" class="level2">
<h2 class="anchored" data-anchor-id="camera-encode">Camera Encode</h2>
<p><img src="images/Eff2.jpg" class="img-fluid" width="600"></p>
<p>Till now what was done was purely geometric , now we have to process the image and in the paper they use <strong>EfficientNet-B0</strong> as the backbone. The last two feature maps are extracted: - <code>x16</code> (stride /16, mid-level features) - <code>x32</code> (stride /32, deep semantic features)</p>
<p>The deeper <code>/32</code> map is <strong>upsampled</strong> to match <code>/16</code> resolution, then both are <strong>concatenated</strong> along channels.<br>
Finally, a <strong>fusion block</strong> (two 3×3 conv layers with BatchNorm + ReLU) refines the combined features into a unified representation <code>[B, 512, H/16, W/16]</code> for the Lift step.</p>
<div id="3f4a162b" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: The architecture here is directly from the implemenation of LIFT,</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">#       I just used timm for simplicity and added some comments.</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Build a feature-extractor backbone </span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">#    out_indices=(3,4) for EfficientNet-B0 correspond to:</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">#      - idx 3: stride ~ /16, channels = 112</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">#      - idx 4: stride ~ /32, channels = 320</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>batch,num_cameras,channel,h,w <span class="op">=</span> images.shape</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> images.view(batch<span class="op">*</span>num_cameras,channel,h,w)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>backbone <span class="op">=</span> timm.create_model(</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">'efficientnet_b0'</span>,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="va">False</span>,            </span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>    features_only<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    out_indices<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">4</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Forward through the backbone to get two scales:</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co">#    x16: [B, 112, H/16, W/16]</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co">#    x32: [B, 320, H/32, W/32]</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>feats <span class="op">=</span> backbone(x)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>x16, x32 <span class="op">=</span> feats[<span class="dv">0</span>], feats[<span class="dv">1</span>]  </span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x16 (idx=3) shape: </span><span class="sc">{</span>x16<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  </span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x32 (idx=4) shape: </span><span class="sc">{</span>x32<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)   </span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 3) Upsample the deeper /32 map to match /16 spatial size.</span></span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>upsample_to_x16 <span class="op">=</span> nn.Upsample(size<span class="op">=</span>x16.shape[<span class="op">-</span><span class="dv">2</span>:], mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>x32_up <span class="op">=</span> upsample_to_x16(x32)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"x32 upsampled to x16 size: </span><span class="sc">{</span>x32_up<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co">#  [B, 320, H/16, W/16]</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Concatenate along channels (skip fusion like U-Net).</span></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="co">#    in_channels = 112 + 320 = 432 for EfficientNet-B0 </span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>x_cat <span class="op">=</span> torch.cat([x16, x32_up], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"concatenated shape: </span><span class="sc">{</span>x_cat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># ~ [B, 432, H/16, W/16]</span></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>in_channels <span class="op">=</span> x_cat.shape[<span class="dv">1</span>]  </span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>out_channels <span class="op">=</span> <span class="dv">512</span>             </span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) Fuse + refine: two 3x3 convs with BN + ReLU.</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="co">#    - First 3x3 mixes skip &amp; deep features and compresses to out_channels</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a><span class="co">#    - Second 3x3 refines after nonlinearity (effective RF ~5x5 total)</span></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="co">#    - bias=False because BatchNorm has its own affine params</span></span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>fuse_and_refine <span class="op">=</span> nn.Sequential(</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(in_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>    nn.BatchNorm2d(out_channels),</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    nn.BatchNorm2d(out_channels),</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>x_feat <span class="op">=</span> fuse_and_refine(x_cat)</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fused features shape: </span><span class="sc">{</span>x_feat<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># ~ [B, 512, H/16, W/16]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>x16 (idx=3) shape: torch.Size([12, 112, 8, 22])
x32 (idx=4) shape: torch.Size([12, 320, 4, 11])
x32 upsampled to x16 size: torch.Size([12, 320, 8, 22])
concatenated shape: torch.Size([12, 432, 8, 22])
fused features shape: torch.Size([12, 512, 8, 22])</code></pre>
</div>
</div>
</section>
<section id="lifting-image-features-to-grid" class="level2">
<h2 class="anchored" data-anchor-id="lifting-image-features-to-grid">Lifting image features to grid</h2>
<p>Now we have extracted the feature from the effiecient backbone , now we have a <strong>depth head</strong> and <strong>context feature head</strong> , the idea is to have a <strong>softattention over depth</strong> and distribute image feature across multiple depth hypotheses and then the network learn how much weight to assign to each depth bin per feature from the loss. * For each pixel, the network predicts a <strong>categorical distribution over depth bins</strong> (via <code>softmax</code>). * Instead of choosing a single depth (hard assignment), the pixel’s feature vector is <strong>weighted by these probabilities</strong> and spread across all depth bins. * This means the network attends to multiple possible depths proportionally to their predicted likelihood. * Thus a volume of C*D , will be created for each feature</p>
<p><code>Benefits</code></p>
<ul>
<li>Depth from a single image is ambiguous. Hard assignment (one depth per pixel) would propagate errors.</li>
<li>Soft attention allows the network to set plausible depths, letting downstream BEV fusion resolve ambiguity using multi-camera context or final target loss.</li>
<li>It’s differentiable, so the entire pipeline can learn end-to-end.</li>
</ul>
<div id="d3893faf" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>number_of_depth_bins <span class="op">=</span> D</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>number_of_conv_features <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Two independent heads (1x1) for depth and feature prediction from the fused features</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>depth_head <span class="op">=</span> nn.Conv2d(out_channels, number_of_depth_bins, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>feature_head <span class="op">=</span> nn.Conv2d(out_channels, number_of_conv_features, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>depth_logits <span class="op">=</span> depth_head(x_feat)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"depth logits shape: </span><span class="sc">{</span>depth_logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># ~ [B, D, H/16, W/16]</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>feature_maps <span class="op">=</span> feature_head(x_feat)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"feature maps shape: </span><span class="sc">{</span>feature_maps<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># ~ [B, 64, H/16, W/16] </span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>depth_prob <span class="op">=</span> nn.functional.softmax(depth_logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>lifted <span class="op">=</span> torch.einsum(<span class="st">'bdhw,bchw-&gt;bcdhw'</span>, depth_prob, feature_maps)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"lifted features shape: </span><span class="sc">{</span>lifted<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># ~ [B, 64, D, H/16, W/16]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>depth logits shape: torch.Size([12, 41, 8, 22])
feature maps shape: torch.Size([12, 64, 8, 22])
lifted features shape: torch.Size([12, 64, 41, 8, 22])</code></pre>
</div>
</div>
<div id="7f59438d" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce per-pixel depth distribution and context features,</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># then "lift" them into a per-depth frustum feature volume (C × D × H × W),</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># as in Lift-Splat-Shoot’s lift step.</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">#   - number_of_depth_bins (D): # of discrete depth planes along each pixel ray</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co">#   - number_of_conv_features (C): channel size of the per-pixel context vector</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>number_of_depth_bins <span class="op">=</span> D</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>number_of_conv_features <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 1×1 heads:</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co">#  - depth_head: per-pixel logits over D depth bins </span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a><span class="co">#  - feature_head: per-pixel context embedding (C channels) that will be later</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a><span class="co">#                  broadcast across depth via an outer product with depth probs</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>depth_head   <span class="op">=</span> nn.Conv2d(out_channels, number_of_depth_bins, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>feature_head <span class="op">=</span> nn.Conv2d(out_channels, number_of_conv_features, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict raw depth logits and per-pixel context features</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>depth_logits  <span class="op">=</span> depth_head(x_feat)        <span class="co"># [B, D, H/16, W/16]</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"depth logits shape: </span><span class="sc">{</span>depth_logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>feature_maps  <span class="op">=</span> feature_head(x_feat)      <span class="co"># [B, C, H/16, W/16]</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"feature maps shape: </span><span class="sc">{</span>feature_maps<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Softmax over the depth-channel dimension to get a categorical distribution</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co"># per pixel. This yields p(d | pixel) that sums to 1 across D for each (h,w).</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>depth_prob <span class="op">=</span> torch.softmax(depth_logits, dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [B, D, H/16, W/16]</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co"># "Lift" operation (outer product across depth and context):</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="co"># For each pixel, expand:</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co">#   - depth_prob: [B, D, H, W]  -&gt; [B, 1, D, H, W]</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="co">#   - feature_maps: [B, C, H, W] -&gt; [B, C, 1, H, W]</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a><span class="co"># and multiply to obtain per-depth features: [B, C, D, H, W].</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>lifted_features <span class="op">=</span> torch.einsum(<span class="st">'bdhw,bchw-&gt;bcdhw'</span>, depth_prob, feature_maps)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"lifted features shape: </span><span class="sc">{</span>lifted_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [B, C, D, H/16, W/16]</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>lifted_features <span class="op">=</span> lifted_features.view(batch_size, num_cameras_per_batch, number_of_conv_features, number_of_depth_bins, feature_map_height, feature_map_width)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"lifted features reshaped to per-camera: </span><span class="sc">{</span>lifted_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [B, num_cams, C, D, H/16, W/16]</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="co"># This permute is needed because the frustum is already is shaper as [B, num_cams, C, D, H, W] and we need to asscociated these two in splat step</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>lifted_features <span class="op">=</span> lifted_features.permute(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">2</span>)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"lifted features permuted to per-camera: </span><span class="sc">{</span>lifted_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [B, num_cams, D, H/16, W/16, C]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>depth logits shape: torch.Size([12, 41, 8, 22])
feature maps shape: torch.Size([12, 64, 8, 22])
lifted features shape: torch.Size([12, 64, 41, 8, 22])
lifted features reshaped to per-camera: torch.Size([2, 6, 64, 41, 8, 22])
lifted features permuted to per-camera: torch.Size([2, 6, 41, 8, 22, 64])</code></pre>
</div>
</div>
</section>
<section id="splat" class="level2">
<h2 class="anchored" data-anchor-id="splat">Splat</h2>
<p><img src="images/Splat.jpg" class="img-fluid" width="600"></p>
<p>Imagine you have a cloud of 3D points (Which is the frustum with ego coordinates which we made earlier) where each point carries a feature vector (Which is the one we created using the efficientnet backbone). These points are scattered in space, but out goal is to organize them into a structured grid so you can run a convolutional network on it (because CNNs need regular grids, not random points). This is exactly what the the splat is doing</p>
<p>Now We have (per batch): - x: shape (B, N, D, H, W, C) - Each element x[b, n, d, h, w] is a learned feature vector representing “what would be at depth bin d along pixel (h,w) of camera n”. - It already encodes both appearance (from EfficientNet) and soft depth focus (via multiplying by depth probabilities; high prob depth bins dominate).</p>
<ul>
<li>frustum_3d_points_in_ego_frame: shape (B, N, D, H, W, 3)
<ul>
<li>Pure geometry: the ego-frame 3D coordinates (X,Y,Z) that correspond to that pixel-depth hypothesis.</li>
<li>No learning happens here; it’s deterministic from intrinsics/extrinsics + augmentation undo + chosen depth bin centers.</li>
</ul></li>
</ul>
<p><strong>Think of frustum_3d_points_in_ego_frame as a mapping function f: (n, d, h, w) → voxel index (ix, iy, iz) via discretization. x supplies the <em>content</em> to deposit there.</strong></p>
<ul>
<li><p>3D frustum coordinate to voxel index conversion <span class="math display">\[
\text{idx}_i = \frac{\text{coord}_i - (b_i - d_i / 2)}{d_i}
\]</span></p>
<p>The splat step follows this sequence:</p>
<p>Flatten → Voxelize → Mask → Sort → Cumsum → BEV Grid</p></li>
<li><p><strong>Flatten</strong>: reshape tensors to <code>(B*N*D*H*W, C)</code> and <code>(B*N*D*H*W, 3)</code></p></li>
<li><p><strong>Voxelize</strong>: map each 3D point to a voxel index</p></li>
<li><p><strong>Mask</strong>: remove out-of-bounds points</p></li>
<li><p><strong>Sort &amp; Cumsum</strong>: group features by voxel index and sum them</p></li>
<li><p><strong>BEV Grid</strong>: place summed features into a structured grid</p></li>
</ul>
<p>This pooling is <strong>parameter-free</strong>, but its structure determines how gradients flow back to image features and depth probabilities.</p>
<div id="682f5cf3" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>lifted_features_flattened <span class="op">=</span> lifted_features.reshape(<span class="op">-</span><span class="dv">1</span>, number_of_conv_features)  <span class="co"># [(B * num_cams * D * H/16 * W/16), C]</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>voxel_indices <span class="op">=</span> (frustum_3d_points_in_ego_frame <span class="op">-</span> (bx <span class="op">-</span> dx<span class="op">/</span><span class="fl">2.0</span>))<span class="op">/</span>dx</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>voxel_indices <span class="op">=</span> voxel_indices.<span class="bu">long</span>().view(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)  <span class="co"># [(B * num_cams * D * H/16 * W/16), 3]</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co"># batch_ix: with rows like [0,0,...,0, 1,1,...,1, ..., B-1,...,B-1]</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> voxel_indices.shape[<span class="dv">0</span>] <span class="op">//</span> batch_size</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>batch_ix <span class="op">=</span> torch.arange(batch_size).repeat_interleave(q).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>voxel_indices <span class="op">=</span> torch.cat((voxel_indices,batch_ix), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># [(B * num_cams * D * H/16 * W/16), 4]</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>valid_indices_mask_within_grid_bounds <span class="op">=</span> (voxel_indices[:,<span class="dv">0</span>] <span class="op">&gt;=</span><span class="dv">0</span>) <span class="op">&amp;</span> (voxel_indices[:, <span class="dv">0</span>] <span class="op">&lt;</span> nx[<span class="dv">0</span>]) <span class="op">\</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>     <span class="op">&amp;</span> (voxel_indices[:, <span class="dv">1</span>] <span class="op">&gt;=</span> <span class="dv">0</span>) <span class="op">&amp;</span> (voxel_indices[:, <span class="dv">1</span>] <span class="op">&lt;</span> nx[<span class="dv">1</span>]) <span class="op">\</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>     <span class="op">&amp;</span> (voxel_indices[:, <span class="dv">2</span>] <span class="op">&gt;=</span> <span class="dv">0</span>) <span class="op">&amp;</span> (voxel_indices[:, <span class="dv">2</span>] <span class="op">&lt;</span> nx[<span class="dv">2</span>])</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>valid_lifted_features <span class="op">=</span> lifted_features_flattened[valid_indices_mask_within_grid_bounds]</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>valid_voxel_indices <span class="op">=</span> voxel_indices[valid_indices_mask_within_grid_bounds]</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># This creates a unique linear ordering key so that All points for the same (ix, iy, iz, b) voxel end up contiguous and will have the same value, we have batch_size included to make sure points from different batches go to different voxels</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Grouping identical voxel indices allows fast segment reduction using a single pass cumulative sum instead of hash maps or scatter with atomic ops which is needed for the cumsum trick below</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>ranks <span class="op">=</span> valid_voxel_indices[:,<span class="dv">0</span>]<span class="op">*</span>(nx[<span class="dv">1</span>]<span class="op">*</span>nx[<span class="dv">2</span>]<span class="op">*</span>batch_size) <span class="op">+</span> valid_voxel_indices[:,<span class="dv">1</span>]<span class="op">*</span>(nx[<span class="dv">2</span>]<span class="op">*</span>batch_size) <span class="op">+</span> valid_voxel_indices[:,<span class="dv">2</span>]<span class="op">*</span>batch_size <span class="op">+</span> valid_voxel_indices[:,<span class="dv">3</span>]</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>sorts <span class="op">=</span> ranks.argsort()</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>valid_lifted_features_sorted <span class="op">=</span> valid_lifted_features[sorts]</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>valid_voxel_indices_sorted <span class="op">=</span> valid_voxel_indices[sorts]</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>ranks <span class="op">=</span> ranks[sorts]    </span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">NOTE</span><span class="co">: Cumsum trick, which is something this paper introduced.</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Below is an example to illustrate the cumsum trick for segment summing</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="co"># rank = [a,a,a,b,c,c]</span></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a><span class="co"># features = [2,3,5,7,11,13]</span></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="co"># indices = [0,1,2,3,4,5]</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="co"># cumsum(features) = [2,5,10,17,28,41]</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="co"># kept =[False, False, True, True, False, True] </span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="co"># cumsum(features)[kept] = [10,17,41]</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a><span class="co"># summed_features = [10, 17-10, 41-17] = [10,7,24]</span></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>valid_lifted_features_cumsum <span class="op">=</span> torch.cumsum(valid_lifted_features_sorted, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a>kept <span class="op">=</span> torch.ones_like(ranks, dtype<span class="op">=</span>torch.<span class="bu">bool</span>)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>kept[:<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> (ranks[<span class="dv">1</span>:] <span class="op">!=</span> ranks[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a>valid_lifted_features_aggregated <span class="op">=</span> valid_lifted_features_cumsum[kept]</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>valid_voxel_indices_sorted <span class="op">=</span> valid_voxel_indices_sorted[kept]</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>valid_summed_features <span class="op">=</span> torch.cat((valid_lifted_features_aggregated[<span class="dv">0</span>:<span class="dv">1</span>], valid_lifted_features_aggregated[<span class="dv">1</span>:] <span class="op">-</span> valid_lifted_features_aggregated[:<span class="op">-</span><span class="dv">1</span>]), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a><span class="co"># griddify (B x C x Z x X x Y)</span></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>voxel_grid_pooled_features <span class="op">=</span> torch.zeros((batch_size, number_of_conv_features, nx[<span class="dv">2</span>], nx[<span class="dv">0</span>], nx[<span class="dv">1</span>]), device<span class="op">=</span>valid_summed_features.device)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>voxel_grid_pooled_features[valid_voxel_indices_sorted[:,<span class="dv">3</span>], :, valid_voxel_indices_sorted[:,<span class="dv">2</span>], valid_voxel_indices_sorted[:,<span class="dv">0</span>], valid_voxel_indices_sorted[:,<span class="dv">1</span>]] <span class="op">=</span> valid_summed_features</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="co"># collapse Z</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a>voxel_grid_pooled_features <span class="op">=</span> torch.cat(voxel_grid_pooled_features.unbind(dim<span class="op">=</span><span class="dv">2</span>), <span class="dv">1</span>)</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"voxel grid pooled features shape after collapsing Z: </span><span class="sc">{</span>voxel_grid_pooled_features<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [B, C*Z, X, Y]</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>voxel grid pooled features shape after collapsing Z: torch.Size([2, 64, 200, 200])</code></pre>
</div>
</div>
<p>The splat step is the <strong>bridge</strong> between unstructured 3D projections and structured BEV processing. It’s where geometry meets learning and it sets the stage for the BEV encoder to make sense of the scene.</p>
</section>
<section id="bevencodeshoot" class="level2">
<h2 class="anchored" data-anchor-id="bevencodeshoot">BevEncode/shoot</h2>
<ul>
<li>The BevEncode module acts as a 2D convolutional encoder that processes the bird’s-eye view (BEV) feature map.</li>
<li>Intuitively, it’s like a mini neural network that learns to interpret the spatial layout of the scene from above</li>
<li>The final output is a map of logits indicating things like occupancy or drivable area, helping the model make predictions about the scene layout from the aggregated voxel features. (In the opensourced paper code its occupancy)</li>
<li>This step is not exactly the shoot they discuss in the paper, but i think it can be considered like shoot as well.</li>
</ul>
<div id="359a19f4" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BevEncode(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inC, outC, backbone<span class="op">=</span><span class="st">'resnet18'</span>, pretrained<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bk <span class="op">=</span> timm.create_model(backbone, in_chans<span class="op">=</span>inC, features_only<span class="op">=</span><span class="va">True</span>, pretrained<span class="op">=</span>pretrained)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        c1, c3 <span class="op">=</span> <span class="va">self</span>.bk.feature_info.channels()[<span class="dv">1</span>], <span class="va">self</span>.bk.feature_info.channels()[<span class="dv">3</span>]  <span class="co"># layer1 (/4), layer3 (/16)</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lat3 <span class="op">=</span> nn.Conv2d(c3, <span class="dv">256</span>, <span class="dv">1</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lat1 <span class="op">=</span> nn.Conv2d(c1, <span class="dv">64</span>, <span class="dv">1</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fuse <span class="op">=</span> nn.Sequential(</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span><span class="op">+</span><span class="dv">64</span>, <span class="dv">256</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            nn.BatchNorm2d(<span class="dv">256</span>),</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nn.Sequential(</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>            nn.Upsample(size<span class="op">=</span>(<span class="dv">200</span>, <span class="dv">200</span>), mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span>, <span class="dv">128</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>), nn.BatchNorm2d(<span class="dv">128</span>), nn.ReLU(inplace<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, outC, <span class="dv">1</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        feats <span class="op">=</span> <span class="va">self</span>.bk(x)            </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        x1, x3 <span class="op">=</span> feats[<span class="dv">1</span>], feats[<span class="dv">3</span>]    </span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        x1_lat <span class="op">=</span> <span class="va">self</span>.lat1(x1)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        x3_up <span class="op">=</span> F.interpolate(<span class="va">self</span>.lat3(x3), size<span class="op">=</span>x1_lat.shape[<span class="op">-</span><span class="dv">2</span>:], mode<span class="op">=</span><span class="st">'bilinear'</span>, align_corners<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.fuse(torch.cat([x3_up, x1_lat], <span class="dv">1</span>))</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.head(y)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>bev_encoder <span class="op">=</span> BevEncode(inC<span class="op">=</span>number_of_conv_features, outC<span class="op">=</span><span class="dv">1</span>, backbone<span class="op">=</span><span class="st">'resnet18'</span>, pretrained<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> bev_encoder(voxel_grid_pooled_features)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"BEV encoder output shape: </span><span class="sc">{</span>preds<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># [B, 1, ny, nx]</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co">##</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>BEV encoder output shape: torch.Size([2, 1, 200, 200])</code></pre>
</div>
</div>
</section>
<section id="loss" class="level2">
<h2 class="anchored" data-anchor-id="loss">Loss</h2>
<ul>
<li>The loss compares BEV logits to a binary BEV occupancy target via BCEWithLogitsLoss.</li>
<li>The key gradient at each cell flows back through the BEV head and backbone, into the summed voxel features, and further into depth probabilities and camera features.</li>
<li>This creates a powerful loop: BEV supervision teaches both which depths to trust and what image features to extract so that lifted features land in the right voxels and classify correctly.</li>
</ul>
<div id="24512511" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_fn(preds.squeeze(<span class="dv">1</span>), target_binary_image)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="gradient-flow-insight-through-voxel-pooling" class="level2">
<h2 class="anchored" data-anchor-id="gradient-flow-insight-through-voxel-pooling">Gradient Flow Insight through voxel pooling</h2>
<section id="how-gradients-flow-through-voxel-pooling" class="level3">
<h3 class="anchored" data-anchor-id="how-gradients-flow-through-voxel-pooling"><strong>How Gradients Flow Through Voxel Pooling</strong></h3>
<p>In Lift-Splat-Shoot, voxel pooling is a <strong>sum operation</strong> over features projected into a voxel from different camera rays.</p>
<p><strong>Forward Pass</strong></p>
<p>Each voxel <span class="math inline">\(v\)</span> gets a feature vector:</p>
<p><span class="math display">\[
V_v = \sum_i x_i
\]</span></p>
<p>where each <span class="math inline">\(x_i\)</span> is a feature from a frustum point (a pixel-depth pair) that projects into voxel <span class="math inline">\(v\)</span>.</p>
<p>Then, the BEV encoder processes <span class="math inline">\(V_v\)</span> to produce logits <span class="math inline">\(\rightarrow\)</span> loss <span class="math inline">\(L\)</span>.</p>
<p><strong>Backward Pass</strong></p>
<p>During backpropagation:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial x_i} = \frac{\partial L}{\partial V_v} \cdot \frac{\partial V_v}{\partial x_i} = \frac{\partial L}{\partial V_v}
\]</span></p>
<p>Because <span class="math inline">\(V_v\)</span> is just a sum, the gradient with respect to each contributing <span class="math inline">\(x_i\)</span> is <strong>identical</strong>.</p>
<section id="intuition" class="level4">
<h4 class="anchored" data-anchor-id="intuition"><strong>Intuition</strong></h4>
<ul>
<li>Every frustum sample that lands in voxel <span class="math inline">\(v\)</span> gets the same gradient.</li>
<li>If voxel <span class="math inline">\(v\)</span> is labeled as <strong>occupied</strong> (foreground), and the model predicts low logit, the gradient will push all contributing <span class="math inline">\(x_i\)</span> to look more like “foreground”.</li>
<li>If voxel <span class="math inline">\(v\)</span> is <strong>background</strong>, the gradient pushes all <span class="math inline">\(x_i\)</span> to look more like “background”.</li>
</ul>
<p>This creates <strong>coherent learning</strong>: all rays that vote for a voxel are nudged in the same direction, reinforcing consistent feature learning.</p>
</section>
<section id="further-gradient-flow" class="level4">
<h4 class="anchored" data-anchor-id="further-gradient-flow"><strong>Further Gradient Flow</strong></h4>
<p>These gradients then flow into:</p>
<ul>
<li><strong>Depth softmax weights</strong>: Encouraging the model to assign higher probability to the correct depth bin.</li>
<li><strong>Camera feature extractor</strong>: Learning better per-pixel features that are discriminative for occupancy.</li>
</ul>
</section>
</section>
<section id="how-depth-probabilities-interact" class="level3">
<h3 class="anchored" data-anchor-id="how-depth-probabilities-interact"><strong>How Depth Probabilities Interact</strong></h3>
<p>Before pooling, each pixel’s feature is <strong>modulated by depth probability</strong>:</p>
<p><span class="math display">\[
\text{new\_x}[n, d, h, w, :] = \text{depth\_prob}[n, d, h, w] \cdot \text{feat}[n, h, w, :]
\]</span></p>
<section id="what-happens-if-depth-is-uncertain" class="level4">
<h4 class="anchored" data-anchor-id="what-happens-if-depth-is-uncertain"><strong>What Happens If Depth Is Uncertain?</strong></h4>
<ul>
<li>Depth probability is spread across bins <span class="math inline">\(\rightarrow\)</span> feature energy is <strong>dispersed</strong> across multiple voxels.</li>
<li>This leads to <strong>diluted evidence</strong> in BEV <span class="math inline">\(\rightarrow\)</span> weaker activations <span class="math inline">\(\rightarrow\)</span> higher loss.</li>
<li>Gradients push depth logits to <strong>sharpen</strong> around the correct depth.</li>
</ul>
</section>
<section id="what-happens-if-depth-is-accurate" class="level4">
<h4 class="anchored" data-anchor-id="what-happens-if-depth-is-accurate"><strong>What Happens If Depth Is Accurate?</strong></h4>
<ul>
<li>Most feature mass lands in the correct voxel <span class="math inline">\(\rightarrow\)</span> strong activation <span class="math inline">\(\rightarrow\)</span> easier classification.</li>
<li>Training encourages depth certainty that aligns with <strong>occupancy labels</strong> and is <strong>consistent across views</strong>.</li>
</ul>
</section>
</section>
<section id="multi-camera-fusion-effect" class="level3">
<h3 class="anchored" data-anchor-id="multi-camera-fusion-effect"><strong>Multi-Camera Fusion Effect</strong></h3>
<p>Multiple cameras may project rays into the <strong>same voxel</strong> from different angles.</p>
<section id="summation-across-cameras" class="level4">
<h4 class="anchored" data-anchor-id="summation-across-cameras"><strong>Summation Across Cameras</strong></h4>
<ul>
<li><strong>True positives</strong>: Multiple views reinforce the voxel’s activation <span class="math inline">\(\rightarrow\)</span> stronger signal.</li>
<li><strong>False positives</strong>: Noise from one camera is unlikely to be corroborated <span class="math inline">\(\rightarrow\)</span> low magnitude <span class="math inline">\(\rightarrow\)</span> treated as background.</li>
</ul>
</section>
<section id="intuition-1" class="level4">
<h4 class="anchored" data-anchor-id="intuition-1"><strong>Intuition</strong></h4>
<p>This acts like a <strong>consensus mechanism</strong>:</p>
<ul>
<li>Voxels with consistent multi-view support are boosted.</li>
<li>Voxels with isolated, weak support are suppressed.</li>
<li>Also note that mutliple coordinates from a single image can land in the same BEV grid as well.</li>
</ul>
</section>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary</strong></h3>
<p>Lift-Splat-Shoot’s voxel pooling and depth-weighted projection create a <strong>differentiable pipeline</strong> where:</p>
<ul>
<li>Gradients flow coherently to all contributing frustum points.</li>
<li>Depth estimation is refined via supervision from occupancy labels.</li>
<li>Multi-camera fusion enhances robustness and reduces noise.</li>
</ul>
</section>
</section>
<section id="credits" class="level2">
<h2 class="anchored" data-anchor-id="credits">Credits</h2>
<p>The paper is extremely well written and the code is opensourced , so all thanks to the authors Jonah Philion and Sanja Fidler</p>
<ul>
<li><a href="https://github.com/nv-tlabs/lift-splat-shoot">Original Lift Splat Shoot Implementation</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/akashprakas\.github\.io\/akashBlog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>