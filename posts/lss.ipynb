{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5b352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96990de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "denormalize_img = T.Compose([\n",
    "    T.Normalize(\n",
    "        mean=[-m/s for m, s in zip(mean, std)],\n",
    "        std=[1/s for s in std],\n",
    "    ),\n",
    "    T.Lambda(lambda x: x.clamp(0, 1)),  \n",
    "    T.ToPILImage(),\n",
    "])\n",
    "\n",
    "normalize_img = T.Compose([\n",
    "    T.ToTensor(),                      \n",
    "    T.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5584fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.599 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot='/home/ap/dataset/nuscenes', verbose=True)\n",
    "\n",
    "last_sample_token = '63c24b51feb94f14bec29022dae4975d'\n",
    "second_last_sample_token  = 'de9432d4fc7a4e5a985e2bc628eb614c'\n",
    "last_sample = nusc.get('sample', last_sample_token)\n",
    "second_last_sample = nusc.get('sample', second_last_sample_token)\n",
    "samples = [second_last_sample, last_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a17f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cams = ['CAM_FRONT_LEFT','CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "\n",
    "input_image_height = 128\n",
    "input_image_width = 352\n",
    "\n",
    "images_per_batch = []\n",
    "rotations_per_batch = []\n",
    "translations_per_batch = []\n",
    "intrinsics_per_batch = []\n",
    "resize_transforms_per_batch = []\n",
    "crop_transforms_per_batch = []\n",
    "gt_binary_images_per_batch = []\n",
    "\n",
    "for my_sample in samples:\n",
    "    images = []\n",
    "    rotations = []\n",
    "    translations = []\n",
    "    intrinsics = []\n",
    "    resize_transforms = []\n",
    "    crop_transforms = []\n",
    "    for cam in cams:\n",
    "        cam_data = nusc.get('sample_data', my_sample['data'][cam])\n",
    "        image_name = os.path.join(nusc.dataroot, cam_data['filename'])\n",
    "        img  = Image.open(image_name)\n",
    "        # print(f\"Image from {cam}: size {img.size}\")\n",
    "\n",
    "        calibrated_sensor_token = cam_data['calibrated_sensor_token']\n",
    "        calibrated_sensor = nusc.get('calibrated_sensor', calibrated_sensor_token)\n",
    "        rotation = torch.tensor(Quaternion(calibrated_sensor['rotation']).rotation_matrix)\n",
    "        translation = torch.tensor(calibrated_sensor['translation'])\n",
    "        camera_intrinsic = torch.tensor(calibrated_sensor['camera_intrinsic'])\n",
    "        H,W = img.size[1], img.size[0]\n",
    "        # print(f\"Rotation:\\n{rotation}\\nTranslation:\\n{translation}\\nIntrinsics:\\n{camera_intrinsic}\\nImage Height: {H}, Width: {W}\\n\")\n",
    "        # print(\"=============================================================================================================================================\")\n",
    "\n",
    "        resize = max(input_image_height / H, input_image_width / W) #this is to maintain aspect ratio\n",
    "        resize_dims = (int(W * resize), int(H * resize))\n",
    "        # print(f\"Resize dimensions to fit feature map: {resize_dims}\\n\\n\")\n",
    "        newW, newH = resize_dims\n",
    "        # cropping from the top since the top usually has more irrelevant info (sky) \n",
    "        # in the orginal implementation, cropping is done from bottom + top randomly with more weight on top\n",
    "        crop_h = newH - input_image_height\n",
    "        crop_w = newW - input_image_width\n",
    "        crop = (crop_w, crop_h, crop_w + input_image_width, crop_h + input_image_height)\n",
    "        # print(f\"Crop box to get feature map size: {crop}\\n\")\n",
    "\n",
    "        img = img.resize(resize_dims).crop(crop)\n",
    "        #post homography transformations,\n",
    "        # we can consider resize and crop as two homography transformations and need to make matrices for them, because we want to undo later during projection because the intrinscs\n",
    "        # and extrinsics are for original image size and this needs to be taken care\n",
    "        # ideally these two should be two dimensional but now making it 3 work numerically well so\n",
    "        # for resize transform for z we multiply by 1, which is a no change and for crop_transform for z we subtract 0 which is also a no change. \n",
    "        resize_transform = torch.diag(torch.tensor([resize,resize, 1.0])) \n",
    "        crop_transform   = -torch.tensor([[crop_w,crop_h,0.0]])\n",
    "\n",
    "        # print(f\"Resize transform:\\n{resize_transform}\")\n",
    "        # print(f\"Crop transform:\\n{crop_transform}\")\n",
    "\n",
    "        images.append(normalize_img(img))\n",
    "        rotations.append(rotation)\n",
    "        translations.append(translation)\n",
    "        intrinsics.append(camera_intrinsic)\n",
    "        resize_transforms.append(resize_transform)\n",
    "        crop_transforms.append(crop_transform)\n",
    "\n",
    "    images_per_batch.append(torch.stack(images))\n",
    "    rotations_per_batch.append(torch.stack(rotations))  \n",
    "    translations_per_batch.append(torch.stack(translations))\n",
    "    intrinsics_per_batch.append(torch.stack(intrinsics))\n",
    "    resize_transforms_per_batch.append(torch.stack(resize_transforms))\n",
    "    crop_transforms_per_batch.append(torch.stack(crop_transforms))\n",
    "    \n",
    "\n",
    "\n",
    "    sd_lidar = nusc.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "    # get ego pose to transform from global to ego vehicle frame\n",
    "    egopose = nusc.get('ego_pose', sd_lidar['ego_pose_token'])\n",
    "    trans = -np.array(egopose['translation'])\n",
    "    rot = Quaternion(egopose['rotation']).inverse\n",
    "\n",
    "    nx = np.array([200, 200,1]) \n",
    "    bx = np.array([-49.75, -49.75, -3.0])\n",
    "    dx = np.array([0.5, 0.5, 20.0])\n",
    "\n",
    "    target_binary_image = np.zeros((nx[0], nx[1]))\n",
    "    for tok in my_sample['anns']:\n",
    "        inst = nusc.get('sample_annotation', tok)\n",
    "        # add category for lyft\n",
    "        if not inst['category_name'].split('.')[0] == 'vehicle':\n",
    "            continue\n",
    "        box = Box(inst['translation'], inst['size'], Quaternion(inst['rotation']))\n",
    "        # bring box to ego frame\n",
    "        box.translate(trans)\n",
    "        box.rotate(rot)\n",
    "\n",
    "        pts = box.bottom_corners()[:2].T\n",
    "        # bring points to grid coords\n",
    "        pts = np.round(\n",
    "            (pts - bx[:2] + dx[:2]/2.) / dx[:2]\n",
    "            ).astype(np.int32)\n",
    "        pts[:, [1, 0]] = pts[:, [0, 1]]\n",
    "        cv2.fillPoly(target_binary_image, [pts], 1.0)\n",
    "    gt_binary_images_per_batch.append(torch.tensor(target_binary_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d01a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"images\":torch.stack(images_per_batch),  # (B, num_cams, 3, H, W)\n",
    "        \"rotations\":torch.stack(rotations_per_batch),  # (B, num_cams, 3, 3)\n",
    "        \"translations\":torch.stack(translations_per_batch),  # (B, num_cams, 3)\n",
    "        \"intrinsics\":torch.stack(intrinsics_per_batch),  # (B, num_cams, 3, 3),\n",
    "        \"resize_transforms\":torch.stack(resize_transforms_per_batch),  # (B, num_cams, 3, 3)\n",
    "        \"crop_transforms\":torch.stack(crop_transforms_per_batch),  # (B, num_cams, 1, 3)\n",
    "        \"gt_binary_images\":torch.stack(gt_binary_images_per_batch)  # (B, H, W)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2df1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.load(\"nuscenes_mini_sample_data_batch.pt\")\n",
    "torch.save(data,\"nuscenes_mini_sample_data_batch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9daef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_batch = torch.stack(images_per_batch)  # (B, num_cams, 3, H, W)\n",
    "rotations_per_batch = torch.stack(rotations_per_batch)  # (B, num_cams, 3, 3)\n",
    "translations_per_batch = torch.stack(translations_per_batch)  # (B, num_cams, 3)\n",
    "intrinsics_per_batch = torch.stack(intrinsics_per_batch)  # (B, num_cams, 3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef9c347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 8, 22, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image height and width after resize\n",
    "image_height = 128\n",
    "image_width = 352\n",
    "\n",
    "nx = np.array([200, 200,1]) # number of cells in x,y,z in the bev grid\n",
    "bx = np.array([-49.75, -49.75, -3.0]) # bottom left corner x,y,z of the bev grid\n",
    "dx = np.array([0.5, 0.5, 20.0]) # x,y,z resolution of the bev grid\n",
    "\n",
    "stride = 16\n",
    "# feature map size after feature extraction from the input image\n",
    "feature_map_height = image_height//stride\n",
    "feature_map_width = image_width//stride  #feature map size, (8, 22)\n",
    "\n",
    "depth_bin = [4.0,45.0,1.0] # start,stop,step \n",
    "ds = torch.arange(*depth_bin,dtype=torch.float).view(-1,1,1).expand(-1,feature_map_height,feature_map_width)\n",
    "\n",
    "D,_,_ = ds.shape\n",
    "xs = torch.linspace(0, image_width - 1, feature_map_width, dtype=torch.float).view(1, 1, feature_map_width).expand(D, feature_map_height, feature_map_width)\n",
    "ys = torch.linspace(0, image_height - 1, feature_map_height, dtype=torch.float).view(1, feature_map_height, 1).expand(D, feature_map_height, feature_map_width)\n",
    "lifted_image_coordinates = torch.stack((xs,ys,ds),-1)\n",
    "lifted_image_coordinates.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30937b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'rotations', 'translations', 'intrinsics', 'resize_transforms', 'crop_transforms', 'gt_binary_images'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cams  = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "data = torch.load(\"nuscenes_mini_sample_data_batch.pt\")\n",
    "data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b178096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93809b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, rotations, translations, intrinsics, resize_transforms, crop_transforms, target_binary_image = (\n",
    "    data['images'],\n",
    "    data['rotations'],\n",
    "    data['translations'],\n",
    "    data['intrinsics'],\n",
    "    data['resize_transforms'],\n",
    "    data['crop_transforms'],\n",
    "    data['target_bin_image']\n",
    ")\n",
    "\n",
    "\n",
    "def plot_all_cams(images_batch, cams):\n",
    "    imgs = images_batch  # tensor [6, C, H, W] or [6, H, W, C]\n",
    "    n = len(cams)\n",
    "    rows, cols = 2, 3\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for i in range(n):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        img = denormalize_img(imgs[i])\n",
    "        plt.imshow(img)\n",
    "        plt.title(cams[i], fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.suptitle('All camera images resized and cropped', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "idx = 0\n",
    "plot_all_cams(images[1], cams)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_bev(target_binary_image, nx, bx, dx):\n",
    "    bev = target_binary_image.detach().cpu().numpy() if torch.is_tensor(target_binary_image) else target_binary_image\n",
    "    bev = bev.T \n",
    "    x_min, y_min = bx[0], bx[1]\n",
    "    x_max = x_min + nx[0] * dx[0]\n",
    "    y_max = y_min + nx[1] * dx[1]\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(bev, cmap='gray', origin='lower', extent=[x_min, x_max, y_min, y_max], interpolation='nearest')\n",
    "    plt.xlabel('X (m) — forward')\n",
    "    plt.ylabel('Y (m) — left')\n",
    "    plt.title('BEV target ')\n",
    "    plt.grid(color='w', alpha=0.1, linestyle=':')\n",
    "    plt.colorbar(label='occupancy')\n",
    "    # Ego at (0,0)\n",
    "    plt.scatter([0], [0], c='red', s=30, label='Ego')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_bev(target_binary_image[0],nx,bx,dx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24512511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ## Frustum Creation \n",
    "\n",
    "\n",
    "\n",
    "# so the lifted image coordinates are in post augmented space, but the intrinsics are pre-augmented\n",
    "# so we should map the points back, in the example i used resize and crop, so we should apply inverse transforms here\n",
    "# before converting them to 3D points in ego frame\n",
    "\n",
    "\n",
    "# so you can think of the transformation as \n",
    "# X_cropped_and_resized = Resize_transform*X + T # where R is the resize scale matrix and T is the crop translation\n",
    "# X = Resize_transform @ (X_cropped_and_resized - T)\n",
    "\n",
    "batch_size,num_cameras_per_batch, _,_ = crop_transforms.shape\n",
    " # the last dimension is for x,y,depth and first two are batch and camera index and the D,H,W need to be broadcasted\n",
    "lifted_image_coordinates_in_orginal_image_dimension = lifted_image_coordinates - crop_transforms.view(batch_size,num_cameras_per_batch,1,1,1,3)\n",
    "lifted_image_coordinates_in_orginal_image_dimension =torch.inverse(resize_transforms).view(batch_size,num_cameras_per_batch,1,1,1,3,3).matmul(lifted_image_coordinates_in_orginal_image_dimension.unsqueeze(-1))\n",
    "\n",
    "\n",
    "lifted_image_coordinates_in_orginal_image_dimension = lifted_image_coordinates_in_orginal_image_dimension.squeeze(-1)\n",
    "\n",
    "lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_xy = lifted_image_coordinates_in_orginal_image_dimension[...,:2]*lifted_image_coordinates_in_orginal_image_dimension[...,2:3]\n",
    "lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_x_y_z = torch.cat((lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_xy, lifted_image_coordinates_in_orginal_image_dimension[...,2:3]),-1).unsqueeze(-1)\n",
    "\n",
    "# first we have to multiply with inverse intrinsics to get the 3D points in camera frame\n",
    "# [X_c, Y_c, Z_c] = inv(K) * [u*Z_c, v*Z_c, Z_c]\n",
    "# Then we can transform to ego frame using rotation and translation\n",
    "# [X_ego, Y_ego, Z_ego] = R_c2e * [X_c, Y_c, Z_c] + T_c2e\n",
    "# we can combine these two steps into one matrix multiplication and one addition\n",
    "combined_k_inverse_and_rotation = rotations.to(dtype=intrinsics.dtype).matmul(torch.inverse(intrinsics))\n",
    "frustum_3d_points_in_ego_frame = combined_k_inverse_and_rotation.view(batch_size,num_cameras_per_batch,1,1,1,3,3).matmul(lifted_image_coordinates_in_orginal_image_dimension_with_unnormalized_x_y_z).squeeze(-1) + translations.view(batch_size,num_cameras_per_batch,1,1,1,3)\n",
    "\n",
    "\n",
    "# ## Now Camera Encode\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "# NOTE: The architecture here is directly from the implemenation of LIFT,\n",
    "#       I just used timm for simplicity and added some comments.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) Build a feature-extractor backbone \n",
    "#    out_indices=(3,4) for EfficientNet-B0 correspond to:\n",
    "#      - idx 3: stride ~ /16, channels = 112\n",
    "#      - idx 4: stride ~ /32, channels = 320\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "batch,num_cameras,channel,h,w = images.shape\n",
    "x = images.view(batch*num_cameras,channel,h,w)\n",
    "\n",
    "backbone = timm.create_model(\n",
    "    'efficientnet_b0',\n",
    "    pretrained=False,            \n",
    "    features_only=True,\n",
    "    out_indices=(3, 4)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) Forward through the backbone to get two scales:\n",
    "#    x16: [B, 112, H/16, W/16]\n",
    "#    x32: [B, 320, H/32, W/32]\n",
    "# ------------------------------------------------------------\n",
    "feats = backbone(x)\n",
    "x16, x32 = feats[0], feats[1]  \n",
    "\n",
    "print(f\"x16 (idx=3) shape: {x16.shape}\")  \n",
    "print(f\"x32 (idx=4) shape: {x32.shape}\")   \n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Upsample the deeper /32 map to match /16 spatial size.\n",
    "# ------------------------------------------------------------\n",
    "upsample_to_x16 = nn.Upsample(size=x16.shape[-2:], mode='bilinear', align_corners=True)\n",
    "x32_up = upsample_to_x16(x32)\n",
    "print(f\"x32 upsampled to x16 size: {x32_up.shape}\")  #  [B, 320, H/16, W/16]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) Concatenate along channels (skip fusion like U-Net).\n",
    "#    in_channels = 112 + 320 = 432 for EfficientNet-B0 \n",
    "# ------------------------------------------------------------\n",
    "x_cat = torch.cat([x16, x32_up], dim=1)\n",
    "print(f\"concatenated shape: {x_cat.shape}\")  # ~ [B, 432, H/16, W/16]\n",
    "\n",
    "in_channels = x_cat.shape[1]  \n",
    "out_channels = 512             \n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Fuse + refine: two 3x3 convs with BN + ReLU.\n",
    "#    - First 3x3 mixes skip & deep features and compresses to out_channels\n",
    "#    - Second 3x3 refines after nonlinearity (effective RF ~5x5 total)\n",
    "#    - bias=False because BatchNorm has its own affine params\n",
    "# ------------------------------------------------------------\n",
    "fuse_and_refine = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(out_channels),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(out_channels),\n",
    "    nn.ReLU(inplace=True),\n",
    ")\n",
    "\n",
    "x_feat = fuse_and_refine(x_cat)\n",
    "print(f\"fused features shape: {x_feat.shape}\")  # ~ [B, 512, H/16, W/16]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "x_feat.shape\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "number_of_depth_bins = D\n",
    "number_of_conv_features = 64\n",
    "# Two independent heads (1x1) for depth and feature prediction from the fused features\n",
    "depth_head = nn.Conv2d(out_channels, number_of_depth_bins, kernel_size=1)\n",
    "feature_head = nn.Conv2d(out_channels, number_of_conv_features, kernel_size=1)\n",
    "\n",
    "depth_logits = depth_head(x_feat)\n",
    "print(f\"depth logits shape: {depth_logits.shape}\")  # ~ [B, D, H/16, W/16]\n",
    "feature_maps = feature_head(x_feat)\n",
    "print(f\"feature maps shape: {feature_maps.shape}\")  # ~ [B, 64, H/16, W/16] \n",
    "\n",
    "\n",
    "depth_prob = nn.functional.softmax(depth_logits, dim=1)\n",
    "\n",
    "lifted = torch.einsum('bdhw,bchw->bcdhw', depth_prob, feature_maps)\n",
    "print(f\"lifted features shape: {lifted.shape}\")  # ~ [B, 64, D, H/16, W/16]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Produce per-pixel depth distribution and context features,\n",
    "# then \"lift\" them into a per-depth frustum feature volume (C × D × H × W),\n",
    "# as in Lift-Splat-Shoot’s lift step.\n",
    "#   - number_of_depth_bins (D): # of discrete depth planes along each pixel ray\n",
    "#   - number_of_conv_features (C): channel size of the per-pixel context vector\n",
    "\n",
    "number_of_depth_bins = D\n",
    "number_of_conv_features = 64\n",
    "\n",
    "# 1×1 heads:\n",
    "#  - depth_head: per-pixel logits over D depth bins \n",
    "#  - feature_head: per-pixel context embedding (C channels) that will be later\n",
    "#                  broadcast across depth via an outer product with depth probs\n",
    "depth_head   = nn.Conv2d(out_channels, number_of_depth_bins, kernel_size=1)\n",
    "feature_head = nn.Conv2d(out_channels, number_of_conv_features, kernel_size=1)\n",
    "\n",
    "# Predict raw depth logits and per-pixel context features\n",
    "depth_logits  = depth_head(x_feat)        # [B, D, H/16, W/16]\n",
    "print(f\"depth logits shape: {depth_logits.shape}\")\n",
    "feature_maps  = feature_head(x_feat)      # [B, C, H/16, W/16]\n",
    "print(f\"feature maps shape: {feature_maps.shape}\")\n",
    "\n",
    "# Softmax over the depth-channel dimension to get a categorical distribution\n",
    "# per pixel. This yields p(d | pixel) that sums to 1 across D for each (h,w).\n",
    "depth_prob = torch.softmax(depth_logits, dim=1)  # [B, D, H/16, W/16]\n",
    "\n",
    "\n",
    "# \"Lift\" operation (outer product across depth and context):\n",
    "# For each pixel, expand:\n",
    "#   - depth_prob: [B, D, H, W]  -> [B, 1, D, H, W]\n",
    "#   - feature_maps: [B, C, H, W] -> [B, C, 1, H, W]\n",
    "# and multiply to obtain per-depth features: [B, C, D, H, W].\n",
    "\n",
    "\n",
    "lifted_features = torch.einsum('bdhw,bchw->bcdhw', depth_prob, feature_maps)\n",
    "\n",
    "print(f\"lifted features shape: {lifted_features.shape}\")  # [B, C, D, H/16, W/16]\n",
    "\n",
    "lifted_features = lifted_features.view(batch_size, num_cameras_per_batch, number_of_conv_features, number_of_depth_bins, feature_map_height, feature_map_width)\n",
    "print(f\"lifted features reshaped to per-camera: {lifted_features.shape}\")  # [B, num_cams, C, D, H/16, W/16]\n",
    "# This permute is needed because the frustum is already is shaper as [B, num_cams, C, D, H, W] and we need to asscociated these two in splat step\n",
    "lifted_features = lifted_features.permute(0,1,3,4,5,2)\n",
    "print(f\"lifted features permuted to per-camera: {lifted_features.shape}\")  # [B, num_cams, D, H/16, W/16, C]\n",
    "\n",
    "\n",
    "# ## Splat\n",
    "\n",
    "# ### Discretize geometry to voxel indices\n",
    "# \n",
    "# \n",
    "# \n",
    "# - Original `geom_feats` shape: `(B, N, D, H, W, 3)` containing continuous coordinates in meters in ego frame.\n",
    "# - You convert metric → index:\n",
    "# \n",
    "#   Continuous index formula for axis i:\n",
    "#   $$\n",
    "#   \\text{idx}_i = \\frac{\\text{coord}_i - (b_i - d_i / 2)}{d_i}\n",
    "#   $$\n",
    "#   Where:\n",
    "#   - `b_i` = center of voxel 0 along axis i (from `self.bx`)\n",
    "#   - `d_i` = voxel size along axis i (from `self.dx`)\n",
    "#   - Subtract `(b_i - d_i/2)` so that coordinate exactly at the lower boundary of the first voxel maps near 0.\n",
    "#   - Dividing by `d_i` scales to voxel units.\n",
    "#   - Casting to `.long()` gives integer voxel indices (floored).\n",
    "# \n",
    "# Why `bx - dx/2`? Because `bx` is the center of voxel 0. The first voxel spans approximately `[bx - dx/2, bx + dx/2)`. So subtracting `(bx - dx/2)` shifts that interval to start at 0.\n",
    "# \n",
    "# After `.view`, `geom_feats`: `(Nprime, 3)` with columns `[ix, iy, iz]`.\n",
    "\n",
    "# In[343]:\n",
    "\n",
    "\n",
    "lifted_features_flattened = lifted_features.reshape(-1, number_of_conv_features)  # [(B * num_cams * D * H/16 * W/16), C]\n",
    "\n",
    "\n",
    "\n",
    "voxel_indices = (frustum_3d_points_in_ego_frame - (bx - dx/2.0))/dx\n",
    "voxel_indices = voxel_indices.long().view(-1,3)  # [(B * num_cams * D * H/16 * W/16), 3]\n",
    "\n",
    "# batch_ix: with rows like [0,0,...,0, 1,1,...,1, ..., B-1,...,B-1]\n",
    "q = voxel_indices.shape[0] // batch_size\n",
    "batch_ix = torch.arange(batch_size).repeat_interleave(q).unsqueeze(1)\n",
    "voxel_indices = torch.cat((voxel_indices,batch_ix), dim=1)  # [(B * num_cams * D * H/16 * W/16), 4]\n",
    "\n",
    "\n",
    "valid_indices_mask_within_grid_bounds = (voxel_indices[:,0] >=0) & (voxel_indices[:, 0] < nx[0]) \\\n",
    "     & (voxel_indices[:, 1] >= 0) & (voxel_indices[:, 1] < nx[1]) \\\n",
    "     & (voxel_indices[:, 2] >= 0) & (voxel_indices[:, 2] < nx[2])\n",
    "\n",
    "     \n",
    "valid_lifted_features = lifted_features_flattened[valid_indices_mask_within_grid_bounds]\n",
    "valid_voxel_indices = voxel_indices[valid_indices_mask_within_grid_bounds]\n",
    "\n",
    "# get tensors from the same voxel next to each other\n",
    "ranks = valid_voxel_indices[:,0]*(nx[1]*nx[2]*batch_size) + valid_voxel_indices[:,1]*(nx[2]*batch_size) + valid_voxel_indices[:,2]*batch_size + valid_voxel_indices[:,3]\n",
    "\n",
    "sorts = ranks.argsort()\n",
    "valid_lifted_features_sorted = valid_lifted_features[sorts]\n",
    "valid_voxel_indices_sorted = valid_voxel_indices[sorts]\n",
    "ranks = ranks[sorts]    \n",
    "\n",
    "# rank = [a,a,a,b,c,c]\n",
    "# features = [2,3,5,7,11,13]\n",
    "# indices = [0,1,2,3,4,5]\n",
    "# cumsum(features) = [2,5,10,17,28,41]\n",
    "# kept =[False, False, True, True, False, True] \n",
    "# cumsum(features)[kept] = [10,17,41]\n",
    "# summed_features = [10, 17-10, 41-17] = [10,7,24]\n",
    "valid_lifted_features_cumsum = torch.cumsum(valid_lifted_features_sorted, dim=0)\n",
    "kept = torch.ones_like(ranks, dtype=torch.bool)\n",
    "kept[:-1] = (ranks[1:] != ranks[:-1])\n",
    "valid_lifted_features_aggregated = valid_lifted_features_cumsum[kept]\n",
    "valid_voxel_indices_sorted = valid_voxel_indices_sorted[kept]\n",
    "valid_summed_features = torch.cat((valid_lifted_features_aggregated[0:1], valid_lifted_features_aggregated[1:] - valid_lifted_features_aggregated[:-1]), dim=0)\n",
    "\n",
    "\n",
    "# griddify (B x C x Z x X x Y)\n",
    "voxel_grid_pooled_features = torch.zeros((batch_size, number_of_conv_features, nx[2], nx[0], nx[1]), device=valid_summed_features.device)\n",
    "voxel_grid_pooled_features[valid_voxel_indices_sorted[:,3], :, valid_voxel_indices_sorted[:,2], valid_voxel_indices_sorted[:,0], valid_voxel_indices_sorted[:,1]] = valid_summed_features\n",
    "\n",
    "# collapse Z\n",
    "voxel_grid_pooled_features = torch.cat(voxel_grid_pooled_features.unbind(dim=2), 1)\n",
    "\n",
    "\n",
    "# ## BevEncode/shoot\n",
    "\n",
    "# In[354]:\n",
    "\n",
    "\n",
    "voxel_grid_pooled_features.shape\n",
    "\n",
    "\n",
    "# In[361]:\n",
    "\n",
    "\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class BevEncode(nn.Module):\n",
    "    def __init__(self, inC, outC, backbone='resnet18', pretrained=False):\n",
    "        super().__init__()\n",
    "        self.bk = timm.create_model(backbone, in_chans=inC, features_only=True, pretrained=pretrained)\n",
    "        c1, c3 = self.bk.feature_info.channels()[1], self.bk.feature_info.channels()[3]  # layer1 (/4), layer3 (/16)\n",
    "        self.lat3 = nn.Conv2d(c3, 256, 1)\n",
    "        self.lat1 = nn.Conv2d(c1, 64, 1)\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv2d(256+64, 256, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Upsample(size=(200, 200), mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, outC, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.bk(x)            \n",
    "        x1, x3 = feats[1], feats[3]    \n",
    "        x1_lat = self.lat1(x1)\n",
    "        x3_up = F.interpolate(self.lat3(x3), size=x1_lat.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        y = self.fuse(torch.cat([x3_up, x1_lat], 1))\n",
    "        return self.head(y)\n",
    "\n",
    "\n",
    "# In[364]:\n",
    "\n",
    "\n",
    "bev_encoder = BevEncode(inC=number_of_conv_features, outC=1, backbone='resnet18', pretrained=False)\n",
    "preds = bev_encoder(voxel_grid_pooled_features)\n",
    "print(f\"BEV encoder output shape: {preds.shape}\")  # [B, 1, ny, nx]\n",
    "\n",
    "\n",
    "# In[365]:\n",
    "\n",
    "\n",
    "target_binary_image.shape\n",
    "\n",
    "\n",
    "# In[369]:\n",
    "\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(preds.squeeze(1), target_binary_image)\n",
    "\n",
    "\n",
    "# In[370]:\n",
    "\n",
    "\n",
    "loss\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7f4263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
